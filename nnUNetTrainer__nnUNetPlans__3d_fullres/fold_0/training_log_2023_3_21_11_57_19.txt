
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [3, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2023-03-21 11:57:21.751513: unpacking dataset... 
2023-03-21 11:57:24.646454: unpacking done... 
2023-03-21 11:57:24.648187: do_dummy_2d_data_aug: False 
2023-03-21 11:57:24.651932: Using splits from existing split file: /data_hdd/users/zengzhilin/nnUNet_preprocessed/Dataset004_Hippocampus/splits_final.json 
2023-03-21 11:57:24.652367: The split file contains 5 splits. 
2023-03-21 11:57:24.652446: Desired fold for training: 0 
2023-03-21 11:57:24.652511: This split has 208 training and 52 validation cases. 
2023-03-21 11:57:24.716714: Unable to plot network architecture: 
2023-03-21 11:57:24.716938: No module named 'hiddenlayer' 
2023-03-21 11:57:24.768509:  
2023-03-21 11:57:24.768732: Epoch 0 
2023-03-21 11:57:24.769079: Current learning rate: 0.01 
2023-03-21 11:57:43.784263: train_loss -0.2436 
2023-03-21 11:57:43.784750: val_loss -0.5222 
2023-03-21 11:57:43.784943: Pseudo dice [0.6082, 0.0] 
2023-03-21 11:57:43.785061: Epoch time: 19.02 s 
2023-03-21 11:57:43.785152: Yayy! New best EMA pseudo Dice: 0.3041 
2023-03-21 11:57:45.071203:  
2023-03-21 11:57:45.071452: Epoch 1 
2023-03-21 11:57:45.071751: Current learning rate: 0.00999 
2023-03-21 11:57:58.716474: train_loss -0.6664 
2023-03-21 11:57:58.716935: val_loss -0.7874 
2023-03-21 11:57:58.717273: Pseudo dice [0.8472, 0.8446] 
2023-03-21 11:57:58.717374: Epoch time: 13.65 s 
2023-03-21 11:57:58.717475: Yayy! New best EMA pseudo Dice: 0.3583 
2023-03-21 11:58:00.225385:  
2023-03-21 11:58:00.225564: Epoch 2 
2023-03-21 11:58:00.225736: Current learning rate: 0.00998 
2023-03-21 11:58:14.030420: train_loss -0.7905 
2023-03-21 11:58:14.030659: val_loss -0.8109 
2023-03-21 11:58:14.030821: Pseudo dice [0.8667, 0.8564] 
2023-03-21 11:58:14.030931: Epoch time: 13.81 s 
2023-03-21 11:58:14.031074: Yayy! New best EMA pseudo Dice: 0.4086 
2023-03-21 11:58:15.707675:  
2023-03-21 11:58:15.707844: Epoch 3 
2023-03-21 11:58:15.708040: Current learning rate: 0.00997 
2023-03-21 11:58:29.293258: train_loss -0.8052 
2023-03-21 11:58:29.293501: val_loss -0.8164 
2023-03-21 11:58:29.293598: Pseudo dice [0.8726, 0.8592] 
2023-03-21 11:58:29.293695: Epoch time: 13.59 s 
2023-03-21 11:58:29.293787: Yayy! New best EMA pseudo Dice: 0.4544 
2023-03-21 11:58:30.820858:  
2023-03-21 11:58:30.820985: Epoch 4 
2023-03-21 11:58:30.821145: Current learning rate: 0.00996 
2023-03-21 11:58:44.189296: train_loss -0.8166 
2023-03-21 11:58:44.189583: val_loss -0.8237 
2023-03-21 11:58:44.189713: Pseudo dice [0.8755, 0.8645] 
2023-03-21 11:58:44.189840: Epoch time: 13.37 s 
2023-03-21 11:58:44.189947: Yayy! New best EMA pseudo Dice: 0.4959 
2023-03-21 11:58:45.737212:  
2023-03-21 11:58:45.737398: Epoch 5 
2023-03-21 11:58:45.737543: Current learning rate: 0.00995 
2023-03-21 11:58:59.082421: train_loss -0.8226 
2023-03-21 11:58:59.082789: val_loss -0.8295 
2023-03-21 11:58:59.083253: Pseudo dice [0.8837, 0.8659] 
2023-03-21 11:58:59.083409: Epoch time: 13.35 s 
2023-03-21 11:58:59.083578: Yayy! New best EMA pseudo Dice: 0.5338 
2023-03-21 11:59:00.556397:  
2023-03-21 11:59:00.556526: Epoch 6 
2023-03-21 11:59:00.556687: Current learning rate: 0.00995 
2023-03-21 11:59:14.660354: train_loss -0.826 
2023-03-21 11:59:14.660524: val_loss -0.8288 
2023-03-21 11:59:14.660636: Pseudo dice [0.8827, 0.8639] 
2023-03-21 11:59:14.660727: Epoch time: 14.1 s 
2023-03-21 11:59:14.660818: Yayy! New best EMA pseudo Dice: 0.5678 
2023-03-21 11:59:16.169576:  
2023-03-21 11:59:16.169702: Epoch 7 
2023-03-21 11:59:16.169840: Current learning rate: 0.00994 
2023-03-21 11:59:29.612034: train_loss -0.8335 
2023-03-21 11:59:29.612548: val_loss -0.8361 
2023-03-21 11:59:29.612784: Pseudo dice [0.8891, 0.871] 
2023-03-21 11:59:29.612974: Epoch time: 13.44 s 
2023-03-21 11:59:29.613147: Yayy! New best EMA pseudo Dice: 0.599 
2023-03-21 11:59:31.293821:  
2023-03-21 11:59:31.293972: Epoch 8 
2023-03-21 11:59:31.294150: Current learning rate: 0.00993 
2023-03-21 11:59:44.509366: train_loss -0.8342 
2023-03-21 11:59:44.509604: val_loss -0.8344 
2023-03-21 11:59:44.509731: Pseudo dice [0.8848, 0.8716] 
2023-03-21 11:59:44.509953: Epoch time: 13.22 s 
2023-03-21 11:59:44.510117: Yayy! New best EMA pseudo Dice: 0.6269 
2023-03-21 11:59:46.028317:  
2023-03-21 11:59:46.028537: Epoch 9 
2023-03-21 11:59:46.028800: Current learning rate: 0.00992 
2023-03-21 11:59:59.497391: train_loss -0.8393 
2023-03-21 11:59:59.497675: val_loss -0.8398 
2023-03-21 11:59:59.497813: Pseudo dice [0.892, 0.8717] 
2023-03-21 11:59:59.497941: Epoch time: 13.47 s 
2023-03-21 11:59:59.498106: Yayy! New best EMA pseudo Dice: 0.6524 
2023-03-21 12:00:00.988390:  
2023-03-21 12:00:00.988554: Epoch 10 
2023-03-21 12:00:00.988724: Current learning rate: 0.00991 
2023-03-21 12:00:14.449665: train_loss -0.8418 
2023-03-21 12:00:14.450079: val_loss -0.8421 
2023-03-21 12:00:14.450590: Pseudo dice [0.8935, 0.8729] 
2023-03-21 12:00:14.450883: Epoch time: 13.46 s 
2023-03-21 12:00:14.451007: Yayy! New best EMA pseudo Dice: 0.6755 
2023-03-21 12:00:15.953132:  
2023-03-21 12:00:15.953275: Epoch 11 
2023-03-21 12:00:15.953467: Current learning rate: 0.0099 
2023-03-21 12:00:29.205738: train_loss -0.8432 
2023-03-21 12:00:29.206151: val_loss -0.8402 
2023-03-21 12:00:29.206312: Pseudo dice [0.894, 0.8713] 
2023-03-21 12:00:29.206409: Epoch time: 13.25 s 
2023-03-21 12:00:29.206505: Yayy! New best EMA pseudo Dice: 0.6962 
2023-03-21 12:00:30.672387:  
2023-03-21 12:00:30.672512: Epoch 12 
2023-03-21 12:00:30.672667: Current learning rate: 0.00989 
2023-03-21 12:00:44.393164: train_loss -0.8454 
2023-03-21 12:00:44.393416: val_loss -0.8373 
2023-03-21 12:00:44.393519: Pseudo dice [0.8885, 0.8728] 
2023-03-21 12:00:44.393624: Epoch time: 13.72 s 
2023-03-21 12:00:44.393723: Yayy! New best EMA pseudo Dice: 0.7146 
2023-03-21 12:00:46.081602:  
2023-03-21 12:00:46.081728: Epoch 13 
2023-03-21 12:00:46.081883: Current learning rate: 0.00988 
2023-03-21 12:00:59.630424: train_loss -0.8456 
2023-03-21 12:00:59.630783: val_loss -0.8414 
2023-03-21 12:00:59.631100: Pseudo dice [0.8914, 0.875] 
2023-03-21 12:00:59.631237: Epoch time: 13.55 s 
2023-03-21 12:00:59.631330: Yayy! New best EMA pseudo Dice: 0.7315 
2023-03-21 12:01:01.152338:  
2023-03-21 12:01:01.152487: Epoch 14 
2023-03-21 12:01:01.152655: Current learning rate: 0.00987 
2023-03-21 12:01:14.921027: train_loss -0.8473 
2023-03-21 12:01:14.921336: val_loss -0.8463 
2023-03-21 12:01:14.921469: Pseudo dice [0.8951, 0.8796] 
2023-03-21 12:01:14.921597: Epoch time: 13.77 s 
2023-03-21 12:01:14.921702: Yayy! New best EMA pseudo Dice: 0.7471 
2023-03-21 12:01:16.486769:  
2023-03-21 12:01:16.486921: Epoch 15 
2023-03-21 12:01:16.487096: Current learning rate: 0.00986 
2023-03-21 12:01:30.066610: train_loss -0.8489 
2023-03-21 12:01:30.066938: val_loss -0.8433 
2023-03-21 12:01:30.067074: Pseudo dice [0.8933, 0.8745] 
2023-03-21 12:01:30.067206: Epoch time: 13.58 s 
2023-03-21 12:01:30.067316: Yayy! New best EMA pseudo Dice: 0.7608 
2023-03-21 12:01:31.617244:  
2023-03-21 12:01:31.617369: Epoch 16 
2023-03-21 12:01:31.617543: Current learning rate: 0.00986 
2023-03-21 12:01:45.204114: train_loss -0.8501 
2023-03-21 12:01:45.204347: val_loss -0.8382 
2023-03-21 12:01:45.204476: Pseudo dice [0.887, 0.8723] 
2023-03-21 12:01:45.204596: Epoch time: 13.59 s 
2023-03-21 12:01:45.204700: Yayy! New best EMA pseudo Dice: 0.7727 
2023-03-21 12:01:46.754558:  
2023-03-21 12:01:46.754700: Epoch 17 
2023-03-21 12:01:46.754898: Current learning rate: 0.00985 
2023-03-21 12:02:00.857237: train_loss -0.8514 
2023-03-21 12:02:00.857466: val_loss -0.8402 
2023-03-21 12:02:00.857577: Pseudo dice [0.8908, 0.8726] 
2023-03-21 12:02:00.857669: Epoch time: 14.1 s 
2023-03-21 12:02:00.857762: Yayy! New best EMA pseudo Dice: 0.7836 
2023-03-21 12:02:02.611702:  
2023-03-21 12:02:02.611830: Epoch 18 
2023-03-21 12:02:02.611986: Current learning rate: 0.00984 
2023-03-21 12:02:16.323544: train_loss -0.8516 
2023-03-21 12:02:16.323792: val_loss -0.8434 
2023-03-21 12:02:16.323904: Pseudo dice [0.8924, 0.874] 
2023-03-21 12:02:16.324002: Epoch time: 13.71 s 
2023-03-21 12:02:16.324080: Yayy! New best EMA pseudo Dice: 0.7935 
2023-03-21 12:02:17.901725:  
2023-03-21 12:02:17.901872: Epoch 19 
2023-03-21 12:02:17.902030: Current learning rate: 0.00983 
2023-03-21 12:02:31.989330: train_loss -0.8542 
2023-03-21 12:02:31.989557: val_loss -0.8448 
2023-03-21 12:02:31.989685: Pseudo dice [0.8957, 0.8765] 
2023-03-21 12:02:31.989812: Epoch time: 14.09 s 
2023-03-21 12:02:31.989894: Yayy! New best EMA pseudo Dice: 0.8028 
2023-03-21 12:02:33.588758:  
2023-03-21 12:02:33.589036: Epoch 20 
2023-03-21 12:02:33.589179: Current learning rate: 0.00982 
2023-03-21 12:02:47.064233: train_loss -0.8558 
2023-03-21 12:02:47.064484: val_loss -0.8466 
2023-03-21 12:02:47.064709: Pseudo dice [0.8944, 0.8796] 
2023-03-21 12:02:47.064960: Epoch time: 13.48 s 
2023-03-21 12:02:47.065172: Yayy! New best EMA pseudo Dice: 0.8112 
2023-03-21 12:02:48.632122:  
2023-03-21 12:02:48.632272: Epoch 21 
2023-03-21 12:02:48.632431: Current learning rate: 0.00981 
2023-03-21 12:03:02.280793: train_loss -0.856 
2023-03-21 12:03:02.281023: val_loss -0.8436 
2023-03-21 12:03:02.281116: Pseudo dice [0.8935, 0.8754] 
2023-03-21 12:03:02.281225: Epoch time: 13.65 s 
2023-03-21 12:03:02.281318: Yayy! New best EMA pseudo Dice: 0.8185 
2023-03-21 12:03:03.768787:  
2023-03-21 12:03:03.768930: Epoch 22 
2023-03-21 12:03:03.769078: Current learning rate: 0.0098 
2023-03-21 12:03:17.320988: train_loss -0.8544 
2023-03-21 12:03:17.321158: val_loss -0.8463 
2023-03-21 12:03:17.321267: Pseudo dice [0.8957, 0.877] 
2023-03-21 12:03:17.321357: Epoch time: 13.55 s 
2023-03-21 12:03:17.321451: Yayy! New best EMA pseudo Dice: 0.8253 
2023-03-21 12:03:18.885483:  
2023-03-21 12:03:18.885653: Epoch 23 
2023-03-21 12:03:18.885839: Current learning rate: 0.00979 
2023-03-21 12:03:32.737381: train_loss -0.8582 
2023-03-21 12:03:32.737761: val_loss -0.847 
2023-03-21 12:03:32.737992: Pseudo dice [0.8959, 0.8782] 
2023-03-21 12:03:32.738287: Epoch time: 13.85 s 
2023-03-21 12:03:32.738545: Yayy! New best EMA pseudo Dice: 0.8315 
2023-03-21 12:03:34.504834:  
2023-03-21 12:03:34.504997: Epoch 24 
2023-03-21 12:03:34.505189: Current learning rate: 0.00978 
2023-03-21 12:03:48.198553: train_loss -0.8599 
2023-03-21 12:03:48.198847: val_loss -0.8519 
2023-03-21 12:03:48.199001: Pseudo dice [0.8989, 0.8827] 
2023-03-21 12:03:48.199122: Epoch time: 13.69 s 
2023-03-21 12:03:48.199241: Yayy! New best EMA pseudo Dice: 0.8374 
2023-03-21 12:03:49.721665:  
2023-03-21 12:03:49.721826: Epoch 25 
2023-03-21 12:03:49.721993: Current learning rate: 0.00977 
2023-03-21 12:04:03.593746: train_loss -0.8598 
2023-03-21 12:04:03.594282: val_loss -0.8447 
2023-03-21 12:04:03.594444: Pseudo dice [0.8947, 0.8758] 
2023-03-21 12:04:03.594660: Epoch time: 13.87 s 
2023-03-21 12:04:03.594867: Yayy! New best EMA pseudo Dice: 0.8422 
2023-03-21 12:04:05.194770:  
2023-03-21 12:04:05.194917: Epoch 26 
2023-03-21 12:04:05.195085: Current learning rate: 0.00977 
2023-03-21 12:04:18.981980: train_loss -0.8609 
2023-03-21 12:04:18.982438: val_loss -0.8467 
2023-03-21 12:04:18.982769: Pseudo dice [0.8943, 0.877] 
2023-03-21 12:04:18.983024: Epoch time: 13.79 s 
2023-03-21 12:04:18.983342: Yayy! New best EMA pseudo Dice: 0.8465 
2023-03-21 12:04:20.488395:  
2023-03-21 12:04:20.488537: Epoch 27 
2023-03-21 12:04:20.488712: Current learning rate: 0.00976 
2023-03-21 12:04:33.956098: train_loss -0.8635 
2023-03-21 12:04:33.956279: val_loss -0.8468 
2023-03-21 12:04:33.956393: Pseudo dice [0.8935, 0.8776] 
2023-03-21 12:04:33.956489: Epoch time: 13.47 s 
2023-03-21 12:04:33.956568: Yayy! New best EMA pseudo Dice: 0.8504 
2023-03-21 12:04:35.452434:  
2023-03-21 12:04:35.452567: Epoch 28 
2023-03-21 12:04:35.452738: Current learning rate: 0.00975 
2023-03-21 12:04:49.119272: train_loss -0.8628 
2023-03-21 12:04:49.119516: val_loss -0.8492 
2023-03-21 12:04:49.119613: Pseudo dice [0.8957, 0.8797] 
2023-03-21 12:04:49.119709: Epoch time: 13.67 s 
2023-03-21 12:04:49.119787: Yayy! New best EMA pseudo Dice: 0.8542 
2023-03-21 12:04:50.589176:  
2023-03-21 12:04:50.589302: Epoch 29 
2023-03-21 12:04:50.589457: Current learning rate: 0.00974 
2023-03-21 12:05:04.243670: train_loss -0.8617 
2023-03-21 12:05:04.243861: val_loss -0.8477 
2023-03-21 12:05:04.243975: Pseudo dice [0.8948, 0.8783] 
2023-03-21 12:05:04.244070: Epoch time: 13.66 s 
2023-03-21 12:05:04.244150: Yayy! New best EMA pseudo Dice: 0.8574 
2023-03-21 12:05:05.717094:  
2023-03-21 12:05:05.717217: Epoch 30 
2023-03-21 12:05:05.717372: Current learning rate: 0.00973 
2023-03-21 12:05:19.295367: train_loss -0.8636 
2023-03-21 12:05:19.295813: val_loss -0.8508 
2023-03-21 12:05:19.295993: Pseudo dice [0.898, 0.8816] 
2023-03-21 12:05:19.296125: Epoch time: 13.58 s 
2023-03-21 12:05:19.296326: Yayy! New best EMA pseudo Dice: 0.8606 
2023-03-21 12:05:20.785662:  
2023-03-21 12:05:20.785787: Epoch 31 
2023-03-21 12:05:20.785944: Current learning rate: 0.00972 
2023-03-21 12:05:34.179326: train_loss -0.8645 
2023-03-21 12:05:34.179503: val_loss -0.8464 
2023-03-21 12:05:34.179630: Pseudo dice [0.8958, 0.8773] 
2023-03-21 12:05:34.179723: Epoch time: 13.39 s 
2023-03-21 12:05:34.179801: Yayy! New best EMA pseudo Dice: 0.8632 
2023-03-21 12:05:35.782902:  
2023-03-21 12:05:35.783201: Epoch 32 
2023-03-21 12:05:35.783451: Current learning rate: 0.00971 
2023-03-21 12:05:49.385114: train_loss -0.8646 
2023-03-21 12:05:49.385365: val_loss -0.8477 
2023-03-21 12:05:49.385484: Pseudo dice [0.896, 0.8776] 
2023-03-21 12:05:49.385601: Epoch time: 13.6 s 
2023-03-21 12:05:49.385687: Yayy! New best EMA pseudo Dice: 0.8656 
2023-03-21 12:05:50.887365:  
2023-03-21 12:05:50.887632: Epoch 33 
2023-03-21 12:05:50.887811: Current learning rate: 0.0097 
2023-03-21 12:06:04.343665: train_loss -0.8648 
2023-03-21 12:06:04.343848: val_loss -0.8469 
2023-03-21 12:06:04.343959: Pseudo dice [0.8969, 0.8773] 
2023-03-21 12:06:04.344049: Epoch time: 13.46 s 
2023-03-21 12:06:04.344126: Yayy! New best EMA pseudo Dice: 0.8677 
2023-03-21 12:06:06.010641:  
2023-03-21 12:06:06.010809: Epoch 34 
2023-03-21 12:06:06.010956: Current learning rate: 0.00969 
2023-03-21 12:06:19.617741: train_loss -0.8668 
2023-03-21 12:06:19.618015: val_loss -0.8499 
2023-03-21 12:06:19.618150: Pseudo dice [0.8977, 0.8794] 
2023-03-21 12:06:19.618255: Epoch time: 13.61 s 
2023-03-21 12:06:19.618342: Yayy! New best EMA pseudo Dice: 0.8698 
2023-03-21 12:06:21.227864:  
2023-03-21 12:06:21.227995: Epoch 35 
2023-03-21 12:06:21.228167: Current learning rate: 0.00968 
2023-03-21 12:06:35.078531: train_loss -0.8672 
2023-03-21 12:06:35.078787: val_loss -0.8475 
2023-03-21 12:06:35.078899: Pseudo dice [0.8964, 0.8758] 
2023-03-21 12:06:35.079010: Epoch time: 13.85 s 
2023-03-21 12:06:35.079098: Yayy! New best EMA pseudo Dice: 0.8715 
2023-03-21 12:06:36.642973:  
2023-03-21 12:06:36.643127: Epoch 36 
2023-03-21 12:06:36.643288: Current learning rate: 0.00968 
2023-03-21 12:06:50.593109: train_loss -0.8681 
2023-03-21 12:06:50.593386: val_loss -0.8472 
2023-03-21 12:06:50.593503: Pseudo dice [0.896, 0.8779] 
2023-03-21 12:06:50.593631: Epoch time: 13.95 s 
2023-03-21 12:06:50.593728: Yayy! New best EMA pseudo Dice: 0.873 
2023-03-21 12:06:52.247159:  
2023-03-21 12:06:52.247582: Epoch 37 
2023-03-21 12:06:52.247836: Current learning rate: 0.00967 
2023-03-21 12:07:05.885931: train_loss -0.8697 
2023-03-21 12:07:05.886339: val_loss -0.8489 
2023-03-21 12:07:05.886609: Pseudo dice [0.8971, 0.8783] 
2023-03-21 12:07:05.886990: Epoch time: 13.64 s 
2023-03-21 12:07:05.887252: Yayy! New best EMA pseudo Dice: 0.8745 
2023-03-21 12:07:07.393208:  
2023-03-21 12:07:07.393347: Epoch 38 
2023-03-21 12:07:07.393527: Current learning rate: 0.00966 
2023-03-21 12:07:20.943315: train_loss -0.8698 
2023-03-21 12:07:20.943509: val_loss -0.8468 
2023-03-21 12:07:20.943634: Pseudo dice [0.8957, 0.8759] 
2023-03-21 12:07:20.943725: Epoch time: 13.55 s 
2023-03-21 12:07:20.943803: Yayy! New best EMA pseudo Dice: 0.8756 
2023-03-21 12:07:22.500564:  
2023-03-21 12:07:22.500708: Epoch 39 
2023-03-21 12:07:22.500873: Current learning rate: 0.00965 
2023-03-21 12:07:35.926851: train_loss -0.867 
2023-03-21 12:07:35.927104: val_loss -0.8502 
2023-03-21 12:07:35.927202: Pseudo dice [0.8977, 0.8789] 
2023-03-21 12:07:35.927296: Epoch time: 13.43 s 
2023-03-21 12:07:35.927375: Yayy! New best EMA pseudo Dice: 0.8769 
2023-03-21 12:07:37.480315:  
2023-03-21 12:07:37.480461: Epoch 40 
2023-03-21 12:07:37.480633: Current learning rate: 0.00964 
2023-03-21 12:07:51.093575: train_loss -0.8675 
2023-03-21 12:07:51.093971: val_loss -0.8455 
2023-03-21 12:07:51.094164: Pseudo dice [0.8957, 0.873] 
2023-03-21 12:07:51.094431: Epoch time: 13.61 s 
2023-03-21 12:07:51.094608: Yayy! New best EMA pseudo Dice: 0.8776 
2023-03-21 12:07:52.661438:  
2023-03-21 12:07:52.661567: Epoch 41 
2023-03-21 12:07:52.661752: Current learning rate: 0.00963 
2023-03-21 12:08:06.589615: train_loss -0.872 
2023-03-21 12:08:06.589862: val_loss -0.8463 
2023-03-21 12:08:06.589964: Pseudo dice [0.8941, 0.879] 
2023-03-21 12:08:06.590062: Epoch time: 13.93 s 
2023-03-21 12:08:06.590287: Yayy! New best EMA pseudo Dice: 0.8785 
2023-03-21 12:08:08.228710:  
2023-03-21 12:08:08.228908: Epoch 42 
2023-03-21 12:08:08.229075: Current learning rate: 0.00962 
2023-03-21 12:08:21.894059: train_loss -0.8722 
2023-03-21 12:08:21.894335: val_loss -0.8465 
2023-03-21 12:08:21.894614: Pseudo dice [0.8943, 0.8779] 
2023-03-21 12:08:21.894812: Epoch time: 13.67 s 
2023-03-21 12:08:21.894909: Yayy! New best EMA pseudo Dice: 0.8793 
2023-03-21 12:08:23.354780:  
2023-03-21 12:08:23.354940: Epoch 43 
2023-03-21 12:08:23.355136: Current learning rate: 0.00961 
2023-03-21 12:08:37.101256: train_loss -0.8722 
2023-03-21 12:08:37.101522: val_loss -0.8477 
2023-03-21 12:08:37.101634: Pseudo dice [0.8966, 0.8792] 
2023-03-21 12:08:37.101730: Epoch time: 13.75 s 
2023-03-21 12:08:37.101809: Yayy! New best EMA pseudo Dice: 0.8801 
2023-03-21 12:08:38.635874:  
2023-03-21 12:08:38.636148: Epoch 44 
2023-03-21 12:08:38.636408: Current learning rate: 0.0096 
2023-03-21 12:08:52.437477: train_loss -0.8726 
2023-03-21 12:08:52.437686: val_loss -0.8432 
2023-03-21 12:08:52.437789: Pseudo dice [0.8927, 0.8755] 
2023-03-21 12:08:52.437887: Epoch time: 13.8 s 
2023-03-21 12:08:52.437985: Yayy! New best EMA pseudo Dice: 0.8805 
2023-03-21 12:08:53.932362:  
2023-03-21 12:08:53.932528: Epoch 45 
2023-03-21 12:08:53.932707: Current learning rate: 0.00959 
2023-03-21 12:09:08.306868: train_loss -0.8744 
2023-03-21 12:09:08.307304: val_loss -0.8516 
2023-03-21 12:09:08.307539: Pseudo dice [0.9005, 0.8816] 
2023-03-21 12:09:08.307802: Epoch time: 14.38 s 
2023-03-21 12:09:08.307914: Yayy! New best EMA pseudo Dice: 0.8816 
2023-03-21 12:09:09.824964:  
2023-03-21 12:09:09.825369: Epoch 46 
2023-03-21 12:09:09.825622: Current learning rate: 0.00959 
2023-03-21 12:09:23.636149: train_loss -0.8735 
2023-03-21 12:09:23.636373: val_loss -0.8407 
2023-03-21 12:09:23.636565: Pseudo dice [0.8886, 0.8759] 
2023-03-21 12:09:23.636755: Epoch time: 13.81 s 
2023-03-21 12:09:23.636917: Yayy! New best EMA pseudo Dice: 0.8817 
2023-03-21 12:09:25.359027:  
2023-03-21 12:09:25.359249: Epoch 47 
2023-03-21 12:09:25.359427: Current learning rate: 0.00958 
2023-03-21 12:09:39.169864: train_loss -0.8744 
2023-03-21 12:09:39.170115: val_loss -0.8484 
2023-03-21 12:09:39.170943: Pseudo dice [0.8983, 0.8774] 
2023-03-21 12:09:39.171074: Epoch time: 13.81 s 
2023-03-21 12:09:39.171180: Yayy! New best EMA pseudo Dice: 0.8823 
2023-03-21 12:09:40.668746:  
2023-03-21 12:09:40.668884: Epoch 48 
2023-03-21 12:09:40.669045: Current learning rate: 0.00957 
2023-03-21 12:09:54.342347: train_loss -0.8739 
2023-03-21 12:09:54.342656: val_loss -0.8447 
2023-03-21 12:09:54.342808: Pseudo dice [0.895, 0.8752] 
2023-03-21 12:09:54.342940: Epoch time: 13.67 s 
2023-03-21 12:09:54.343042: Yayy! New best EMA pseudo Dice: 0.8826 
2023-03-21 12:09:55.854976:  
2023-03-21 12:09:55.855124: Epoch 49 
2023-03-21 12:09:55.855281: Current learning rate: 0.00956 
2023-03-21 12:10:09.708194: train_loss -0.8745 
2023-03-21 12:10:09.708678: val_loss -0.8453 
2023-03-21 12:10:09.708808: Pseudo dice [0.8935, 0.8753] 
2023-03-21 12:10:09.708933: Epoch time: 13.85 s 
2023-03-21 12:10:09.814416: Yayy! New best EMA pseudo Dice: 0.8827 
2023-03-21 12:10:11.352843:  
2023-03-21 12:10:11.352975: Epoch 50 
2023-03-21 12:10:11.353133: Current learning rate: 0.00955 
2023-03-21 12:10:24.887465: train_loss -0.8761 
2023-03-21 12:10:24.887695: val_loss -0.8507 
2023-03-21 12:10:24.887805: Pseudo dice [0.8966, 0.8811] 
2023-03-21 12:10:24.887897: Epoch time: 13.54 s 
2023-03-21 12:10:24.887973: Yayy! New best EMA pseudo Dice: 0.8834 
2023-03-21 12:10:26.345050:  
2023-03-21 12:10:26.345256: Epoch 51 
2023-03-21 12:10:26.345499: Current learning rate: 0.00954 
2023-03-21 12:10:39.947291: train_loss -0.8763 
2023-03-21 12:10:39.947481: val_loss -0.848 
2023-03-21 12:10:39.947595: Pseudo dice [0.897, 0.8794] 
2023-03-21 12:10:39.947690: Epoch time: 13.6 s 
2023-03-21 12:10:39.947771: Yayy! New best EMA pseudo Dice: 0.8838 
2023-03-21 12:10:41.538958:  
2023-03-21 12:10:41.539093: Epoch 52 
2023-03-21 12:10:41.539248: Current learning rate: 0.00953 
2023-03-21 12:10:55.043884: train_loss -0.8751 
2023-03-21 12:10:55.044127: val_loss -0.8421 
2023-03-21 12:10:55.044237: Pseudo dice [0.8934, 0.8732] 
2023-03-21 12:10:55.044333: Epoch time: 13.51 s 
2023-03-21 12:10:56.223735:  
2023-03-21 12:10:56.223882: Epoch 53 
2023-03-21 12:10:56.224042: Current learning rate: 0.00952 
2023-03-21 12:11:09.771203: train_loss -0.8758 
2023-03-21 12:11:09.771405: val_loss -0.8548 
2023-03-21 12:11:09.771532: Pseudo dice [0.901, 0.8844] 
2023-03-21 12:11:09.771631: Epoch time: 13.55 s 
2023-03-21 12:11:09.771714: Yayy! New best EMA pseudo Dice: 0.8847 
2023-03-21 12:11:11.257411:  
2023-03-21 12:11:11.257542: Epoch 54 
2023-03-21 12:11:11.257704: Current learning rate: 0.00951 
2023-03-21 12:11:24.611837: train_loss -0.8766 
2023-03-21 12:11:24.612087: val_loss -0.8484 
2023-03-21 12:11:24.612200: Pseudo dice [0.8983, 0.8785] 
2023-03-21 12:11:24.612293: Epoch time: 13.36 s 
2023-03-21 12:11:24.612370: Yayy! New best EMA pseudo Dice: 0.885 
2023-03-21 12:11:26.197616:  
2023-03-21 12:11:26.197743: Epoch 55 
2023-03-21 12:11:26.197903: Current learning rate: 0.0095 
2023-03-21 12:11:39.777335: train_loss -0.8778 
2023-03-21 12:11:39.777613: val_loss -0.8463 
2023-03-21 12:11:39.777741: Pseudo dice [0.8949, 0.8777] 
2023-03-21 12:11:39.777854: Epoch time: 13.58 s 
2023-03-21 12:11:39.777942: Yayy! New best EMA pseudo Dice: 0.8852 
2023-03-21 12:11:41.275632:  
2023-03-21 12:11:41.275762: Epoch 56 
2023-03-21 12:11:41.275919: Current learning rate: 0.00949 
2023-03-21 12:11:54.947839: train_loss -0.8779 
2023-03-21 12:11:54.948064: val_loss -0.8458 
2023-03-21 12:11:54.948197: Pseudo dice [0.8959, 0.8753] 
2023-03-21 12:11:54.948319: Epoch time: 13.67 s 
2023-03-21 12:11:54.948426: Yayy! New best EMA pseudo Dice: 0.8852 
2023-03-21 12:11:56.542606:  
2023-03-21 12:11:56.542761: Epoch 57 
2023-03-21 12:11:56.542922: Current learning rate: 0.00949 
2023-03-21 12:12:10.300140: train_loss -0.8786 
2023-03-21 12:12:10.300407: val_loss -0.844 
2023-03-21 12:12:10.300526: Pseudo dice [0.8936, 0.8766] 
2023-03-21 12:12:10.300632: Epoch time: 13.76 s 
2023-03-21 12:12:11.538833:  
2023-03-21 12:12:11.538985: Epoch 58 
2023-03-21 12:12:11.539158: Current learning rate: 0.00948 
2023-03-21 12:12:25.099962: train_loss -0.8787 
2023-03-21 12:12:25.100274: val_loss -0.8488 
2023-03-21 12:12:25.100408: Pseudo dice [0.898, 0.879] 
2023-03-21 12:12:25.100541: Epoch time: 13.56 s 
2023-03-21 12:12:25.100652: Yayy! New best EMA pseudo Dice: 0.8855 
2023-03-21 12:12:26.659578:  
2023-03-21 12:12:26.659708: Epoch 59 
2023-03-21 12:12:26.659868: Current learning rate: 0.00947 
2023-03-21 12:12:40.396322: train_loss -0.8814 
2023-03-21 12:12:40.396587: val_loss -0.846 
2023-03-21 12:12:40.396703: Pseudo dice [0.8969, 0.8769] 
2023-03-21 12:12:40.396803: Epoch time: 13.74 s 
2023-03-21 12:12:40.396899: Yayy! New best EMA pseudo Dice: 0.8857 
2023-03-21 12:12:41.885000:  
2023-03-21 12:12:41.885145: Epoch 60 
2023-03-21 12:12:41.885302: Current learning rate: 0.00946 
2023-03-21 12:12:55.349686: train_loss -0.8799 
2023-03-21 12:12:55.349933: val_loss -0.8506 
2023-03-21 12:12:55.350048: Pseudo dice [0.897, 0.8817] 
2023-03-21 12:12:55.350149: Epoch time: 13.47 s 
2023-03-21 12:12:55.350245: Yayy! New best EMA pseudo Dice: 0.886 
2023-03-21 12:12:56.842673:  
2023-03-21 12:12:56.842814: Epoch 61 
2023-03-21 12:12:56.842971: Current learning rate: 0.00945 
2023-03-21 12:13:10.459195: train_loss -0.8812 
2023-03-21 12:13:10.460672: val_loss -0.8438 
2023-03-21 12:13:10.460778: Pseudo dice [0.8908, 0.8771] 
2023-03-21 12:13:10.460879: Epoch time: 13.62 s 
2023-03-21 12:13:11.780344:  
2023-03-21 12:13:11.780511: Epoch 62 
2023-03-21 12:13:11.780674: Current learning rate: 0.00944 
2023-03-21 12:13:25.473117: train_loss -0.8802 
2023-03-21 12:13:25.473378: val_loss -0.8497 
2023-03-21 12:13:25.473510: Pseudo dice [0.8989, 0.8794] 
2023-03-21 12:13:25.473613: Epoch time: 13.69 s 
2023-03-21 12:13:25.473697: Yayy! New best EMA pseudo Dice: 0.8862 
2023-03-21 12:13:26.960090:  
2023-03-21 12:13:26.960227: Epoch 63 
2023-03-21 12:13:26.960383: Current learning rate: 0.00943 
2023-03-21 12:13:40.640250: train_loss -0.8794 
2023-03-21 12:13:40.640455: val_loss -0.8437 
2023-03-21 12:13:40.640562: Pseudo dice [0.8926, 0.8763] 
2023-03-21 12:13:40.640661: Epoch time: 13.68 s 
2023-03-21 12:13:41.860830:  
2023-03-21 12:13:41.860960: Epoch 64 
2023-03-21 12:13:41.861115: Current learning rate: 0.00942 
2023-03-21 12:13:55.641628: train_loss -0.8802 
2023-03-21 12:13:55.641918: val_loss -0.8452 
2023-03-21 12:13:55.642052: Pseudo dice [0.8955, 0.8763] 
2023-03-21 12:13:55.642180: Epoch time: 13.78 s 
2023-03-21 12:13:56.874803:  
2023-03-21 12:13:56.874975: Epoch 65 
2023-03-21 12:13:56.875138: Current learning rate: 0.00941 
2023-03-21 12:14:10.469846: train_loss -0.8815 
2023-03-21 12:14:10.470065: val_loss -0.8474 
2023-03-21 12:14:10.470175: Pseudo dice [0.8976, 0.877] 
2023-03-21 12:14:10.470267: Epoch time: 13.6 s 
2023-03-21 12:14:11.638640:  
2023-03-21 12:14:11.638806: Epoch 66 
2023-03-21 12:14:11.638959: Current learning rate: 0.0094 
2023-03-21 12:14:25.430936: train_loss -0.8832 
2023-03-21 12:14:25.431346: val_loss -0.8456 
2023-03-21 12:14:25.431571: Pseudo dice [0.8952, 0.8759] 
2023-03-21 12:14:25.431791: Epoch time: 13.79 s 
2023-03-21 12:14:26.797754:  
2023-03-21 12:14:26.797886: Epoch 67 
2023-03-21 12:14:26.798040: Current learning rate: 0.00939 
2023-03-21 12:14:40.190975: train_loss -0.8831 
2023-03-21 12:14:40.191170: val_loss -0.8463 
2023-03-21 12:14:40.191272: Pseudo dice [0.8967, 0.8765] 
2023-03-21 12:14:40.191370: Epoch time: 13.39 s 
2023-03-21 12:14:41.434307:  
2023-03-21 12:14:41.434467: Epoch 68 
2023-03-21 12:14:41.434622: Current learning rate: 0.00939 
2023-03-21 12:14:54.896019: train_loss -0.8826 
2023-03-21 12:14:54.896284: val_loss -0.8505 
2023-03-21 12:14:54.896421: Pseudo dice [0.8979, 0.8799] 
2023-03-21 12:14:54.896565: Epoch time: 13.46 s 
2023-03-21 12:14:54.897418: Yayy! New best EMA pseudo Dice: 0.8864 
2023-03-21 12:14:56.392881:  
2023-03-21 12:14:56.393011: Epoch 69 
2023-03-21 12:14:56.393189: Current learning rate: 0.00938 
2023-03-21 12:15:09.908471: train_loss -0.8833 
2023-03-21 12:15:09.908805: val_loss -0.8467 
2023-03-21 12:15:09.908928: Pseudo dice [0.8936, 0.8787] 
2023-03-21 12:15:09.909167: Epoch time: 13.52 s 
2023-03-21 12:15:11.139318:  
2023-03-21 12:15:11.139444: Epoch 70 
2023-03-21 12:15:11.139599: Current learning rate: 0.00937 
2023-03-21 12:15:24.786291: train_loss -0.884 
2023-03-21 12:15:24.786556: val_loss -0.8521 
2023-03-21 12:15:24.786653: Pseudo dice [0.8987, 0.8827] 
2023-03-21 12:15:24.786776: Epoch time: 13.65 s 
2023-03-21 12:15:24.786871: Yayy! New best EMA pseudo Dice: 0.8868 
2023-03-21 12:15:26.277122:  
2023-03-21 12:15:26.277248: Epoch 71 
2023-03-21 12:15:26.277404: Current learning rate: 0.00936 
2023-03-21 12:15:39.603169: train_loss -0.8832 
2023-03-21 12:15:39.603420: val_loss -0.8496 
2023-03-21 12:15:39.603540: Pseudo dice [0.8987, 0.8795] 
2023-03-21 12:15:39.603644: Epoch time: 13.33 s 
2023-03-21 12:15:39.603744: Yayy! New best EMA pseudo Dice: 0.887 
2023-03-21 12:15:41.326260:  
2023-03-21 12:15:41.326415: Epoch 72 
2023-03-21 12:15:41.326605: Current learning rate: 0.00935 
2023-03-21 12:15:54.921620: train_loss -0.8853 
2023-03-21 12:15:54.921870: val_loss -0.8442 
2023-03-21 12:15:54.921984: Pseudo dice [0.8951, 0.8752] 
2023-03-21 12:15:54.922083: Epoch time: 13.6 s 
2023-03-21 12:15:56.164491:  
2023-03-21 12:15:56.164632: Epoch 73 
2023-03-21 12:15:56.164805: Current learning rate: 0.00934 
2023-03-21 12:16:09.918571: train_loss -0.8839 
2023-03-21 12:16:09.920039: val_loss -0.8485 
2023-03-21 12:16:09.920171: Pseudo dice [0.8982, 0.879] 
2023-03-21 12:16:09.920278: Epoch time: 13.75 s 
2023-03-21 12:16:11.252339:  
2023-03-21 12:16:11.252469: Epoch 74 
2023-03-21 12:16:11.252612: Current learning rate: 0.00933 
2023-03-21 12:16:24.815719: train_loss -0.8849 
2023-03-21 12:16:24.816301: val_loss -0.8473 
2023-03-21 12:16:24.816627: Pseudo dice [0.8968, 0.8799] 
2023-03-21 12:16:24.816836: Epoch time: 13.56 s 
2023-03-21 12:16:24.817209: Yayy! New best EMA pseudo Dice: 0.8872 
2023-03-21 12:16:26.345337:  
2023-03-21 12:16:26.345466: Epoch 75 
2023-03-21 12:16:26.345620: Current learning rate: 0.00932 
2023-03-21 12:16:39.830240: train_loss -0.8855 
2023-03-21 12:16:39.830858: val_loss -0.8474 
2023-03-21 12:16:39.831007: Pseudo dice [0.8978, 0.8759] 
2023-03-21 12:16:39.831183: Epoch time: 13.49 s 
2023-03-21 12:16:41.408135:  
2023-03-21 12:16:41.408464: Epoch 76 
2023-03-21 12:16:41.408744: Current learning rate: 0.00931 
2023-03-21 12:16:55.047522: train_loss -0.8843 
2023-03-21 12:16:55.048023: val_loss -0.8432 
2023-03-21 12:16:55.048134: Pseudo dice [0.8932, 0.8747] 
2023-03-21 12:16:55.048261: Epoch time: 13.64 s 
2023-03-21 12:16:56.285624:  
2023-03-21 12:16:56.285815: Epoch 77 
2023-03-21 12:16:56.285968: Current learning rate: 0.0093 
2023-03-21 12:17:10.253242: train_loss -0.8872 
2023-03-21 12:17:10.253679: val_loss -0.8394 
2023-03-21 12:17:10.253911: Pseudo dice [0.8908, 0.8734] 
2023-03-21 12:17:10.254102: Epoch time: 13.97 s 
2023-03-21 12:17:11.527883:  
2023-03-21 12:17:11.528042: Epoch 78 
2023-03-21 12:17:11.528190: Current learning rate: 0.0093 
2023-03-21 12:17:25.169078: train_loss -0.8873 
2023-03-21 12:17:25.169353: val_loss -0.8424 
2023-03-21 12:17:25.169475: Pseudo dice [0.8924, 0.8758] 
2023-03-21 12:17:25.169583: Epoch time: 13.64 s 
2023-03-21 12:17:26.480688:  
2023-03-21 12:17:26.480883: Epoch 79 
2023-03-21 12:17:26.481041: Current learning rate: 0.00929 
2023-03-21 12:17:40.607670: train_loss -0.8859 
2023-03-21 12:17:40.607938: val_loss -0.8452 
2023-03-21 12:17:40.608045: Pseudo dice [0.8949, 0.8786] 
2023-03-21 12:17:40.608151: Epoch time: 14.13 s 
2023-03-21 12:17:41.886377:  
2023-03-21 12:17:41.886529: Epoch 80 
2023-03-21 12:17:41.886698: Current learning rate: 0.00928 
2023-03-21 12:17:55.883149: train_loss -0.8862 
2023-03-21 12:17:55.883626: val_loss -0.8483 
2023-03-21 12:17:55.883884: Pseudo dice [0.8977, 0.8784] 
2023-03-21 12:17:55.884358: Epoch time: 14.0 s 
2023-03-21 12:17:57.349879:  
2023-03-21 12:17:57.350055: Epoch 81 
2023-03-21 12:17:57.350274: Current learning rate: 0.00927 
2023-03-21 12:18:11.059062: train_loss -0.8866 
2023-03-21 12:18:11.059292: val_loss -0.8463 
2023-03-21 12:18:11.059395: Pseudo dice [0.8967, 0.8782] 
2023-03-21 12:18:11.059494: Epoch time: 13.71 s 
2023-03-21 12:18:12.340557:  
2023-03-21 12:18:12.341076: Epoch 82 
2023-03-21 12:18:12.341428: Current learning rate: 0.00926 
2023-03-21 12:18:26.279783: train_loss -0.8871 
2023-03-21 12:18:26.280260: val_loss -0.846 
2023-03-21 12:18:26.280697: Pseudo dice [0.8968, 0.8774] 
2023-03-21 12:18:26.280988: Epoch time: 13.94 s 
2023-03-21 12:18:27.476001:  
2023-03-21 12:18:27.476148: Epoch 83 
2023-03-21 12:18:27.476319: Current learning rate: 0.00925 
2023-03-21 12:18:41.331489: train_loss -0.8875 
2023-03-21 12:18:41.331796: val_loss -0.8432 
2023-03-21 12:18:41.332305: Pseudo dice [0.8939, 0.8757] 
2023-03-21 12:18:41.332492: Epoch time: 13.86 s 
2023-03-21 12:18:42.511245:  
2023-03-21 12:18:42.511389: Epoch 84 
2023-03-21 12:18:42.511522: Current learning rate: 0.00924 
2023-03-21 12:18:56.209184: train_loss -0.8884 
2023-03-21 12:18:56.209466: val_loss -0.8484 
2023-03-21 12:18:56.209572: Pseudo dice [0.8988, 0.8786] 
2023-03-21 12:18:56.209679: Epoch time: 13.7 s 
2023-03-21 12:18:57.409434:  
2023-03-21 12:18:57.409580: Epoch 85 
2023-03-21 12:18:57.409745: Current learning rate: 0.00923 
2023-03-21 12:19:11.145858: train_loss -0.8883 
2023-03-21 12:19:11.146279: val_loss -0.8433 
2023-03-21 12:19:11.146441: Pseudo dice [0.8947, 0.8749] 
2023-03-21 12:19:11.146624: Epoch time: 13.74 s 
2023-03-21 12:19:12.467767:  
2023-03-21 12:19:12.467932: Epoch 86 
2023-03-21 12:19:12.468096: Current learning rate: 0.00922 
2023-03-21 12:19:26.461429: train_loss -0.889 
2023-03-21 12:19:26.461866: val_loss -0.8467 
2023-03-21 12:19:26.462098: Pseudo dice [0.8969, 0.8766] 
2023-03-21 12:19:26.462319: Epoch time: 13.99 s 
2023-03-21 12:19:27.692734:  
2023-03-21 12:19:27.692885: Epoch 87 
2023-03-21 12:19:27.693046: Current learning rate: 0.00921 
2023-03-21 12:19:41.647469: train_loss -0.8888 
2023-03-21 12:19:41.647861: val_loss -0.8445 
2023-03-21 12:19:41.648097: Pseudo dice [0.8955, 0.876] 
2023-03-21 12:19:41.648300: Epoch time: 13.96 s 
2023-03-21 12:19:42.863502:  
2023-03-21 12:19:42.863667: Epoch 88 
2023-03-21 12:19:42.863834: Current learning rate: 0.0092 
2023-03-21 12:19:56.929542: train_loss -0.8886 
2023-03-21 12:19:56.929967: val_loss -0.8474 
2023-03-21 12:19:56.930087: Pseudo dice [0.8977, 0.877] 
2023-03-21 12:19:56.930201: Epoch time: 14.07 s 
2023-03-21 12:19:58.142439:  
2023-03-21 12:19:58.142606: Epoch 89 
2023-03-21 12:19:58.142786: Current learning rate: 0.0092 
2023-03-21 12:20:11.976986: train_loss -0.8902 
2023-03-21 12:20:11.977434: val_loss -0.8434 
2023-03-21 12:20:11.977687: Pseudo dice [0.8942, 0.8745] 
2023-03-21 12:20:11.977977: Epoch time: 13.84 s 
2023-03-21 12:20:13.172696:  
2023-03-21 12:20:13.172941: Epoch 90 
2023-03-21 12:20:13.173077: Current learning rate: 0.00919 
2023-03-21 12:20:26.857703: train_loss -0.8891 
2023-03-21 12:20:26.858115: val_loss -0.8478 
2023-03-21 12:20:26.858218: Pseudo dice [0.8958, 0.88] 
2023-03-21 12:20:26.858336: Epoch time: 13.69 s 
2023-03-21 12:20:28.210640:  
2023-03-21 12:20:28.210846: Epoch 91 
2023-03-21 12:20:28.211029: Current learning rate: 0.00918 
2023-03-21 12:20:41.946139: train_loss -0.8905 
2023-03-21 12:20:41.946413: val_loss -0.8459 
2023-03-21 12:20:41.946517: Pseudo dice [0.8948, 0.8787] 
2023-03-21 12:20:41.946638: Epoch time: 13.74 s 
2023-03-21 12:20:43.147545:  
2023-03-21 12:20:43.148041: Epoch 92 
2023-03-21 12:20:43.148196: Current learning rate: 0.00917 
2023-03-21 12:20:57.208321: train_loss -0.8898 
2023-03-21 12:20:57.208663: val_loss -0.8471 
2023-03-21 12:20:57.208810: Pseudo dice [0.8975, 0.8775] 
2023-03-21 12:20:57.208947: Epoch time: 14.06 s 
2023-03-21 12:20:58.456752:  
2023-03-21 12:20:58.457046: Epoch 93 
2023-03-21 12:20:58.457182: Current learning rate: 0.00916 
2023-03-21 12:21:12.199885: train_loss -0.8896 
2023-03-21 12:21:12.200185: val_loss -0.8441 
2023-03-21 12:21:12.200345: Pseudo dice [0.8929, 0.8779] 
2023-03-21 12:21:12.200469: Epoch time: 13.74 s 
2023-03-21 12:21:13.376324:  
2023-03-21 12:21:13.376498: Epoch 94 
2023-03-21 12:21:13.376684: Current learning rate: 0.00915 
2023-03-21 12:21:27.537440: train_loss -0.8905 
2023-03-21 12:21:27.537806: val_loss -0.8449 
2023-03-21 12:21:27.537920: Pseudo dice [0.8957, 0.8761] 
2023-03-21 12:21:27.538041: Epoch time: 14.16 s 
2023-03-21 12:21:28.714186:  
2023-03-21 12:21:28.714330: Epoch 95 
2023-03-21 12:21:28.714500: Current learning rate: 0.00914 
2023-03-21 12:21:42.571464: train_loss -0.8898 
2023-03-21 12:21:42.571929: val_loss -0.8473 
2023-03-21 12:21:42.572133: Pseudo dice [0.897, 0.879] 
2023-03-21 12:21:42.572348: Epoch time: 13.86 s 
2023-03-21 12:21:43.895619:  
2023-03-21 12:21:43.895792: Epoch 96 
2023-03-21 12:21:43.895962: Current learning rate: 0.00913 
2023-03-21 12:21:57.854290: train_loss -0.8906 
2023-03-21 12:21:57.854678: val_loss -0.8416 
2023-03-21 12:21:57.854938: Pseudo dice [0.8935, 0.8753] 
2023-03-21 12:21:57.855133: Epoch time: 13.96 s 
2023-03-21 12:21:59.062574:  
2023-03-21 12:21:59.062736: Epoch 97 
2023-03-21 12:21:59.062925: Current learning rate: 0.00912 
2023-03-21 12:22:12.850121: train_loss -0.8905 
2023-03-21 12:22:12.850394: val_loss -0.8394 
2023-03-21 12:22:12.850503: Pseudo dice [0.8916, 0.874] 
2023-03-21 12:22:12.850612: Epoch time: 13.79 s 
2023-03-21 12:22:14.117650:  
2023-03-21 12:22:14.117822: Epoch 98 
2023-03-21 12:22:14.118000: Current learning rate: 0.00911 
2023-03-21 12:22:27.977292: train_loss -0.8899 
2023-03-21 12:22:27.978318: val_loss -0.8462 
2023-03-21 12:22:27.978431: Pseudo dice [0.8954, 0.8805] 
2023-03-21 12:22:27.978553: Epoch time: 13.86 s 
2023-03-21 12:22:29.169942:  
2023-03-21 12:22:29.170134: Epoch 99 
2023-03-21 12:22:29.170301: Current learning rate: 0.0091 
2023-03-21 12:22:43.012856: train_loss -0.8911 
2023-03-21 12:22:43.013148: val_loss -0.8461 
2023-03-21 12:22:43.013253: Pseudo dice [0.8958, 0.8768] 
2023-03-21 12:22:43.013373: Epoch time: 13.84 s 
2023-03-21 12:22:44.479780:  
2023-03-21 12:22:44.479941: Epoch 100 
2023-03-21 12:22:44.480091: Current learning rate: 0.0091 
2023-03-21 12:22:58.495252: train_loss -0.8901 
2023-03-21 12:22:58.495532: val_loss -0.8502 
2023-03-21 12:22:58.495647: Pseudo dice [0.8982, 0.8821] 
2023-03-21 12:22:58.495769: Epoch time: 14.02 s 
2023-03-21 12:22:59.857192:  
2023-03-21 12:22:59.857522: Epoch 101 
2023-03-21 12:22:59.857708: Current learning rate: 0.00909 
2023-03-21 12:23:13.692857: train_loss -0.8918 
2023-03-21 12:23:13.693139: val_loss -0.8436 
2023-03-21 12:23:13.693249: Pseudo dice [0.8924, 0.876] 
2023-03-21 12:23:13.693364: Epoch time: 13.84 s 
2023-03-21 12:23:14.866548:  
2023-03-21 12:23:14.866718: Epoch 102 
2023-03-21 12:23:14.866924: Current learning rate: 0.00908 
2023-03-21 12:23:28.773505: train_loss -0.8925 
2023-03-21 12:23:28.773770: val_loss -0.8506 
2023-03-21 12:23:28.773877: Pseudo dice [0.8992, 0.8807] 
2023-03-21 12:23:28.773997: Epoch time: 13.91 s 
2023-03-21 12:23:29.992821:  
2023-03-21 12:23:29.993161: Epoch 103 
2023-03-21 12:23:29.993326: Current learning rate: 0.00907 
2023-03-21 12:23:43.888034: train_loss -0.8943 
2023-03-21 12:23:43.888312: val_loss -0.8441 
2023-03-21 12:23:43.888419: Pseudo dice [0.8952, 0.8764] 
2023-03-21 12:23:43.888526: Epoch time: 13.9 s 
2023-03-21 12:23:45.067920:  
2023-03-21 12:23:45.068068: Epoch 104 
2023-03-21 12:23:45.068194: Current learning rate: 0.00906 
2023-03-21 12:23:59.032734: train_loss -0.8933 
2023-03-21 12:23:59.033040: val_loss -0.8457 
2023-03-21 12:23:59.033160: Pseudo dice [0.895, 0.8803] 
2023-03-21 12:23:59.033269: Epoch time: 13.97 s 
2023-03-21 12:24:00.276423:  
2023-03-21 12:24:00.276587: Epoch 105 
2023-03-21 12:24:00.276739: Current learning rate: 0.00905 
2023-03-21 12:24:14.259990: train_loss -0.8926 
2023-03-21 12:24:14.260271: val_loss -0.8482 
2023-03-21 12:24:14.260385: Pseudo dice [0.8985, 0.8768] 
2023-03-21 12:24:14.260491: Epoch time: 13.98 s 
2023-03-21 12:24:15.665378:  
2023-03-21 12:24:15.665694: Epoch 106 
2023-03-21 12:24:15.665853: Current learning rate: 0.00904 
2023-03-21 12:24:29.547626: train_loss -0.8934 
2023-03-21 12:24:29.547956: val_loss -0.844 
2023-03-21 12:24:29.548090: Pseudo dice [0.8959, 0.8768] 
2023-03-21 12:24:29.548220: Epoch time: 13.88 s 
2023-03-21 12:24:30.737580:  
2023-03-21 12:24:30.737780: Epoch 107 
2023-03-21 12:24:30.737945: Current learning rate: 0.00903 
2023-03-21 12:24:44.394396: train_loss -0.8944 
2023-03-21 12:24:44.395218: val_loss -0.8414 
2023-03-21 12:24:44.395464: Pseudo dice [0.8939, 0.8743] 
2023-03-21 12:24:44.395611: Epoch time: 13.66 s 
2023-03-21 12:24:45.628994:  
2023-03-21 12:24:45.629197: Epoch 108 
2023-03-21 12:24:45.629334: Current learning rate: 0.00902 
2023-03-21 12:24:59.416631: train_loss -0.8919 
2023-03-21 12:24:59.416976: val_loss -0.8466 
2023-03-21 12:24:59.417113: Pseudo dice [0.8965, 0.879] 
2023-03-21 12:24:59.417245: Epoch time: 13.79 s 
2023-03-21 12:25:00.644939:  
2023-03-21 12:25:00.645085: Epoch 109 
2023-03-21 12:25:00.645229: Current learning rate: 0.00901 
2023-03-21 12:25:14.403860: train_loss -0.8954 
2023-03-21 12:25:14.404118: val_loss -0.8488 
2023-03-21 12:25:14.404222: Pseudo dice [0.8977, 0.8803] 
2023-03-21 12:25:14.404323: Epoch time: 13.76 s 
2023-03-21 12:25:15.631346:  
2023-03-21 12:25:15.631485: Epoch 110 
2023-03-21 12:25:15.631629: Current learning rate: 0.009 
2023-03-21 12:25:29.248407: train_loss -0.8932 
2023-03-21 12:25:29.248664: val_loss -0.8487 
2023-03-21 12:25:29.248765: Pseudo dice [0.8979, 0.8807] 
2023-03-21 12:25:29.248866: Epoch time: 13.62 s 
2023-03-21 12:25:30.564998:  
2023-03-21 12:25:30.565150: Epoch 111 
2023-03-21 12:25:30.565305: Current learning rate: 0.009 
2023-03-21 12:25:44.305928: train_loss -0.8939 
2023-03-21 12:25:44.307050: val_loss -0.8429 
2023-03-21 12:25:44.307176: Pseudo dice [0.8939, 0.8784] 
2023-03-21 12:25:44.307285: Epoch time: 13.74 s 
2023-03-21 12:25:45.526914:  
2023-03-21 12:25:45.527082: Epoch 112 
2023-03-21 12:25:45.527216: Current learning rate: 0.00899 
2023-03-21 12:25:59.124155: train_loss -0.8949 
2023-03-21 12:25:59.124458: val_loss -0.8453 
2023-03-21 12:25:59.124570: Pseudo dice [0.8975, 0.8787] 
2023-03-21 12:25:59.124684: Epoch time: 13.6 s 
2023-03-21 12:26:00.343114:  
2023-03-21 12:26:00.343253: Epoch 113 
2023-03-21 12:26:00.343398: Current learning rate: 0.00898 
2023-03-21 12:26:13.868569: train_loss -0.8954 
2023-03-21 12:26:13.868851: val_loss -0.8462 
2023-03-21 12:26:13.868960: Pseudo dice [0.8957, 0.8792] 
2023-03-21 12:26:13.869068: Epoch time: 13.53 s 
2023-03-21 12:26:15.077637:  
2023-03-21 12:26:15.077955: Epoch 114 
2023-03-21 12:26:15.078197: Current learning rate: 0.00897 
2023-03-21 12:26:28.702064: train_loss -0.895 
2023-03-21 12:26:28.702429: val_loss -0.8507 
2023-03-21 12:26:28.702562: Pseudo dice [0.9003, 0.8806] 
2023-03-21 12:26:28.702694: Epoch time: 13.63 s 
2023-03-21 12:26:28.702822: Yayy! New best EMA pseudo Dice: 0.8875 
2023-03-21 12:26:30.193929:  
2023-03-21 12:26:30.194182: Epoch 115 
2023-03-21 12:26:30.194388: Current learning rate: 0.00896 
2023-03-21 12:26:43.743288: train_loss -0.894 
2023-03-21 12:26:43.743501: val_loss -0.8447 
2023-03-21 12:26:43.743606: Pseudo dice [0.8961, 0.8776] 
2023-03-21 12:26:43.743705: Epoch time: 13.55 s 
2023-03-21 12:26:45.161933:  
2023-03-21 12:26:45.162095: Epoch 116 
2023-03-21 12:26:45.162242: Current learning rate: 0.00895 
2023-03-21 12:26:58.671001: train_loss -0.8938 
2023-03-21 12:26:58.671297: val_loss -0.8453 
2023-03-21 12:26:58.671424: Pseudo dice [0.8969, 0.8794] 
2023-03-21 12:26:58.671537: Epoch time: 13.51 s 
2023-03-21 12:26:58.671628: Yayy! New best EMA pseudo Dice: 0.8875 
2023-03-21 12:27:00.258858:  
2023-03-21 12:27:00.258995: Epoch 117 
2023-03-21 12:27:00.259140: Current learning rate: 0.00894 
2023-03-21 12:27:14.193981: train_loss -0.8937 
2023-03-21 12:27:14.194408: val_loss -0.8472 
2023-03-21 12:27:14.194613: Pseudo dice [0.8976, 0.8786] 
2023-03-21 12:27:14.194802: Epoch time: 13.94 s 
2023-03-21 12:27:14.194952: Yayy! New best EMA pseudo Dice: 0.8875 
2023-03-21 12:27:15.660396:  
2023-03-21 12:27:15.660615: Epoch 118 
2023-03-21 12:27:15.660810: Current learning rate: 0.00893 
2023-03-21 12:27:29.258505: train_loss -0.8961 
2023-03-21 12:27:29.258800: val_loss -0.8499 
2023-03-21 12:27:29.258914: Pseudo dice [0.898, 0.8817] 
2023-03-21 12:27:29.259027: Epoch time: 13.6 s 
2023-03-21 12:27:29.259148: Yayy! New best EMA pseudo Dice: 0.8878 
2023-03-21 12:27:30.786223:  
2023-03-21 12:27:30.786367: Epoch 119 
2023-03-21 12:27:30.786530: Current learning rate: 0.00892 
2023-03-21 12:27:44.420118: train_loss -0.8963 
2023-03-21 12:27:44.420381: val_loss -0.8497 
2023-03-21 12:27:44.420481: Pseudo dice [0.8984, 0.8819] 
2023-03-21 12:27:44.420581: Epoch time: 13.63 s 
2023-03-21 12:27:44.420662: Yayy! New best EMA pseudo Dice: 0.888 
2023-03-21 12:27:45.993143:  
2023-03-21 12:27:45.993300: Epoch 120 
2023-03-21 12:27:45.993448: Current learning rate: 0.00891 
2023-03-21 12:27:59.411800: train_loss -0.8959 
2023-03-21 12:27:59.412070: val_loss -0.8456 
2023-03-21 12:27:59.412172: Pseudo dice [0.8958, 0.8791] 
2023-03-21 12:27:59.412272: Epoch time: 13.42 s 
2023-03-21 12:28:00.585931:  
2023-03-21 12:28:00.586085: Epoch 121 
2023-03-21 12:28:00.586234: Current learning rate: 0.0089 
2023-03-21 12:28:14.241096: train_loss -0.8963 
2023-03-21 12:28:14.241349: val_loss -0.8423 
2023-03-21 12:28:14.241459: Pseudo dice [0.8955, 0.875] 
2023-03-21 12:28:14.241567: Epoch time: 13.66 s 
2023-03-21 12:28:15.477910:  
2023-03-21 12:28:15.478156: Epoch 122 
2023-03-21 12:28:15.478418: Current learning rate: 0.00889 
2023-03-21 12:28:29.215345: train_loss -0.8963 
2023-03-21 12:28:29.215658: val_loss -0.845 
2023-03-21 12:28:29.215765: Pseudo dice [0.8963, 0.8793] 
2023-03-21 12:28:29.215869: Epoch time: 13.74 s 
2023-03-21 12:28:30.436313:  
2023-03-21 12:28:30.436502: Epoch 123 
2023-03-21 12:28:30.436656: Current learning rate: 0.00889 
2023-03-21 12:28:44.362360: train_loss -0.8953 
2023-03-21 12:28:44.362647: val_loss -0.8463 
2023-03-21 12:28:44.362781: Pseudo dice [0.8969, 0.8784] 
2023-03-21 12:28:44.362894: Epoch time: 13.93 s 
2023-03-21 12:28:45.540214:  
2023-03-21 12:28:45.540353: Epoch 124 
2023-03-21 12:28:45.540498: Current learning rate: 0.00888 
2023-03-21 12:28:59.215450: train_loss -0.8975 
2023-03-21 12:28:59.215728: val_loss -0.8466 
2023-03-21 12:28:59.215831: Pseudo dice [0.8979, 0.8788] 
2023-03-21 12:28:59.215934: Epoch time: 13.68 s 
2023-03-21 12:29:00.550692:  
2023-03-21 12:29:00.550900: Epoch 125 
2023-03-21 12:29:00.551050: Current learning rate: 0.00887 
2023-03-21 12:29:14.256445: train_loss -0.8967 
2023-03-21 12:29:14.256716: val_loss -0.8471 
2023-03-21 12:29:14.256837: Pseudo dice [0.8979, 0.8798] 
2023-03-21 12:29:14.256971: Epoch time: 13.71 s 
2023-03-21 12:29:15.489666:  
2023-03-21 12:29:15.489817: Epoch 126 
2023-03-21 12:29:15.489959: Current learning rate: 0.00886 
2023-03-21 12:29:29.389842: train_loss -0.8976 
2023-03-21 12:29:29.390134: val_loss -0.8461 
2023-03-21 12:29:29.390271: Pseudo dice [0.8972, 0.8789] 
2023-03-21 12:29:29.390382: Epoch time: 13.9 s 
2023-03-21 12:29:30.601832:  
2023-03-21 12:29:30.601995: Epoch 127 
2023-03-21 12:29:30.602170: Current learning rate: 0.00885 
2023-03-21 12:29:44.551399: train_loss -0.8971 
2023-03-21 12:29:44.551689: val_loss -0.8439 
2023-03-21 12:29:44.551792: Pseudo dice [0.8945, 0.8751] 
2023-03-21 12:29:44.551895: Epoch time: 13.95 s 
2023-03-21 12:29:45.743394:  
2023-03-21 12:29:45.743559: Epoch 128 
2023-03-21 12:29:45.743691: Current learning rate: 0.00884 
2023-03-21 12:29:59.204296: train_loss -0.8986 
2023-03-21 12:29:59.204580: val_loss -0.8449 
2023-03-21 12:29:59.204685: Pseudo dice [0.8962, 0.8776] 
2023-03-21 12:29:59.204794: Epoch time: 13.46 s 
2023-03-21 12:30:00.398979:  
2023-03-21 12:30:00.399134: Epoch 129 
2023-03-21 12:30:00.399282: Current learning rate: 0.00883 
2023-03-21 12:30:14.140966: train_loss -0.897 
2023-03-21 12:30:14.141227: val_loss -0.8448 
2023-03-21 12:30:14.141329: Pseudo dice [0.8952, 0.8792] 
2023-03-21 12:30:14.141429: Epoch time: 13.74 s 
2023-03-21 12:30:15.480279:  
2023-03-21 12:30:15.480442: Epoch 130 
2023-03-21 12:30:15.480595: Current learning rate: 0.00882 
2023-03-21 12:30:29.147617: train_loss -0.8966 
2023-03-21 12:30:29.148094: val_loss -0.8494 
2023-03-21 12:30:29.148318: Pseudo dice [0.9004, 0.8807] 
2023-03-21 12:30:29.148449: Epoch time: 13.67 s 
2023-03-21 12:30:30.429505:  
2023-03-21 12:30:30.429659: Epoch 131 
2023-03-21 12:30:30.429809: Current learning rate: 0.00881 
2023-03-21 12:30:44.235831: train_loss -0.8985 
2023-03-21 12:30:44.236107: val_loss -0.8408 
2023-03-21 12:30:44.236213: Pseudo dice [0.8928, 0.8754] 
2023-03-21 12:30:44.236320: Epoch time: 13.81 s 
2023-03-21 12:30:45.436024:  
2023-03-21 12:30:45.436175: Epoch 132 
2023-03-21 12:30:45.436321: Current learning rate: 0.0088 
2023-03-21 12:30:59.171949: train_loss -0.8964 
2023-03-21 12:30:59.172216: val_loss -0.8445 
2023-03-21 12:30:59.172328: Pseudo dice [0.8939, 0.878] 
2023-03-21 12:30:59.172434: Epoch time: 13.74 s 
2023-03-21 12:31:00.482937:  
2023-03-21 12:31:00.483084: Epoch 133 
2023-03-21 12:31:00.483215: Current learning rate: 0.00879 
2023-03-21 12:31:14.152848: train_loss -0.8973 
2023-03-21 12:31:14.153329: val_loss -0.8468 
2023-03-21 12:31:14.153535: Pseudo dice [0.897, 0.8785] 
2023-03-21 12:31:14.153867: Epoch time: 13.67 s 
2023-03-21 12:31:15.445280:  
2023-03-21 12:31:15.445431: Epoch 134 
2023-03-21 12:31:15.445611: Current learning rate: 0.00879 
2023-03-21 12:31:29.233331: train_loss -0.8971 
2023-03-21 12:31:29.234519: val_loss -0.8451 
2023-03-21 12:31:29.234655: Pseudo dice [0.8976, 0.8777] 
2023-03-21 12:31:29.234828: Epoch time: 13.79 s 
2023-03-21 12:31:30.607003:  
2023-03-21 12:31:30.607160: Epoch 135 
2023-03-21 12:31:30.607327: Current learning rate: 0.00878 
2023-03-21 12:31:44.076041: train_loss -0.8972 
2023-03-21 12:31:44.076519: val_loss -0.8447 
2023-03-21 12:31:44.076722: Pseudo dice [0.8955, 0.8791] 
2023-03-21 12:31:44.076916: Epoch time: 13.47 s 
2023-03-21 12:31:45.431957:  
2023-03-21 12:31:45.432111: Epoch 136 
2023-03-21 12:31:45.432260: Current learning rate: 0.00877 
2023-03-21 12:31:59.384702: train_loss -0.8973 
2023-03-21 12:31:59.385029: val_loss -0.8453 
2023-03-21 12:31:59.385135: Pseudo dice [0.896, 0.8776] 
2023-03-21 12:31:59.385260: Epoch time: 13.95 s 
2023-03-21 12:32:00.599443:  
2023-03-21 12:32:00.599597: Epoch 137 
2023-03-21 12:32:00.599762: Current learning rate: 0.00876 
2023-03-21 12:32:14.443921: train_loss -0.897 
2023-03-21 12:32:14.444208: val_loss -0.843 
2023-03-21 12:32:14.444312: Pseudo dice [0.8951, 0.8763] 
2023-03-21 12:32:14.444421: Epoch time: 13.85 s 
2023-03-21 12:32:15.729115:  
2023-03-21 12:32:15.729276: Epoch 138 
2023-03-21 12:32:15.729414: Current learning rate: 0.00875 
2023-03-21 12:32:29.765280: train_loss -0.8967 
2023-03-21 12:32:29.765776: val_loss -0.845 
2023-03-21 12:32:29.765880: Pseudo dice [0.8946, 0.879] 
2023-03-21 12:32:29.766124: Epoch time: 14.04 s 
2023-03-21 12:32:31.089503:  
2023-03-21 12:32:31.089648: Epoch 139 
2023-03-21 12:32:31.089813: Current learning rate: 0.00874 
2023-03-21 12:32:45.031436: train_loss -0.8978 
2023-03-21 12:32:45.031654: val_loss -0.8482 
2023-03-21 12:32:45.031756: Pseudo dice [0.8982, 0.8793] 
2023-03-21 12:32:45.031852: Epoch time: 13.94 s 
2023-03-21 12:32:46.240101:  
2023-03-21 12:32:46.240241: Epoch 140 
2023-03-21 12:32:46.240386: Current learning rate: 0.00873 
2023-03-21 12:32:59.749975: train_loss -0.8965 
2023-03-21 12:32:59.750249: val_loss -0.8458 
2023-03-21 12:32:59.750359: Pseudo dice [0.8987, 0.8771] 
2023-03-21 12:32:59.750467: Epoch time: 13.51 s 
2023-03-21 12:33:00.964345:  
2023-03-21 12:33:00.964482: Epoch 141 
2023-03-21 12:33:00.964628: Current learning rate: 0.00872 
2023-03-21 12:33:14.757937: train_loss -0.8981 
2023-03-21 12:33:14.758224: val_loss -0.8447 
2023-03-21 12:33:14.758330: Pseudo dice [0.8968, 0.8775] 
2023-03-21 12:33:14.758435: Epoch time: 13.79 s 
2023-03-21 12:33:15.989059:  
2023-03-21 12:33:15.989211: Epoch 142 
2023-03-21 12:33:15.989364: Current learning rate: 0.00871 
2023-03-21 12:33:29.975325: train_loss -0.8971 
2023-03-21 12:33:29.975620: val_loss -0.849 
2023-03-21 12:33:29.975744: Pseudo dice [0.8982, 0.8806] 
2023-03-21 12:33:29.975860: Epoch time: 13.99 s 
2023-03-21 12:33:31.207510:  
2023-03-21 12:33:31.207665: Epoch 143 
2023-03-21 12:33:31.207832: Current learning rate: 0.0087 
2023-03-21 12:33:44.828660: train_loss -0.8993 
2023-03-21 12:33:44.828943: val_loss -0.8451 
2023-03-21 12:33:44.829046: Pseudo dice [0.8976, 0.8771] 
2023-03-21 12:33:44.829148: Epoch time: 13.62 s 
2023-03-21 12:33:46.272556:  
2023-03-21 12:33:46.272723: Epoch 144 
2023-03-21 12:33:46.272874: Current learning rate: 0.00869 
2023-03-21 12:33:59.847203: train_loss -0.8993 
2023-03-21 12:33:59.847589: val_loss -0.8491 
2023-03-21 12:33:59.847958: Pseudo dice [0.8993, 0.8813] 
2023-03-21 12:33:59.848074: Epoch time: 13.58 s 
2023-03-21 12:34:01.041224:  
2023-03-21 12:34:01.041384: Epoch 145 
2023-03-21 12:34:01.041521: Current learning rate: 0.00868 
2023-03-21 12:34:14.922953: train_loss -0.8972 
2023-03-21 12:34:14.923317: val_loss -0.842 
2023-03-21 12:34:14.923439: Pseudo dice [0.894, 0.8764] 
2023-03-21 12:34:14.923548: Epoch time: 13.88 s 
2023-03-21 12:34:16.107123:  
2023-03-21 12:34:16.107270: Epoch 146 
2023-03-21 12:34:16.107415: Current learning rate: 0.00868 
2023-03-21 12:34:29.779646: train_loss -0.8985 
2023-03-21 12:34:29.779879: val_loss -0.8461 
2023-03-21 12:34:29.779989: Pseudo dice [0.8959, 0.8789] 
2023-03-21 12:34:29.780094: Epoch time: 13.67 s 
2023-03-21 12:34:31.039747:  
2023-03-21 12:34:31.039917: Epoch 147 
2023-03-21 12:34:31.040072: Current learning rate: 0.00867 
2023-03-21 12:34:44.801811: train_loss -0.8984 
2023-03-21 12:34:44.802159: val_loss -0.8482 
2023-03-21 12:34:44.802290: Pseudo dice [0.8978, 0.8816] 
2023-03-21 12:34:44.802418: Epoch time: 13.76 s 
2023-03-21 12:34:45.983186:  
2023-03-21 12:34:45.983323: Epoch 148 
2023-03-21 12:34:45.983470: Current learning rate: 0.00866 
2023-03-21 12:34:59.842638: train_loss -0.8986 
2023-03-21 12:34:59.842958: val_loss -0.8412 
2023-03-21 12:34:59.843089: Pseudo dice [0.8935, 0.8744] 
2023-03-21 12:34:59.843227: Epoch time: 13.86 s 
2023-03-21 12:35:01.255818:  
2023-03-21 12:35:01.255997: Epoch 149 
2023-03-21 12:35:01.256166: Current learning rate: 0.00865 
2023-03-21 12:35:14.781860: train_loss -0.8986 
2023-03-21 12:35:14.782298: val_loss -0.8467 
2023-03-21 12:35:14.782521: Pseudo dice [0.8984, 0.879] 
2023-03-21 12:35:14.784143: Epoch time: 13.53 s 
2023-03-21 12:35:16.281290:  
2023-03-21 12:35:16.281526: Epoch 150 
2023-03-21 12:35:16.281678: Current learning rate: 0.00864 
2023-03-21 12:35:30.067408: train_loss -0.8991 
2023-03-21 12:35:30.067727: val_loss -0.8467 
2023-03-21 12:35:30.067857: Pseudo dice [0.8987, 0.8806] 
2023-03-21 12:35:30.067982: Epoch time: 13.79 s 
2023-03-21 12:35:31.303612:  
2023-03-21 12:35:31.303756: Epoch 151 
2023-03-21 12:35:31.303901: Current learning rate: 0.00863 
2023-03-21 12:35:44.849004: train_loss -0.9 
2023-03-21 12:35:44.849433: val_loss -0.8444 
2023-03-21 12:35:44.849830: Pseudo dice [0.8957, 0.8761] 
2023-03-21 12:35:44.850083: Epoch time: 13.55 s 
2023-03-21 12:35:46.125624:  
2023-03-21 12:35:46.125776: Epoch 152 
2023-03-21 12:35:46.125922: Current learning rate: 0.00862 
2023-03-21 12:35:59.892967: train_loss -0.8993 
2023-03-21 12:35:59.893341: val_loss -0.8442 
2023-03-21 12:35:59.893487: Pseudo dice [0.8959, 0.8769] 
2023-03-21 12:35:59.893657: Epoch time: 13.77 s 
2023-03-21 12:36:01.215523:  
2023-03-21 12:36:01.215667: Epoch 153 
2023-03-21 12:36:01.215819: Current learning rate: 0.00861 
2023-03-21 12:36:15.011060: train_loss -0.8987 
2023-03-21 12:36:15.011354: val_loss -0.8438 
2023-03-21 12:36:15.011466: Pseudo dice [0.8951, 0.8775] 
2023-03-21 12:36:15.011574: Epoch time: 13.8 s 
2023-03-21 12:36:16.245891:  
2023-03-21 12:36:16.246044: Epoch 154 
2023-03-21 12:36:16.246190: Current learning rate: 0.0086 
2023-03-21 12:36:30.012940: train_loss -0.899 
2023-03-21 12:36:30.013221: val_loss -0.8395 
2023-03-21 12:36:30.013326: Pseudo dice [0.8931, 0.872] 
2023-03-21 12:36:30.013431: Epoch time: 13.77 s 
2023-03-21 12:36:31.254870:  
2023-03-21 12:36:31.255022: Epoch 155 
2023-03-21 12:36:31.255170: Current learning rate: 0.00859 
2023-03-21 12:36:44.727808: train_loss -0.8994 
2023-03-21 12:36:44.728107: val_loss -0.8474 
2023-03-21 12:36:44.728213: Pseudo dice [0.8961, 0.8795] 
2023-03-21 12:36:44.728318: Epoch time: 13.47 s 
2023-03-21 12:36:45.950215:  
2023-03-21 12:36:45.950386: Epoch 156 
2023-03-21 12:36:45.950531: Current learning rate: 0.00858 
2023-03-21 12:36:59.465957: train_loss -0.9003 
2023-03-21 12:36:59.466273: val_loss -0.8402 
2023-03-21 12:36:59.466409: Pseudo dice [0.8931, 0.8749] 
2023-03-21 12:36:59.466539: Epoch time: 13.52 s 
2023-03-21 12:37:00.725840:  
2023-03-21 12:37:00.725990: Epoch 157 
2023-03-21 12:37:00.726165: Current learning rate: 0.00858 
2023-03-21 12:37:14.606695: train_loss -0.9001 
2023-03-21 12:37:14.607399: val_loss -0.8436 
2023-03-21 12:37:14.607598: Pseudo dice [0.8939, 0.8788] 
2023-03-21 12:37:14.607841: Epoch time: 13.88 s 
2023-03-21 12:37:15.964372:  
2023-03-21 12:37:15.964543: Epoch 158 
2023-03-21 12:37:15.964691: Current learning rate: 0.00857 
2023-03-21 12:37:29.693333: train_loss -0.9 
2023-03-21 12:37:29.693616: val_loss -0.8489 
2023-03-21 12:37:29.693721: Pseudo dice [0.9003, 0.8807] 
2023-03-21 12:37:29.693822: Epoch time: 13.73 s 
2023-03-21 12:37:30.921275:  
2023-03-21 12:37:30.921429: Epoch 159 
2023-03-21 12:37:30.921574: Current learning rate: 0.00856 
2023-03-21 12:37:44.550215: train_loss -0.9007 
2023-03-21 12:37:44.550542: val_loss -0.8452 
2023-03-21 12:37:44.550662: Pseudo dice [0.8968, 0.8784] 
2023-03-21 12:37:44.550822: Epoch time: 13.63 s 
2023-03-21 12:37:45.808219:  
2023-03-21 12:37:45.808366: Epoch 160 
2023-03-21 12:37:45.808513: Current learning rate: 0.00855 
2023-03-21 12:37:59.356673: train_loss -0.8998 
2023-03-21 12:37:59.356979: val_loss -0.849 
2023-03-21 12:37:59.357119: Pseudo dice [0.9002, 0.88] 
2023-03-21 12:37:59.357241: Epoch time: 13.55 s 
2023-03-21 12:38:00.599725:  
2023-03-21 12:38:00.599872: Epoch 161 
2023-03-21 12:38:00.600020: Current learning rate: 0.00854 
2023-03-21 12:38:14.033116: train_loss -0.9003 
2023-03-21 12:38:14.033393: val_loss -0.8486 
2023-03-21 12:38:14.033499: Pseudo dice [0.8983, 0.8812] 
2023-03-21 12:38:14.033605: Epoch time: 13.43 s 
2023-03-21 12:38:15.274866:  
2023-03-21 12:38:15.275013: Epoch 162 
2023-03-21 12:38:15.275159: Current learning rate: 0.00853 
2023-03-21 12:38:28.874558: train_loss -0.9008 
2023-03-21 12:38:28.874852: val_loss -0.8464 
2023-03-21 12:38:28.874978: Pseudo dice [0.8959, 0.8797] 
2023-03-21 12:38:28.875087: Epoch time: 13.6 s 
2023-03-21 12:38:30.271875:  
2023-03-21 12:38:30.272026: Epoch 163 
2023-03-21 12:38:30.272166: Current learning rate: 0.00852 
2023-03-21 12:38:44.232835: train_loss -0.9003 
2023-03-21 12:38:44.233104: val_loss -0.8438 
2023-03-21 12:38:44.233208: Pseudo dice [0.8954, 0.8773] 
2023-03-21 12:38:44.233312: Epoch time: 13.96 s 
2023-03-21 12:38:45.457224:  
2023-03-21 12:38:45.457435: Epoch 164 
2023-03-21 12:38:45.457573: Current learning rate: 0.00851 
2023-03-21 12:38:58.814484: train_loss -0.8993 
2023-03-21 12:38:58.814970: val_loss -0.8425 
2023-03-21 12:38:58.815415: Pseudo dice [0.8936, 0.879] 
2023-03-21 12:38:58.815593: Epoch time: 13.36 s 
2023-03-21 12:39:00.012344:  
2023-03-21 12:39:00.012500: Epoch 165 
2023-03-21 12:39:00.012644: Current learning rate: 0.0085 
2023-03-21 12:39:13.736626: train_loss -0.8998 
2023-03-21 12:39:13.736917: val_loss -0.8422 
2023-03-21 12:39:13.737023: Pseudo dice [0.8946, 0.8783] 
2023-03-21 12:39:13.737129: Epoch time: 13.72 s 
2023-03-21 12:39:14.952037:  
2023-03-21 12:39:14.952251: Epoch 166 
2023-03-21 12:39:14.952388: Current learning rate: 0.00849 
2023-03-21 12:39:28.655186: train_loss -0.9012 
2023-03-21 12:39:28.655488: val_loss -0.8488 
2023-03-21 12:39:28.655603: Pseudo dice [0.8997, 0.8807] 
2023-03-21 12:39:28.655884: Epoch time: 13.7 s 
2023-03-21 12:39:29.885316:  
2023-03-21 12:39:29.885469: Epoch 167 
2023-03-21 12:39:29.885620: Current learning rate: 0.00848 
2023-03-21 12:39:43.542512: train_loss -0.8993 
2023-03-21 12:39:43.542833: val_loss -0.8487 
2023-03-21 12:39:43.542951: Pseudo dice [0.8995, 0.8795] 
2023-03-21 12:39:43.543065: Epoch time: 13.66 s 
2023-03-21 12:39:44.918274:  
2023-03-21 12:39:44.918416: Epoch 168 
2023-03-21 12:39:44.918563: Current learning rate: 0.00847 
2023-03-21 12:39:58.614815: train_loss -0.9011 
2023-03-21 12:39:58.615092: val_loss -0.8418 
2023-03-21 12:39:58.615199: Pseudo dice [0.8944, 0.8753] 
2023-03-21 12:39:58.615306: Epoch time: 13.7 s 
2023-03-21 12:39:59.906469:  
2023-03-21 12:39:59.906623: Epoch 169 
2023-03-21 12:39:59.906808: Current learning rate: 0.00847 
2023-03-21 12:40:13.619041: train_loss -0.8997 
2023-03-21 12:40:13.619461: val_loss -0.8411 
2023-03-21 12:40:13.619648: Pseudo dice [0.8919, 0.8772] 
2023-03-21 12:40:13.619824: Epoch time: 13.71 s 
2023-03-21 12:40:14.839661:  
2023-03-21 12:40:14.839816: Epoch 170 
2023-03-21 12:40:14.839947: Current learning rate: 0.00846 
2023-03-21 12:40:28.349516: train_loss -0.901 
2023-03-21 12:40:28.349786: val_loss -0.8402 
2023-03-21 12:40:28.349894: Pseudo dice [0.8927, 0.875] 
2023-03-21 12:40:28.349998: Epoch time: 13.51 s 
2023-03-21 12:40:29.657659:  
2023-03-21 12:40:29.657795: Epoch 171 
2023-03-21 12:40:29.657938: Current learning rate: 0.00845 
2023-03-21 12:40:43.493493: train_loss -0.8995 
2023-03-21 12:40:43.493976: val_loss -0.8447 
2023-03-21 12:40:43.494080: Pseudo dice [0.895, 0.879] 
2023-03-21 12:40:43.494183: Epoch time: 13.84 s 
2023-03-21 12:40:44.712471:  
2023-03-21 12:40:44.712624: Epoch 172 
2023-03-21 12:40:44.712773: Current learning rate: 0.00844 
2023-03-21 12:40:58.279682: train_loss -0.9003 
2023-03-21 12:40:58.279962: val_loss -0.8412 
2023-03-21 12:40:58.280067: Pseudo dice [0.8924, 0.8758] 
2023-03-21 12:40:58.280172: Epoch time: 13.57 s 
2023-03-21 12:40:59.653668:  
2023-03-21 12:40:59.653832: Epoch 173 
2023-03-21 12:40:59.653982: Current learning rate: 0.00843 
2023-03-21 12:41:13.405329: train_loss -0.9003 
2023-03-21 12:41:13.405638: val_loss -0.8429 
2023-03-21 12:41:13.405753: Pseudo dice [0.8952, 0.8786] 
2023-03-21 12:41:13.405864: Epoch time: 13.75 s 
2023-03-21 12:41:14.711289:  
2023-03-21 12:41:14.711437: Epoch 174 
2023-03-21 12:41:14.711587: Current learning rate: 0.00842 
2023-03-21 12:41:28.664036: train_loss -0.9025 
2023-03-21 12:41:28.664529: val_loss -0.8458 
2023-03-21 12:41:28.664813: Pseudo dice [0.897, 0.8785] 
2023-03-21 12:41:28.665071: Epoch time: 13.95 s 
2023-03-21 12:41:29.928584:  
2023-03-21 12:41:29.928735: Epoch 175 
2023-03-21 12:41:29.928885: Current learning rate: 0.00841 
2023-03-21 12:41:43.729641: train_loss -0.9006 
2023-03-21 12:41:43.729937: val_loss -0.8366 
2023-03-21 12:41:43.730067: Pseudo dice [0.8909, 0.8723] 
2023-03-21 12:41:43.730181: Epoch time: 13.8 s 
2023-03-21 12:41:44.993524:  
2023-03-21 12:41:44.993850: Epoch 176 
2023-03-21 12:41:44.993991: Current learning rate: 0.0084 
2023-03-21 12:41:58.853907: train_loss -0.9021 
2023-03-21 12:41:58.854220: val_loss -0.8438 
2023-03-21 12:41:58.854353: Pseudo dice [0.8957, 0.8762] 
2023-03-21 12:41:58.854485: Epoch time: 13.86 s 
2023-03-21 12:42:00.313790:  
2023-03-21 12:42:00.313927: Epoch 177 
2023-03-21 12:42:00.314061: Current learning rate: 0.00839 
2023-03-21 12:42:13.976756: train_loss -0.9019 
2023-03-21 12:42:13.977029: val_loss -0.8388 
2023-03-21 12:42:13.977513: Pseudo dice [0.8922, 0.8738] 
2023-03-21 12:42:13.977619: Epoch time: 13.66 s 
2023-03-21 12:42:15.257305:  
2023-03-21 12:42:15.257457: Epoch 178 
2023-03-21 12:42:15.257605: Current learning rate: 0.00838 
2023-03-21 12:42:28.651018: train_loss -0.9025 
2023-03-21 12:42:28.651364: val_loss -0.8406 
2023-03-21 12:42:28.651594: Pseudo dice [0.895, 0.8762] 
2023-03-21 12:42:28.651790: Epoch time: 13.39 s 
2023-03-21 12:42:29.863978:  
2023-03-21 12:42:29.864131: Epoch 179 
2023-03-21 12:42:29.864280: Current learning rate: 0.00837 
2023-03-21 12:42:43.851095: train_loss -0.9019 
2023-03-21 12:42:43.851533: val_loss -0.8449 
2023-03-21 12:42:43.851648: Pseudo dice [0.8954, 0.8791] 
2023-03-21 12:42:43.851756: Epoch time: 13.99 s 
2023-03-21 12:42:45.062200:  
2023-03-21 12:42:45.062341: Epoch 180 
2023-03-21 12:42:45.062486: Current learning rate: 0.00836 
2023-03-21 12:42:58.419861: train_loss -0.9015 
2023-03-21 12:42:58.420121: val_loss -0.8429 
2023-03-21 12:42:58.420220: Pseudo dice [0.894, 0.8772] 
2023-03-21 12:42:58.420321: Epoch time: 13.36 s 
2023-03-21 12:42:59.671316:  
2023-03-21 12:42:59.671456: Epoch 181 
2023-03-21 12:42:59.671599: Current learning rate: 0.00836 
2023-03-21 12:43:13.252903: train_loss -0.9012 
2023-03-21 12:43:13.253186: val_loss -0.8411 
2023-03-21 12:43:13.253300: Pseudo dice [0.891, 0.8764] 
2023-03-21 12:43:13.253412: Epoch time: 13.58 s 
2023-03-21 12:43:14.585922:  
2023-03-21 12:43:14.586068: Epoch 182 
2023-03-21 12:43:14.586203: Current learning rate: 0.00835 
2023-03-21 12:43:27.967649: train_loss -0.9009 
2023-03-21 12:43:27.968812: val_loss -0.8424 
2023-03-21 12:43:27.969038: Pseudo dice [0.8946, 0.8749] 
2023-03-21 12:43:27.969252: Epoch time: 13.38 s 
2023-03-21 12:43:29.203381:  
2023-03-21 12:43:29.203561: Epoch 183 
2023-03-21 12:43:29.203715: Current learning rate: 0.00834 
2023-03-21 12:43:43.013013: train_loss -0.9004 
2023-03-21 12:43:43.013358: val_loss -0.8367 
2023-03-21 12:43:43.013501: Pseudo dice [0.8916, 0.8723] 
2023-03-21 12:43:43.013633: Epoch time: 13.81 s 
2023-03-21 12:43:44.251426:  
2023-03-21 12:43:44.251584: Epoch 184 
2023-03-21 12:43:44.251730: Current learning rate: 0.00833 
2023-03-21 12:43:57.970187: train_loss -0.9036 
2023-03-21 12:43:57.970609: val_loss -0.8441 
2023-03-21 12:43:57.970716: Pseudo dice [0.8965, 0.8791] 
2023-03-21 12:43:57.970850: Epoch time: 13.72 s 
2023-03-21 12:43:59.165430:  
2023-03-21 12:43:59.165639: Epoch 185 
2023-03-21 12:43:59.165830: Current learning rate: 0.00832 
2023-03-21 12:44:13.021299: train_loss -0.9002 
2023-03-21 12:44:13.021754: val_loss -0.8425 
2023-03-21 12:44:13.022034: Pseudo dice [0.8939, 0.8783] 
2023-03-21 12:44:13.022295: Epoch time: 13.86 s 
2023-03-21 12:44:14.262275:  
2023-03-21 12:44:14.262446: Epoch 186 
2023-03-21 12:44:14.262629: Current learning rate: 0.00831 
2023-03-21 12:44:27.719954: train_loss -0.9012 
2023-03-21 12:44:27.721276: val_loss -0.8482 
2023-03-21 12:44:27.721379: Pseudo dice [0.8984, 0.8812] 
2023-03-21 12:44:27.721482: Epoch time: 13.46 s 
2023-03-21 12:44:29.082255:  
2023-03-21 12:44:29.082409: Epoch 187 
2023-03-21 12:44:29.082551: Current learning rate: 0.0083 
2023-03-21 12:44:42.542071: train_loss -0.9028 
2023-03-21 12:44:42.542387: val_loss -0.8404 
2023-03-21 12:44:42.542516: Pseudo dice [0.8933, 0.8762] 
2023-03-21 12:44:42.542642: Epoch time: 13.46 s 
2023-03-21 12:44:43.801776:  
2023-03-21 12:44:43.801945: Epoch 188 
2023-03-21 12:44:43.802076: Current learning rate: 0.00829 
2023-03-21 12:44:57.370273: train_loss -0.9029 
2023-03-21 12:44:57.370541: val_loss -0.8441 
2023-03-21 12:44:57.370641: Pseudo dice [0.8969, 0.877] 
2023-03-21 12:44:57.370777: Epoch time: 13.57 s 
2023-03-21 12:44:58.618829:  
2023-03-21 12:44:58.618973: Epoch 189 
2023-03-21 12:44:58.619134: Current learning rate: 0.00828 
2023-03-21 12:45:12.285118: train_loss -0.9038 
2023-03-21 12:45:12.285387: val_loss -0.8415 
2023-03-21 12:45:12.285486: Pseudo dice [0.894, 0.8768] 
2023-03-21 12:45:12.285586: Epoch time: 13.67 s 
2023-03-21 12:45:13.505690:  
2023-03-21 12:45:13.505848: Epoch 190 
2023-03-21 12:45:13.505994: Current learning rate: 0.00827 
2023-03-21 12:45:27.278826: train_loss -0.9033 
2023-03-21 12:45:27.279111: val_loss -0.8424 
2023-03-21 12:45:27.279217: Pseudo dice [0.8943, 0.8785] 
2023-03-21 12:45:27.279322: Epoch time: 13.77 s 
2023-03-21 12:45:28.678389:  
2023-03-21 12:45:28.678550: Epoch 191 
2023-03-21 12:45:28.678684: Current learning rate: 0.00826 
2023-03-21 12:45:42.635184: train_loss -0.9037 
2023-03-21 12:45:42.635514: val_loss -0.8463 
2023-03-21 12:45:42.635649: Pseudo dice [0.8965, 0.8794] 
2023-03-21 12:45:42.635780: Epoch time: 13.96 s 
2023-03-21 12:45:43.936593:  
2023-03-21 12:45:43.936749: Epoch 192 
2023-03-21 12:45:43.936900: Current learning rate: 0.00825 
2023-03-21 12:45:57.835648: train_loss -0.9042 
2023-03-21 12:45:57.835963: val_loss -0.8431 
2023-03-21 12:45:57.836097: Pseudo dice [0.8962, 0.8782] 
2023-03-21 12:45:57.836226: Epoch time: 13.9 s 
2023-03-21 12:45:59.165105:  
2023-03-21 12:45:59.165252: Epoch 193 
2023-03-21 12:45:59.165401: Current learning rate: 0.00824 
2023-03-21 12:46:13.189011: train_loss -0.9038 
2023-03-21 12:46:13.189265: val_loss -0.8468 
2023-03-21 12:46:13.189385: Pseudo dice [0.8982, 0.8788] 
2023-03-21 12:46:13.189490: Epoch time: 14.02 s 
2023-03-21 12:46:14.517305:  
2023-03-21 12:46:14.517448: Epoch 194 
2023-03-21 12:46:14.517579: Current learning rate: 0.00824 
2023-03-21 12:46:28.090029: train_loss -0.9036 
2023-03-21 12:46:28.090517: val_loss -0.8448 
2023-03-21 12:46:28.090623: Pseudo dice [0.8962, 0.8774] 
2023-03-21 12:46:28.090734: Epoch time: 13.57 s 
2023-03-21 12:46:29.320986:  
2023-03-21 12:46:29.321118: Epoch 195 
2023-03-21 12:46:29.321245: Current learning rate: 0.00823 
2023-03-21 12:46:43.098077: train_loss -0.9032 
2023-03-21 12:46:43.098435: val_loss -0.8451 
2023-03-21 12:46:43.098663: Pseudo dice [0.8962, 0.8816] 
2023-03-21 12:46:43.098956: Epoch time: 13.78 s 
2023-03-21 12:46:44.489937:  
2023-03-21 12:46:44.490097: Epoch 196 
2023-03-21 12:46:44.490225: Current learning rate: 0.00822 
2023-03-21 12:46:58.187223: train_loss -0.9027 
2023-03-21 12:46:58.187800: val_loss -0.8455 
2023-03-21 12:46:58.188046: Pseudo dice [0.8959, 0.8801] 
2023-03-21 12:46:58.188155: Epoch time: 13.7 s 
2023-03-21 12:46:59.413510:  
2023-03-21 12:46:59.413704: Epoch 197 
2023-03-21 12:46:59.413837: Current learning rate: 0.00821 
2023-03-21 12:47:13.306377: train_loss -0.9038 
2023-03-21 12:47:13.306811: val_loss -0.8473 
2023-03-21 12:47:13.307077: Pseudo dice [0.8983, 0.8815] 
2023-03-21 12:47:13.307311: Epoch time: 13.89 s 
2023-03-21 12:47:14.548196:  
2023-03-21 12:47:14.548350: Epoch 198 
2023-03-21 12:47:14.548478: Current learning rate: 0.0082 
2023-03-21 12:47:28.020025: train_loss -0.9038 
2023-03-21 12:47:28.020480: val_loss -0.8465 
2023-03-21 12:47:28.020702: Pseudo dice [0.8968, 0.8809] 
2023-03-21 12:47:28.020839: Epoch time: 13.47 s 
2023-03-21 12:47:29.263404:  
2023-03-21 12:47:29.263543: Epoch 199 
2023-03-21 12:47:29.263687: Current learning rate: 0.00819 
2023-03-21 12:47:43.060067: train_loss -0.901 
2023-03-21 12:47:43.060353: val_loss -0.842 
2023-03-21 12:47:43.060458: Pseudo dice [0.8955, 0.8762] 
2023-03-21 12:47:43.060559: Epoch time: 13.8 s 
2023-03-21 12:47:44.515668:  
2023-03-21 12:47:44.515807: Epoch 200 
2023-03-21 12:47:44.515953: Current learning rate: 0.00818 
2023-03-21 12:47:58.218037: train_loss -0.9029 
2023-03-21 12:47:58.218366: val_loss -0.8426 
2023-03-21 12:47:58.218497: Pseudo dice [0.8956, 0.8788] 
2023-03-21 12:47:58.218621: Epoch time: 13.7 s 
2023-03-21 12:47:59.677110:  
2023-03-21 12:47:59.677270: Epoch 201 
2023-03-21 12:47:59.677443: Current learning rate: 0.00817 
2023-03-21 12:48:13.389191: train_loss -0.9037 
2023-03-21 12:48:13.389472: val_loss -0.843 
2023-03-21 12:48:13.389584: Pseudo dice [0.8958, 0.877] 
2023-03-21 12:48:13.389680: Epoch time: 13.71 s 
2023-03-21 12:48:14.642756:  
2023-03-21 12:48:14.642901: Epoch 202 
2023-03-21 12:48:14.643046: Current learning rate: 0.00816 
2023-03-21 12:48:28.570483: train_loss -0.9043 
2023-03-21 12:48:28.570953: val_loss -0.8408 
2023-03-21 12:48:28.571089: Pseudo dice [0.8954, 0.876] 
2023-03-21 12:48:28.571231: Epoch time: 13.93 s 
2023-03-21 12:48:29.834813:  
2023-03-21 12:48:29.834964: Epoch 203 
2023-03-21 12:48:29.835142: Current learning rate: 0.00815 
2023-03-21 12:48:43.687577: train_loss -0.9037 
2023-03-21 12:48:43.687868: val_loss -0.8446 
2023-03-21 12:48:43.687975: Pseudo dice [0.8965, 0.8794] 
2023-03-21 12:48:43.688094: Epoch time: 13.85 s 
2023-03-21 12:48:44.979783:  
2023-03-21 12:48:44.979939: Epoch 204 
2023-03-21 12:48:44.980195: Current learning rate: 0.00814 
2023-03-21 12:48:58.515864: train_loss -0.9044 
2023-03-21 12:48:58.516164: val_loss -0.8437 
2023-03-21 12:48:58.516267: Pseudo dice [0.8965, 0.8785] 
2023-03-21 12:48:58.516371: Epoch time: 13.54 s 
2023-03-21 12:48:59.878974:  
2023-03-21 12:48:59.879802: Epoch 205 
2023-03-21 12:48:59.880099: Current learning rate: 0.00813 
2023-03-21 12:49:13.499718: train_loss -0.9041 
2023-03-21 12:49:13.500094: val_loss -0.8418 
2023-03-21 12:49:13.500292: Pseudo dice [0.8941, 0.8766] 
2023-03-21 12:49:13.500406: Epoch time: 13.62 s 
2023-03-21 12:49:14.809658:  
2023-03-21 12:49:14.809797: Epoch 206 
2023-03-21 12:49:14.809957: Current learning rate: 0.00813 
2023-03-21 12:49:28.444790: train_loss -0.9045 
2023-03-21 12:49:28.445131: val_loss -0.8419 
2023-03-21 12:49:28.445282: Pseudo dice [0.8959, 0.8758] 
2023-03-21 12:49:28.445414: Epoch time: 13.64 s 
2023-03-21 12:49:29.635529:  
2023-03-21 12:49:29.635668: Epoch 207 
2023-03-21 12:49:29.635829: Current learning rate: 0.00812 
2023-03-21 12:49:43.181160: train_loss -0.9036 
2023-03-21 12:49:43.182755: val_loss -0.8368 
2023-03-21 12:49:43.182865: Pseudo dice [0.893, 0.8726] 
2023-03-21 12:49:43.182966: Epoch time: 13.55 s 
2023-03-21 12:49:44.358954:  
2023-03-21 12:49:44.359112: Epoch 208 
2023-03-21 12:49:44.359283: Current learning rate: 0.00811 
2023-03-21 12:49:58.180279: train_loss -0.9054 
2023-03-21 12:49:58.180564: val_loss -0.8445 
2023-03-21 12:49:58.180668: Pseudo dice [0.8969, 0.8773] 
2023-03-21 12:49:58.180786: Epoch time: 13.82 s 
2023-03-21 12:49:59.317387:  
2023-03-21 12:49:59.317541: Epoch 209 
2023-03-21 12:49:59.317701: Current learning rate: 0.0081 
2023-03-21 12:50:13.358486: train_loss -0.9036 
2023-03-21 12:50:13.359348: val_loss -0.844 
2023-03-21 12:50:13.359515: Pseudo dice [0.8957, 0.8777] 
2023-03-21 12:50:13.359801: Epoch time: 14.04 s 
2023-03-21 12:50:14.519813:  
2023-03-21 12:50:14.519959: Epoch 210 
2023-03-21 12:50:14.520127: Current learning rate: 0.00809 
2023-03-21 12:50:27.986668: train_loss -0.9048 
2023-03-21 12:50:27.986990: val_loss -0.8483 
2023-03-21 12:50:27.987096: Pseudo dice [0.8985, 0.8826] 
2023-03-21 12:50:27.987200: Epoch time: 13.47 s 
2023-03-21 12:50:29.316525:  
2023-03-21 12:50:29.316757: Epoch 211 
2023-03-21 12:50:29.316921: Current learning rate: 0.00808 
2023-03-21 12:50:42.907457: train_loss -0.9055 
2023-03-21 12:50:42.907676: val_loss -0.8458 
2023-03-21 12:50:42.907783: Pseudo dice [0.898, 0.8781] 
2023-03-21 12:50:42.907885: Epoch time: 13.59 s 
2023-03-21 12:50:44.256493:  
2023-03-21 12:50:44.256637: Epoch 212 
2023-03-21 12:50:44.256783: Current learning rate: 0.00807 
2023-03-21 12:50:58.070898: train_loss -0.9046 
2023-03-21 12:50:58.071174: val_loss -0.8431 
2023-03-21 12:50:58.071283: Pseudo dice [0.8953, 0.879] 
2023-03-21 12:50:58.071399: Epoch time: 13.82 s 
2023-03-21 12:50:59.301975:  
2023-03-21 12:50:59.302119: Epoch 213 
2023-03-21 12:50:59.302281: Current learning rate: 0.00806 
2023-03-21 12:51:13.177293: train_loss -0.9045 
2023-03-21 12:51:13.177912: val_loss -0.8449 
2023-03-21 12:51:13.178099: Pseudo dice [0.8975, 0.8778] 
2023-03-21 12:51:13.178240: Epoch time: 13.88 s 
2023-03-21 12:51:14.398000:  
2023-03-21 12:51:14.398147: Epoch 214 
2023-03-21 12:51:14.398314: Current learning rate: 0.00805 
2023-03-21 12:51:28.066613: train_loss -0.9044 
2023-03-21 12:51:28.066905: val_loss -0.8357 
2023-03-21 12:51:28.067011: Pseudo dice [0.8935, 0.8726] 
2023-03-21 12:51:28.067129: Epoch time: 13.67 s 
2023-03-21 12:51:29.234711:  
2023-03-21 12:51:29.234880: Epoch 215 
2023-03-21 12:51:29.235028: Current learning rate: 0.00804 
2023-03-21 12:51:43.069920: train_loss -0.9035 
2023-03-21 12:51:43.070212: val_loss -0.8421 
2023-03-21 12:51:43.070320: Pseudo dice [0.895, 0.8777] 
2023-03-21 12:51:43.070424: Epoch time: 13.84 s 
2023-03-21 12:51:44.383037:  
2023-03-21 12:51:44.383184: Epoch 216 
2023-03-21 12:51:44.383349: Current learning rate: 0.00803 
2023-03-21 12:51:58.285526: train_loss -0.9076 
2023-03-21 12:51:58.285829: val_loss -0.8451 
2023-03-21 12:51:58.285963: Pseudo dice [0.8953, 0.8802] 
2023-03-21 12:51:58.286089: Epoch time: 13.9 s 
2023-03-21 12:51:59.472259:  
2023-03-21 12:51:59.472400: Epoch 217 
2023-03-21 12:51:59.472568: Current learning rate: 0.00802 
2023-03-21 12:52:13.105215: train_loss -0.9047 
2023-03-21 12:52:13.105499: val_loss -0.8508 
2023-03-21 12:52:13.105603: Pseudo dice [0.9002, 0.8829] 
2023-03-21 12:52:13.105705: Epoch time: 13.63 s 
2023-03-21 12:52:14.396178:  
2023-03-21 12:52:14.396338: Epoch 218 
2023-03-21 12:52:14.396504: Current learning rate: 0.00801 
2023-03-21 12:52:28.088119: train_loss -0.9057 
2023-03-21 12:52:28.088580: val_loss -0.8447 
2023-03-21 12:52:28.088720: Pseudo dice [0.8963, 0.8789] 
2023-03-21 12:52:28.088852: Epoch time: 13.69 s 
2023-03-21 12:52:29.317320:  
2023-03-21 12:52:29.317467: Epoch 219 
2023-03-21 12:52:29.317636: Current learning rate: 0.00801 
2023-03-21 12:52:42.938074: train_loss -0.9057 
2023-03-21 12:52:42.938306: val_loss -0.8448 
2023-03-21 12:52:42.938413: Pseudo dice [0.8987, 0.8801] 
2023-03-21 12:52:42.938522: Epoch time: 13.62 s 
2023-03-21 12:52:44.093267:  
2023-03-21 12:52:44.093418: Epoch 220 
2023-03-21 12:52:44.093563: Current learning rate: 0.008 
2023-03-21 12:52:58.045898: train_loss -0.9049 
2023-03-21 12:52:58.046655: val_loss -0.844 
2023-03-21 12:52:58.047059: Pseudo dice [0.896, 0.8792] 
2023-03-21 12:52:58.047398: Epoch time: 13.95 s 
2023-03-21 12:52:59.402821:  
2023-03-21 12:52:59.402978: Epoch 221 
2023-03-21 12:52:59.403133: Current learning rate: 0.00799 
2023-03-21 12:53:13.245066: train_loss -0.905 
2023-03-21 12:53:13.245339: val_loss -0.8433 
2023-03-21 12:53:13.245440: Pseudo dice [0.8946, 0.8798] 
2023-03-21 12:53:13.245540: Epoch time: 13.84 s 
2023-03-21 12:53:14.409343:  
2023-03-21 12:53:14.409492: Epoch 222 
2023-03-21 12:53:14.409646: Current learning rate: 0.00798 
2023-03-21 12:53:28.038876: train_loss -0.9045 
2023-03-21 12:53:28.039211: val_loss -0.8428 
2023-03-21 12:53:28.039499: Pseudo dice [0.8957, 0.8771] 
2023-03-21 12:53:28.039631: Epoch time: 13.63 s 
2023-03-21 12:53:29.234808:  
2023-03-21 12:53:29.234953: Epoch 223 
2023-03-21 12:53:29.235119: Current learning rate: 0.00797 
2023-03-21 12:53:42.770310: train_loss -0.9056 
2023-03-21 12:53:42.770604: val_loss -0.8414 
2023-03-21 12:53:42.770709: Pseudo dice [0.8959, 0.8766] 
2023-03-21 12:53:42.770844: Epoch time: 13.54 s 
2023-03-21 12:53:43.939241:  
2023-03-21 12:53:43.939385: Epoch 224 
2023-03-21 12:53:43.939529: Current learning rate: 0.00796 
2023-03-21 12:53:57.562431: train_loss -0.9027 
2023-03-21 12:53:57.562699: val_loss -0.8456 
2023-03-21 12:53:57.562832: Pseudo dice [0.8968, 0.8805] 
2023-03-21 12:53:57.562940: Epoch time: 13.62 s 
2023-03-21 12:53:58.726778:  
2023-03-21 12:53:58.726920: Epoch 225 
2023-03-21 12:53:58.727082: Current learning rate: 0.00795 
2023-03-21 12:54:12.512517: train_loss -0.9059 
2023-03-21 12:54:12.512750: val_loss -0.8428 
2023-03-21 12:54:12.512863: Pseudo dice [0.8965, 0.8781] 
2023-03-21 12:54:12.512964: Epoch time: 13.79 s 
2023-03-21 12:54:13.823515:  
2023-03-21 12:54:13.823699: Epoch 226 
2023-03-21 12:54:13.823866: Current learning rate: 0.00794 
2023-03-21 12:54:27.759496: train_loss -0.9049 
2023-03-21 12:54:27.759808: val_loss -0.8421 
2023-03-21 12:54:27.759922: Pseudo dice [0.8962, 0.8778] 
2023-03-21 12:54:27.760031: Epoch time: 13.94 s 
2023-03-21 12:54:28.976199:  
2023-03-21 12:54:28.976358: Epoch 227 
2023-03-21 12:54:28.976542: Current learning rate: 0.00793 
2023-03-21 12:54:42.847140: train_loss -0.9053 
2023-03-21 12:54:42.848693: val_loss -0.8455 
2023-03-21 12:54:42.848812: Pseudo dice [0.8977, 0.8805] 
2023-03-21 12:54:42.848917: Epoch time: 13.87 s 
2023-03-21 12:54:44.026506:  
2023-03-21 12:54:44.026662: Epoch 228 
2023-03-21 12:54:44.026848: Current learning rate: 0.00792 
2023-03-21 12:54:57.510467: train_loss -0.9057 
2023-03-21 12:54:57.511051: val_loss -0.8496 
2023-03-21 12:54:57.511311: Pseudo dice [0.9002, 0.8827] 
2023-03-21 12:54:57.511561: Epoch time: 13.48 s 
2023-03-21 12:54:58.647699:  
2023-03-21 12:54:58.647850: Epoch 229 
2023-03-21 12:54:58.648013: Current learning rate: 0.00791 
2023-03-21 12:55:12.312057: train_loss -0.9072 
2023-03-21 12:55:12.312339: val_loss -0.8467 
2023-03-21 12:55:12.312440: Pseudo dice [0.8982, 0.8797] 
2023-03-21 12:55:12.312545: Epoch time: 13.66 s 
2023-03-21 12:55:12.312629: Yayy! New best EMA pseudo Dice: 0.888 
2023-03-21 12:55:13.781130:  
2023-03-21 12:55:13.781271: Epoch 230 
2023-03-21 12:55:13.781434: Current learning rate: 0.0079 
2023-03-21 12:55:27.703824: train_loss -0.9054 
2023-03-21 12:55:27.704256: val_loss -0.8385 
2023-03-21 12:55:27.704389: Pseudo dice [0.8919, 0.8752] 
2023-03-21 12:55:27.704621: Epoch time: 13.92 s 
2023-03-21 12:55:28.991564:  
2023-03-21 12:55:28.991717: Epoch 231 
2023-03-21 12:55:28.991866: Current learning rate: 0.00789 
2023-03-21 12:55:42.548758: train_loss -0.9054 
2023-03-21 12:55:42.549057: val_loss -0.8418 
2023-03-21 12:55:42.549166: Pseudo dice [0.8948, 0.8784] 
2023-03-21 12:55:42.549275: Epoch time: 13.56 s 
2023-03-21 12:55:43.747766:  
2023-03-21 12:55:43.747916: Epoch 232 
2023-03-21 12:55:43.748077: Current learning rate: 0.00789 
2023-03-21 12:55:57.366964: train_loss -0.9049 
2023-03-21 12:55:57.367257: val_loss -0.8464 
2023-03-21 12:55:57.367364: Pseudo dice [0.8982, 0.8803] 
2023-03-21 12:55:57.367470: Epoch time: 13.62 s 
2023-03-21 12:55:58.528841:  
2023-03-21 12:55:58.528985: Epoch 233 
2023-03-21 12:55:58.529147: Current learning rate: 0.00788 
2023-03-21 12:56:12.315260: train_loss -0.9064 
2023-03-21 12:56:12.315549: val_loss -0.8382 
2023-03-21 12:56:12.315663: Pseudo dice [0.8919, 0.8752] 
2023-03-21 12:56:12.315784: Epoch time: 13.79 s 
2023-03-21 12:56:13.454560:  
2023-03-21 12:56:13.454809: Epoch 234 
2023-03-21 12:56:13.455091: Current learning rate: 0.00787 
2023-03-21 12:56:27.151995: train_loss -0.9065 
2023-03-21 12:56:27.152327: val_loss -0.8431 
2023-03-21 12:56:27.152507: Pseudo dice [0.8956, 0.8782] 
2023-03-21 12:56:27.152779: Epoch time: 13.7 s 
2023-03-21 12:56:28.303383:  
2023-03-21 12:56:28.303643: Epoch 235 
2023-03-21 12:56:28.303839: Current learning rate: 0.00786 
2023-03-21 12:56:42.080698: train_loss -0.9062 
2023-03-21 12:56:42.080962: val_loss -0.8417 
2023-03-21 12:56:42.081069: Pseudo dice [0.894, 0.8771] 
2023-03-21 12:56:42.081175: Epoch time: 13.78 s 
2023-03-21 12:56:43.393142:  
2023-03-21 12:56:43.393288: Epoch 236 
2023-03-21 12:56:43.393450: Current learning rate: 0.00785 
2023-03-21 12:56:57.215241: train_loss -0.9065 
2023-03-21 12:56:57.215571: val_loss -0.8445 
2023-03-21 12:56:57.215720: Pseudo dice [0.896, 0.8799] 
2023-03-21 12:56:57.215877: Epoch time: 13.82 s 
2023-03-21 12:56:58.418499:  
2023-03-21 12:56:58.418657: Epoch 237 
2023-03-21 12:56:58.418890: Current learning rate: 0.00784 
2023-03-21 12:57:11.901730: train_loss -0.9066 
2023-03-21 12:57:11.902018: val_loss -0.8444 
2023-03-21 12:57:11.902123: Pseudo dice [0.8951, 0.8789] 
2023-03-21 12:57:11.902245: Epoch time: 13.48 s 
2023-03-21 12:57:13.120866:  
2023-03-21 12:57:13.121006: Epoch 238 
2023-03-21 12:57:13.121166: Current learning rate: 0.00783 
2023-03-21 12:57:26.848923: train_loss -0.907 
2023-03-21 12:57:26.849200: val_loss -0.8451 
2023-03-21 12:57:26.849303: Pseudo dice [0.8964, 0.8809] 
2023-03-21 12:57:26.849403: Epoch time: 13.73 s 
2023-03-21 12:57:28.003566:  
2023-03-21 12:57:28.003744: Epoch 239 
2023-03-21 12:57:28.003892: Current learning rate: 0.00782 
2023-03-21 12:57:41.841659: train_loss -0.9074 
2023-03-21 12:57:41.842011: val_loss -0.8404 
2023-03-21 12:57:41.842121: Pseudo dice [0.8932, 0.8762] 
2023-03-21 12:57:41.842231: Epoch time: 13.84 s 
2023-03-21 12:57:43.109438:  
2023-03-21 12:57:43.109578: Epoch 240 
2023-03-21 12:57:43.109722: Current learning rate: 0.00781 
2023-03-21 12:57:56.976817: train_loss -0.9062 
2023-03-21 12:57:56.977247: val_loss -0.8438 
2023-03-21 12:57:56.977615: Pseudo dice [0.8956, 0.8782] 
2023-03-21 12:57:56.977771: Epoch time: 13.87 s 
2023-03-21 12:57:58.326279:  
2023-03-21 12:57:58.326464: Epoch 241 
2023-03-21 12:57:58.326631: Current learning rate: 0.0078 
2023-03-21 12:58:12.260669: train_loss -0.9071 
2023-03-21 12:58:12.260931: val_loss -0.8441 
2023-03-21 12:58:12.261047: Pseudo dice [0.8975, 0.8788] 
2023-03-21 12:58:12.261148: Epoch time: 13.94 s 
2023-03-21 12:58:13.444268:  
2023-03-21 12:58:13.444427: Epoch 242 
2023-03-21 12:58:13.444576: Current learning rate: 0.00779 
2023-03-21 12:58:27.282705: train_loss -0.9077 
2023-03-21 12:58:27.283006: val_loss -0.841 
2023-03-21 12:58:27.283143: Pseudo dice [0.8958, 0.8768] 
2023-03-21 12:58:27.283250: Epoch time: 13.84 s 
2023-03-21 12:58:28.427577:  
2023-03-21 12:58:28.427744: Epoch 243 
2023-03-21 12:58:28.427895: Current learning rate: 0.00778 
2023-03-21 12:58:42.354931: train_loss -0.9063 
2023-03-21 12:58:42.355262: val_loss -0.842 
2023-03-21 12:58:42.355372: Pseudo dice [0.8961, 0.8774] 
2023-03-21 12:58:42.355482: Epoch time: 13.93 s 
2023-03-21 12:58:43.532092:  
2023-03-21 12:58:43.532239: Epoch 244 
2023-03-21 12:58:43.532387: Current learning rate: 0.00777 
2023-03-21 12:58:57.197980: train_loss -0.9072 
2023-03-21 12:58:57.198246: val_loss -0.8403 
2023-03-21 12:58:57.198345: Pseudo dice [0.8928, 0.8758] 
2023-03-21 12:58:57.198443: Epoch time: 13.67 s 
2023-03-21 12:58:58.445175:  
2023-03-21 12:58:58.445308: Epoch 245 
2023-03-21 12:58:58.445451: Current learning rate: 0.00777 
2023-03-21 12:59:12.274364: train_loss -0.9076 
2023-03-21 12:59:12.274647: val_loss -0.8435 
2023-03-21 12:59:12.274780: Pseudo dice [0.8957, 0.8791] 
2023-03-21 12:59:12.274914: Epoch time: 13.83 s 
2023-03-21 12:59:13.619620:  
2023-03-21 12:59:13.619764: Epoch 246 
2023-03-21 12:59:13.619925: Current learning rate: 0.00776 
2023-03-21 12:59:27.215863: train_loss -0.9073 
2023-03-21 12:59:27.216288: val_loss -0.8402 
2023-03-21 12:59:27.216757: Pseudo dice [0.8938, 0.8781] 
2023-03-21 12:59:27.216938: Epoch time: 13.6 s 
2023-03-21 12:59:28.426271:  
2023-03-21 12:59:28.426416: Epoch 247 
2023-03-21 12:59:28.426562: Current learning rate: 0.00775 
2023-03-21 12:59:42.145910: train_loss -0.909 
2023-03-21 12:59:42.146219: val_loss -0.8427 
2023-03-21 12:59:42.146349: Pseudo dice [0.8971, 0.8776] 
2023-03-21 12:59:42.146477: Epoch time: 13.72 s 
2023-03-21 12:59:43.324167:  
2023-03-21 12:59:43.324318: Epoch 248 
2023-03-21 12:59:43.324465: Current learning rate: 0.00774 
2023-03-21 12:59:57.053712: train_loss -0.9068 
2023-03-21 12:59:57.054233: val_loss -0.8463 
2023-03-21 12:59:57.054455: Pseudo dice [0.8976, 0.8811] 
2023-03-21 12:59:57.054676: Epoch time: 13.73 s 
2023-03-21 12:59:58.239156:  
2023-03-21 12:59:58.239293: Epoch 249 
2023-03-21 12:59:58.239440: Current learning rate: 0.00773 
2023-03-21 13:00:12.044612: train_loss -0.9081 
2023-03-21 13:00:12.044880: val_loss -0.8423 
2023-03-21 13:00:12.044984: Pseudo dice [0.8947, 0.8765] 
2023-03-21 13:00:12.045089: Epoch time: 13.81 s 
2023-03-21 13:00:13.654311:  
2023-03-21 13:00:13.654451: Epoch 250 
2023-03-21 13:00:13.654600: Current learning rate: 0.00772 
2023-03-21 13:00:27.387351: train_loss -0.9073 
2023-03-21 13:00:27.387679: val_loss -0.8495 
2023-03-21 13:00:27.387812: Pseudo dice [0.8981, 0.8841] 
2023-03-21 13:00:27.387959: Epoch time: 13.73 s 
2023-03-21 13:00:28.690849:  
2023-03-21 13:00:28.691185: Epoch 251 
2023-03-21 13:00:28.691324: Current learning rate: 0.00771 
2023-03-21 13:00:42.443398: train_loss -0.907 
2023-03-21 13:00:42.443660: val_loss -0.8468 
2023-03-21 13:00:42.443761: Pseudo dice [0.8985, 0.8827] 
2023-03-21 13:00:42.443881: Epoch time: 13.75 s 
2023-03-21 13:00:43.602255:  
2023-03-21 13:00:43.602411: Epoch 252 
2023-03-21 13:00:43.602557: Current learning rate: 0.0077 
2023-03-21 13:00:57.285276: train_loss -0.908 
2023-03-21 13:00:57.285562: val_loss -0.8436 
2023-03-21 13:00:57.285661: Pseudo dice [0.8971, 0.8802] 
2023-03-21 13:00:57.285761: Epoch time: 13.68 s 
2023-03-21 13:00:58.447684:  
2023-03-21 13:00:58.447840: Epoch 253 
2023-03-21 13:00:58.447992: Current learning rate: 0.00769 
2023-03-21 13:01:12.037543: train_loss -0.9079 
2023-03-21 13:01:12.037859: val_loss -0.8365 
2023-03-21 13:01:12.037971: Pseudo dice [0.8914, 0.8753] 
2023-03-21 13:01:12.038076: Epoch time: 13.59 s 
2023-03-21 13:01:13.270450:  
2023-03-21 13:01:13.270617: Epoch 254 
2023-03-21 13:01:13.270800: Current learning rate: 0.00768 
2023-03-21 13:01:27.112817: train_loss -0.9088 
2023-03-21 13:01:27.113113: val_loss -0.843 
2023-03-21 13:01:27.113223: Pseudo dice [0.8952, 0.8798] 
2023-03-21 13:01:27.113343: Epoch time: 13.84 s 
2023-03-21 13:01:28.333702:  
2023-03-21 13:01:28.333843: Epoch 255 
2023-03-21 13:01:28.333989: Current learning rate: 0.00767 
2023-03-21 13:01:41.988331: train_loss -0.9083 
2023-03-21 13:01:41.988608: val_loss -0.8445 
2023-03-21 13:01:41.988712: Pseudo dice [0.8974, 0.8795] 
2023-03-21 13:01:41.988813: Epoch time: 13.66 s 
2023-03-21 13:01:43.364365:  
2023-03-21 13:01:43.364514: Epoch 256 
2023-03-21 13:01:43.364684: Current learning rate: 0.00766 
2023-03-21 13:01:57.096998: train_loss -0.9071 
2023-03-21 13:01:57.097298: val_loss -0.8434 
2023-03-21 13:01:57.097405: Pseudo dice [0.8944, 0.8791] 
2023-03-21 13:01:57.097517: Epoch time: 13.73 s 
2023-03-21 13:01:58.273826:  
2023-03-21 13:01:58.273974: Epoch 257 
2023-03-21 13:01:58.274123: Current learning rate: 0.00765 
2023-03-21 13:02:11.707457: train_loss -0.9073 
2023-03-21 13:02:11.709074: val_loss -0.8442 
2023-03-21 13:02:11.709180: Pseudo dice [0.8963, 0.8789] 
2023-03-21 13:02:11.709287: Epoch time: 13.43 s 
2023-03-21 13:02:12.908016:  
2023-03-21 13:02:12.908177: Epoch 258 
2023-03-21 13:02:12.908338: Current learning rate: 0.00764 
2023-03-21 13:02:26.404172: train_loss -0.9075 
2023-03-21 13:02:26.404452: val_loss -0.8412 
2023-03-21 13:02:26.404556: Pseudo dice [0.8939, 0.8776] 
2023-03-21 13:02:26.404674: Epoch time: 13.5 s 
2023-03-21 13:02:27.571649:  
2023-03-21 13:02:27.571801: Epoch 259 
2023-03-21 13:02:27.571963: Current learning rate: 0.00764 
2023-03-21 13:02:41.126201: train_loss -0.907 
2023-03-21 13:02:41.126419: val_loss -0.844 
2023-03-21 13:02:41.126521: Pseudo dice [0.8963, 0.8792] 
2023-03-21 13:02:41.126618: Epoch time: 13.56 s 
2023-03-21 13:02:42.349915:  
2023-03-21 13:02:42.350053: Epoch 260 
2023-03-21 13:02:42.350211: Current learning rate: 0.00763 
2023-03-21 13:02:56.180698: train_loss -0.9076 
2023-03-21 13:02:56.180991: val_loss -0.8398 
2023-03-21 13:02:56.181098: Pseudo dice [0.8912, 0.8759] 
2023-03-21 13:02:56.181225: Epoch time: 13.83 s 
2023-03-21 13:02:57.521078:  
2023-03-21 13:02:57.521239: Epoch 261 
2023-03-21 13:02:57.521386: Current learning rate: 0.00762 
2023-03-21 13:03:11.200218: train_loss -0.9081 
2023-03-21 13:03:11.200762: val_loss -0.8405 
2023-03-21 13:03:11.201092: Pseudo dice [0.8947, 0.8778] 
2023-03-21 13:03:11.201486: Epoch time: 13.68 s 
2023-03-21 13:03:12.486090:  
2023-03-21 13:03:12.486241: Epoch 262 
2023-03-21 13:03:12.486391: Current learning rate: 0.00761 
2023-03-21 13:03:25.999707: train_loss -0.9076 
2023-03-21 13:03:26.000166: val_loss -0.8399 
2023-03-21 13:03:26.000496: Pseudo dice [0.8952, 0.8779] 
2023-03-21 13:03:26.000838: Epoch time: 13.51 s 
2023-03-21 13:03:27.261560:  
2023-03-21 13:03:27.261753: Epoch 263 
2023-03-21 13:03:27.261897: Current learning rate: 0.0076 
2023-03-21 13:03:40.695504: train_loss -0.9084 
2023-03-21 13:03:40.695784: val_loss -0.8414 
2023-03-21 13:03:40.695890: Pseudo dice [0.8933, 0.8768] 
2023-03-21 13:03:40.695992: Epoch time: 13.43 s 
2023-03-21 13:03:41.902352:  
2023-03-21 13:03:41.902506: Epoch 264 
2023-03-21 13:03:41.902671: Current learning rate: 0.00759 
2023-03-21 13:03:55.196053: train_loss -0.9088 
2023-03-21 13:03:55.196331: val_loss -0.8483 
2023-03-21 13:03:55.196433: Pseudo dice [0.8998, 0.8814] 
2023-03-21 13:03:55.196535: Epoch time: 13.29 s 
2023-03-21 13:03:56.445132:  
2023-03-21 13:03:56.445273: Epoch 265 
2023-03-21 13:03:56.445434: Current learning rate: 0.00758 
2023-03-21 13:04:09.834524: train_loss -0.9085 
2023-03-21 13:04:09.834775: val_loss -0.8435 
2023-03-21 13:04:09.834887: Pseudo dice [0.8966, 0.8787] 
2023-03-21 13:04:09.834991: Epoch time: 13.39 s 
2023-03-21 13:04:11.109410:  
2023-03-21 13:04:11.109573: Epoch 266 
2023-03-21 13:04:11.109751: Current learning rate: 0.00757 
2023-03-21 13:04:24.930393: train_loss -0.9088 
2023-03-21 13:04:24.930884: val_loss -0.8451 
2023-03-21 13:04:24.930998: Pseudo dice [0.8981, 0.8792] 
2023-03-21 13:04:24.931105: Epoch time: 13.82 s 
2023-03-21 13:04:26.157499:  
2023-03-21 13:04:26.157644: Epoch 267 
2023-03-21 13:04:26.157789: Current learning rate: 0.00756 
2023-03-21 13:04:39.795122: train_loss -0.9095 
2023-03-21 13:04:39.795380: val_loss -0.8427 
2023-03-21 13:04:39.795487: Pseudo dice [0.8973, 0.8782] 
2023-03-21 13:04:39.795595: Epoch time: 13.64 s 
2023-03-21 13:04:40.980121:  
2023-03-21 13:04:40.980273: Epoch 268 
2023-03-21 13:04:40.980455: Current learning rate: 0.00755 
2023-03-21 13:04:54.541792: train_loss -0.9083 
2023-03-21 13:04:54.542072: val_loss -0.8494 
2023-03-21 13:04:54.542189: Pseudo dice [0.8983, 0.8837] 
2023-03-21 13:04:54.542295: Epoch time: 13.56 s 
2023-03-21 13:04:55.717357:  
2023-03-21 13:04:55.717513: Epoch 269 
2023-03-21 13:04:55.717675: Current learning rate: 0.00754 
2023-03-21 13:05:09.188699: train_loss -0.9079 
2023-03-21 13:05:09.188958: val_loss -0.8407 
2023-03-21 13:05:09.189060: Pseudo dice [0.8938, 0.8766] 
2023-03-21 13:05:09.189163: Epoch time: 13.47 s 
2023-03-21 13:05:10.386932:  
2023-03-21 13:05:10.387071: Epoch 270 
2023-03-21 13:05:10.387252: Current learning rate: 0.00753 
2023-03-21 13:05:24.018446: train_loss -0.9082 
2023-03-21 13:05:24.018708: val_loss -0.843 
2023-03-21 13:05:24.018895: Pseudo dice [0.8945, 0.8798] 
2023-03-21 13:05:24.019005: Epoch time: 13.63 s 
2023-03-21 13:05:25.322510:  
2023-03-21 13:05:25.322787: Epoch 271 
2023-03-21 13:05:25.322987: Current learning rate: 0.00752 
2023-03-21 13:05:39.088691: train_loss -0.9081 
2023-03-21 13:05:39.088968: val_loss -0.8347 
2023-03-21 13:05:39.089095: Pseudo dice [0.8902, 0.8726] 
2023-03-21 13:05:39.089205: Epoch time: 13.77 s 
2023-03-21 13:05:40.264060:  
2023-03-21 13:05:40.264195: Epoch 272 
2023-03-21 13:05:40.264341: Current learning rate: 0.00751 
2023-03-21 13:05:54.220672: train_loss -0.9106 
2023-03-21 13:05:54.221140: val_loss -0.8384 
2023-03-21 13:05:54.221335: Pseudo dice [0.8937, 0.8743] 
2023-03-21 13:05:54.221587: Epoch time: 13.96 s 
2023-03-21 13:05:55.416005:  
2023-03-21 13:05:55.416161: Epoch 273 
2023-03-21 13:05:55.416308: Current learning rate: 0.00751 
2023-03-21 13:06:09.018969: train_loss -0.9105 
2023-03-21 13:06:09.019243: val_loss -0.8424 
2023-03-21 13:06:09.019347: Pseudo dice [0.8954, 0.878] 
2023-03-21 13:06:09.019450: Epoch time: 13.6 s 
2023-03-21 13:06:10.213311:  
2023-03-21 13:06:10.213468: Epoch 274 
2023-03-21 13:06:10.213618: Current learning rate: 0.0075 
2023-03-21 13:06:23.487941: train_loss -0.9079 
2023-03-21 13:06:23.488223: val_loss -0.8408 
2023-03-21 13:06:23.488348: Pseudo dice [0.8945, 0.8787] 
2023-03-21 13:06:23.488457: Epoch time: 13.28 s 
2023-03-21 13:06:24.680257:  
2023-03-21 13:06:24.680411: Epoch 275 
2023-03-21 13:06:24.680596: Current learning rate: 0.00749 
2023-03-21 13:06:38.366106: train_loss -0.9083 
2023-03-21 13:06:38.367600: val_loss -0.8399 
2023-03-21 13:06:38.367727: Pseudo dice [0.8932, 0.8772] 
2023-03-21 13:06:38.367831: Epoch time: 13.69 s 
2023-03-21 13:06:39.639958:  
2023-03-21 13:06:39.640103: Epoch 276 
2023-03-21 13:06:39.640250: Current learning rate: 0.00748 
2023-03-21 13:06:53.363894: train_loss -0.9109 
2023-03-21 13:06:53.364255: val_loss -0.8413 
2023-03-21 13:06:53.364546: Pseudo dice [0.895, 0.8774] 
2023-03-21 13:06:53.364881: Epoch time: 13.72 s 
2023-03-21 13:06:54.602878:  
2023-03-21 13:06:54.603032: Epoch 277 
2023-03-21 13:06:54.603197: Current learning rate: 0.00747 
2023-03-21 13:07:08.323634: train_loss -0.908 
2023-03-21 13:07:08.323903: val_loss -0.8416 
2023-03-21 13:07:08.324001: Pseudo dice [0.8951, 0.8788] 
2023-03-21 13:07:08.324100: Epoch time: 13.72 s 
2023-03-21 13:07:09.553128:  
2023-03-21 13:07:09.553272: Epoch 278 
2023-03-21 13:07:09.553597: Current learning rate: 0.00746 
2023-03-21 13:07:22.934287: train_loss -0.9113 
2023-03-21 13:07:22.934569: val_loss -0.8386 
2023-03-21 13:07:22.934702: Pseudo dice [0.8929, 0.8762] 
2023-03-21 13:07:22.934849: Epoch time: 13.38 s 
2023-03-21 13:07:24.101031:  
2023-03-21 13:07:24.101190: Epoch 279 
2023-03-21 13:07:24.101339: Current learning rate: 0.00745 
2023-03-21 13:07:37.577316: train_loss -0.9094 
2023-03-21 13:07:37.577617: val_loss -0.8402 
2023-03-21 13:07:37.577729: Pseudo dice [0.8942, 0.8764] 
2023-03-21 13:07:37.577840: Epoch time: 13.48 s 
2023-03-21 13:07:38.760379:  
2023-03-21 13:07:38.760532: Epoch 280 
2023-03-21 13:07:38.760691: Current learning rate: 0.00744 
2023-03-21 13:07:52.437960: train_loss -0.9089 
2023-03-21 13:07:52.438385: val_loss -0.8397 
2023-03-21 13:07:52.438610: Pseudo dice [0.8942, 0.8769] 
2023-03-21 13:07:52.438840: Epoch time: 13.68 s 
2023-03-21 13:07:53.773001:  
2023-03-21 13:07:53.773156: Epoch 281 
2023-03-21 13:07:53.773299: Current learning rate: 0.00743 
2023-03-21 13:08:07.680492: train_loss -0.9088 
2023-03-21 13:08:07.680772: val_loss -0.8446 
2023-03-21 13:08:07.680880: Pseudo dice [0.8964, 0.8808] 
2023-03-21 13:08:07.680985: Epoch time: 13.91 s 
2023-03-21 13:08:08.958494:  
2023-03-21 13:08:08.958658: Epoch 282 
2023-03-21 13:08:08.958869: Current learning rate: 0.00742 
2023-03-21 13:08:22.780549: train_loss -0.91 
2023-03-21 13:08:22.781264: val_loss -0.8424 
2023-03-21 13:08:22.781371: Pseudo dice [0.8957, 0.8771] 
2023-03-21 13:08:22.781489: Epoch time: 13.82 s 
2023-03-21 13:08:24.049706:  
2023-03-21 13:08:24.049848: Epoch 283 
2023-03-21 13:08:24.050015: Current learning rate: 0.00741 
2023-03-21 13:08:37.820157: train_loss -0.9084 
2023-03-21 13:08:37.820446: val_loss -0.8446 
2023-03-21 13:08:37.820552: Pseudo dice [0.8967, 0.8798] 
2023-03-21 13:08:37.820660: Epoch time: 13.77 s 
2023-03-21 13:08:39.019664:  
2023-03-21 13:08:39.019828: Epoch 284 
2023-03-21 13:08:39.019984: Current learning rate: 0.0074 
2023-03-21 13:08:53.107355: train_loss -0.9089 
2023-03-21 13:08:53.107670: val_loss -0.8415 
2023-03-21 13:08:53.107778: Pseudo dice [0.8939, 0.8784] 
2023-03-21 13:08:53.107895: Epoch time: 14.09 s 
2023-03-21 13:08:54.353863:  
2023-03-21 13:08:54.353998: Epoch 285 
2023-03-21 13:08:54.354171: Current learning rate: 0.00739 
2023-03-21 13:09:08.769200: train_loss -0.9101 
2023-03-21 13:09:08.769494: val_loss -0.8403 
2023-03-21 13:09:08.769604: Pseudo dice [0.8948, 0.8775] 
2023-03-21 13:09:08.769712: Epoch time: 14.42 s 
2023-03-21 13:09:10.230627:  
2023-03-21 13:09:10.230791: Epoch 286 
2023-03-21 13:09:10.230942: Current learning rate: 0.00738 
2023-03-21 13:09:24.304339: train_loss -0.9091 
2023-03-21 13:09:24.304767: val_loss -0.847 
2023-03-21 13:09:24.304982: Pseudo dice [0.8976, 0.8815] 
2023-03-21 13:09:24.305244: Epoch time: 14.07 s 
2023-03-21 13:09:25.556174:  
2023-03-21 13:09:25.556395: Epoch 287 
2023-03-21 13:09:25.556665: Current learning rate: 0.00738 
2023-03-21 13:09:39.452982: train_loss -0.9098 
2023-03-21 13:09:39.453358: val_loss -0.8427 
2023-03-21 13:09:39.453521: Pseudo dice [0.8967, 0.8783] 
2023-03-21 13:09:39.453623: Epoch time: 13.9 s 
2023-03-21 13:09:40.797727:  
2023-03-21 13:09:40.798083: Epoch 288 
2023-03-21 13:09:40.798255: Current learning rate: 0.00737 
2023-03-21 13:09:54.379244: train_loss -0.9109 
2023-03-21 13:09:54.379576: val_loss -0.841 
2023-03-21 13:09:54.379794: Pseudo dice [0.896, 0.8783] 
2023-03-21 13:09:54.380013: Epoch time: 13.58 s 
2023-03-21 13:09:55.625405:  
2023-03-21 13:09:55.625650: Epoch 289 
2023-03-21 13:09:55.625814: Current learning rate: 0.00736 
2023-03-21 13:10:09.400435: train_loss -0.9105 
2023-03-21 13:10:09.400662: val_loss -0.8432 
2023-03-21 13:10:09.400763: Pseudo dice [0.896, 0.8807] 
2023-03-21 13:10:09.400858: Epoch time: 13.78 s 
2023-03-21 13:10:10.620034:  
2023-03-21 13:10:10.620160: Epoch 290 
2023-03-21 13:10:10.620327: Current learning rate: 0.00735 
2023-03-21 13:10:24.567111: train_loss -0.9106 
2023-03-21 13:10:24.567366: val_loss -0.8477 
2023-03-21 13:10:24.567467: Pseudo dice [0.899, 0.8838] 
2023-03-21 13:10:24.567568: Epoch time: 13.95 s 
2023-03-21 13:10:25.925946:  
2023-03-21 13:10:25.926078: Epoch 291 
2023-03-21 13:10:25.926239: Current learning rate: 0.00734 
2023-03-21 13:10:39.676455: train_loss -0.9104 
2023-03-21 13:10:39.676766: val_loss -0.839 
2023-03-21 13:10:39.676901: Pseudo dice [0.8926, 0.8757] 
2023-03-21 13:10:39.677033: Epoch time: 13.75 s 
2023-03-21 13:10:40.945760:  
2023-03-21 13:10:40.945894: Epoch 292 
2023-03-21 13:10:40.946062: Current learning rate: 0.00733 
2023-03-21 13:10:54.712979: train_loss -0.9113 
2023-03-21 13:10:54.713228: val_loss -0.8406 
2023-03-21 13:10:54.713362: Pseudo dice [0.8929, 0.8773] 
2023-03-21 13:10:54.713476: Epoch time: 13.77 s 
2023-03-21 13:10:55.957387:  
2023-03-21 13:10:55.957531: Epoch 293 
2023-03-21 13:10:55.957667: Current learning rate: 0.00732 
2023-03-21 13:11:09.777707: train_loss -0.9104 
2023-03-21 13:11:09.778151: val_loss -0.8409 
2023-03-21 13:11:09.778329: Pseudo dice [0.8939, 0.8778] 
2023-03-21 13:11:09.778445: Epoch time: 13.82 s 
2023-03-21 13:11:10.990872:  
2023-03-21 13:11:10.991009: Epoch 294 
2023-03-21 13:11:10.991156: Current learning rate: 0.00731 
2023-03-21 13:11:24.795698: train_loss -0.9102 
2023-03-21 13:11:24.796002: val_loss -0.8409 
2023-03-21 13:11:24.796168: Pseudo dice [0.8944, 0.8778] 
2023-03-21 13:11:24.796334: Epoch time: 13.81 s 
2023-03-21 13:11:26.081175:  
2023-03-21 13:11:26.081303: Epoch 295 
2023-03-21 13:11:26.081454: Current learning rate: 0.0073 
2023-03-21 13:11:39.816177: train_loss -0.9097 
2023-03-21 13:11:39.816377: val_loss -0.8382 
2023-03-21 13:11:39.816489: Pseudo dice [0.8924, 0.8754] 
2023-03-21 13:11:39.816579: Epoch time: 13.74 s 
2023-03-21 13:11:41.203134:  
2023-03-21 13:11:41.203276: Epoch 296 
2023-03-21 13:11:41.203409: Current learning rate: 0.00729 
2023-03-21 13:11:54.745047: train_loss -0.9113 
2023-03-21 13:11:54.745284: val_loss -0.8454 
2023-03-21 13:11:54.745398: Pseudo dice [0.8962, 0.8809] 
2023-03-21 13:11:54.745495: Epoch time: 13.54 s 
2023-03-21 13:11:55.966380:  
2023-03-21 13:11:55.966504: Epoch 297 
2023-03-21 13:11:55.966649: Current learning rate: 0.00728 
2023-03-21 13:12:09.572594: train_loss -0.9102 
2023-03-21 13:12:09.572864: val_loss -0.8415 
2023-03-21 13:12:09.572965: Pseudo dice [0.8948, 0.8793] 
2023-03-21 13:12:09.573064: Epoch time: 13.61 s 
2023-03-21 13:12:10.826529:  
2023-03-21 13:12:10.826669: Epoch 298 
2023-03-21 13:12:10.826859: Current learning rate: 0.00727 
2023-03-21 13:12:24.796833: train_loss -0.9112 
2023-03-21 13:12:24.797287: val_loss -0.8446 
2023-03-21 13:12:24.797570: Pseudo dice [0.8957, 0.8787] 
2023-03-21 13:12:24.797904: Epoch time: 13.97 s 
2023-03-21 13:12:26.060669:  
2023-03-21 13:12:26.060798: Epoch 299 
2023-03-21 13:12:26.060933: Current learning rate: 0.00726 
2023-03-21 13:12:39.668140: train_loss -0.9111 
2023-03-21 13:12:39.668380: val_loss -0.8418 
2023-03-21 13:12:39.668514: Pseudo dice [0.8949, 0.8783] 
2023-03-21 13:12:39.668641: Epoch time: 13.61 s 
2023-03-21 13:12:41.205039:  
2023-03-21 13:12:41.205247: Epoch 300 
2023-03-21 13:12:41.205454: Current learning rate: 0.00725 
2023-03-21 13:12:54.987672: train_loss -0.9101 
2023-03-21 13:12:54.987958: val_loss -0.8467 
2023-03-21 13:12:54.988076: Pseudo dice [0.8982, 0.8799] 
2023-03-21 13:12:54.988194: Epoch time: 13.78 s 
2023-03-21 13:12:56.369753:  
2023-03-21 13:12:56.370003: Epoch 301 
2023-03-21 13:12:56.370210: Current learning rate: 0.00724 
2023-03-21 13:13:10.296277: train_loss -0.9092 
2023-03-21 13:13:10.296555: val_loss -0.8466 
2023-03-21 13:13:10.296657: Pseudo dice [0.8992, 0.8799] 
2023-03-21 13:13:10.296756: Epoch time: 13.93 s 
2023-03-21 13:13:11.522359:  
2023-03-21 13:13:11.522496: Epoch 302 
2023-03-21 13:13:11.522625: Current learning rate: 0.00724 
2023-03-21 13:13:25.626995: train_loss -0.911 
2023-03-21 13:13:25.627718: val_loss -0.842 
2023-03-21 13:13:25.627841: Pseudo dice [0.895, 0.8769] 
2023-03-21 13:13:25.627966: Epoch time: 14.11 s 
2023-03-21 13:13:26.973033:  
2023-03-21 13:13:26.973219: Epoch 303 
2023-03-21 13:13:26.973391: Current learning rate: 0.00723 
2023-03-21 13:13:41.065711: train_loss -0.9108 
2023-03-21 13:13:41.066023: val_loss -0.8459 
2023-03-21 13:13:41.066143: Pseudo dice [0.8975, 0.8803] 
2023-03-21 13:13:41.066249: Epoch time: 14.09 s 
2023-03-21 13:13:42.281370:  
2023-03-21 13:13:42.281523: Epoch 304 
2023-03-21 13:13:42.281709: Current learning rate: 0.00722 
2023-03-21 13:13:55.867165: train_loss -0.9109 
2023-03-21 13:13:55.867445: val_loss -0.8431 
2023-03-21 13:13:55.867555: Pseudo dice [0.8943, 0.8795] 
2023-03-21 13:13:55.867665: Epoch time: 13.59 s 
2023-03-21 13:13:57.239757:  
2023-03-21 13:13:57.239892: Epoch 305 
2023-03-21 13:13:57.240035: Current learning rate: 0.00721 
2023-03-21 13:14:10.948314: train_loss -0.9089 
2023-03-21 13:14:10.948601: val_loss -0.8372 
2023-03-21 13:14:10.948728: Pseudo dice [0.8906, 0.8752] 
2023-03-21 13:14:10.948838: Epoch time: 13.71 s 
2023-03-21 13:14:12.245543:  
2023-03-21 13:14:12.245802: Epoch 306 
2023-03-21 13:14:12.246064: Current learning rate: 0.0072 
2023-03-21 13:14:25.707161: train_loss -0.9119 
2023-03-21 13:14:25.708762: val_loss -0.8451 
2023-03-21 13:14:25.708876: Pseudo dice [0.8967, 0.8788] 
2023-03-21 13:14:25.708988: Epoch time: 13.46 s 
2023-03-21 13:14:26.976392:  
2023-03-21 13:14:26.976542: Epoch 307 
2023-03-21 13:14:26.976687: Current learning rate: 0.00719 
2023-03-21 13:14:40.593349: train_loss -0.9107 
2023-03-21 13:14:40.593619: val_loss -0.8398 
2023-03-21 13:14:40.593718: Pseudo dice [0.8931, 0.877] 
2023-03-21 13:14:40.593816: Epoch time: 13.62 s 
2023-03-21 13:14:41.811649:  
2023-03-21 13:14:41.811798: Epoch 308 
2023-03-21 13:14:41.811943: Current learning rate: 0.00718 
2023-03-21 13:14:55.518381: train_loss -0.9122 
2023-03-21 13:14:55.518657: val_loss -0.8435 
2023-03-21 13:14:55.518791: Pseudo dice [0.8973, 0.8785] 
2023-03-21 13:14:55.518909: Epoch time: 13.71 s 
2023-03-21 13:14:56.794438:  
2023-03-21 13:14:56.794575: Epoch 309 
2023-03-21 13:14:56.794767: Current learning rate: 0.00717 
2023-03-21 13:15:09.987997: train_loss -0.911 
2023-03-21 13:15:09.988245: val_loss -0.8414 
2023-03-21 13:15:09.988375: Pseudo dice [0.8941, 0.8763] 
2023-03-21 13:15:09.988496: Epoch time: 13.19 s 
2023-03-21 13:15:11.426887:  
2023-03-21 13:15:11.427145: Epoch 310 
2023-03-21 13:15:11.427322: Current learning rate: 0.00716 
2023-03-21 13:15:24.673593: train_loss -0.9125 
2023-03-21 13:15:24.674045: val_loss -0.8451 
2023-03-21 13:15:24.674461: Pseudo dice [0.8949, 0.8812] 
2023-03-21 13:15:24.674670: Epoch time: 13.25 s 
2023-03-21 13:15:25.896619:  
2023-03-21 13:15:25.896770: Epoch 311 
2023-03-21 13:15:25.896913: Current learning rate: 0.00715 
2023-03-21 13:15:39.705314: train_loss -0.9115 
2023-03-21 13:15:39.706815: val_loss -0.8391 
2023-03-21 13:15:39.706929: Pseudo dice [0.8946, 0.8759] 
2023-03-21 13:15:39.707031: Epoch time: 13.81 s 
2023-03-21 13:15:40.880623:  
2023-03-21 13:15:40.880770: Epoch 312 
2023-03-21 13:15:40.880915: Current learning rate: 0.00714 
2023-03-21 13:15:54.507435: train_loss -0.9115 
2023-03-21 13:15:54.507769: val_loss -0.8409 
2023-03-21 13:15:54.507910: Pseudo dice [0.8957, 0.8765] 
2023-03-21 13:15:54.508068: Epoch time: 13.63 s 
2023-03-21 13:15:55.737736:  
2023-03-21 13:15:55.737897: Epoch 313 
2023-03-21 13:15:55.738050: Current learning rate: 0.00713 
2023-03-21 13:16:09.373547: train_loss -0.9099 
2023-03-21 13:16:09.373876: val_loss -0.8431 
2023-03-21 13:16:09.374009: Pseudo dice [0.8966, 0.877] 
2023-03-21 13:16:09.374141: Epoch time: 13.64 s 
2023-03-21 13:16:10.611613:  
2023-03-21 13:16:10.611766: Epoch 314 
2023-03-21 13:16:10.611911: Current learning rate: 0.00712 
2023-03-21 13:16:24.110629: train_loss -0.9106 
2023-03-21 13:16:24.110978: val_loss -0.838 
2023-03-21 13:16:24.111118: Pseudo dice [0.8935, 0.8766] 
2023-03-21 13:16:24.111253: Epoch time: 13.5 s 
2023-03-21 13:16:25.516552:  
2023-03-21 13:16:25.516717: Epoch 315 
2023-03-21 13:16:25.516856: Current learning rate: 0.00711 
2023-03-21 13:16:39.093065: train_loss -0.9114 
2023-03-21 13:16:39.093386: val_loss -0.8427 
2023-03-21 13:16:39.093517: Pseudo dice [0.8959, 0.8797] 
2023-03-21 13:16:39.093642: Epoch time: 13.58 s 
2023-03-21 13:16:40.293832:  
2023-03-21 13:16:40.293989: Epoch 316 
2023-03-21 13:16:40.294150: Current learning rate: 0.0071 
2023-03-21 13:16:54.077205: train_loss -0.9105 
2023-03-21 13:16:54.077475: val_loss -0.8442 
2023-03-21 13:16:54.077579: Pseudo dice [0.8964, 0.879] 
2023-03-21 13:16:54.077681: Epoch time: 13.78 s 
2023-03-21 13:16:55.278386:  
2023-03-21 13:16:55.278525: Epoch 317 
2023-03-21 13:16:55.278651: Current learning rate: 0.0071 
2023-03-21 13:17:09.187910: train_loss -0.9127 
2023-03-21 13:17:09.188174: val_loss -0.845 
2023-03-21 13:17:09.188295: Pseudo dice [0.8952, 0.8817] 
2023-03-21 13:17:09.188398: Epoch time: 13.91 s 
2023-03-21 13:17:10.455155:  
2023-03-21 13:17:10.455301: Epoch 318 
2023-03-21 13:17:10.455469: Current learning rate: 0.00709 
2023-03-21 13:17:24.371739: train_loss -0.9124 
2023-03-21 13:17:24.372024: val_loss -0.8428 
2023-03-21 13:17:24.372135: Pseudo dice [0.8957, 0.879] 
2023-03-21 13:17:24.372239: Epoch time: 13.92 s 
2023-03-21 13:17:25.589085:  
2023-03-21 13:17:25.589226: Epoch 319 
2023-03-21 13:17:25.589386: Current learning rate: 0.00708 
2023-03-21 13:17:39.195956: train_loss -0.9095 
2023-03-21 13:17:39.196701: val_loss -0.8446 
2023-03-21 13:17:39.197042: Pseudo dice [0.8972, 0.8802] 
2023-03-21 13:17:39.197316: Epoch time: 13.61 s 
2023-03-21 13:17:40.527959:  
2023-03-21 13:17:40.528100: Epoch 320 
2023-03-21 13:17:40.528226: Current learning rate: 0.00707 
2023-03-21 13:17:54.010185: train_loss -0.9121 
2023-03-21 13:17:54.010461: val_loss -0.8448 
2023-03-21 13:17:54.010564: Pseudo dice [0.8971, 0.881] 
2023-03-21 13:17:54.010664: Epoch time: 13.48 s 
2023-03-21 13:17:55.258339:  
2023-03-21 13:17:55.258497: Epoch 321 
2023-03-21 13:17:55.258647: Current learning rate: 0.00706 
2023-03-21 13:18:08.733842: train_loss -0.9112 
2023-03-21 13:18:08.734113: val_loss -0.843 
2023-03-21 13:18:08.734213: Pseudo dice [0.896, 0.8788] 
2023-03-21 13:18:08.734333: Epoch time: 13.48 s 
2023-03-21 13:18:09.953575:  
2023-03-21 13:18:09.953718: Epoch 322 
2023-03-21 13:18:09.953848: Current learning rate: 0.00705 
2023-03-21 13:18:23.548807: train_loss -0.9126 
2023-03-21 13:18:23.549074: val_loss -0.8415 
2023-03-21 13:18:23.549175: Pseudo dice [0.8957, 0.8762] 
2023-03-21 13:18:23.549275: Epoch time: 13.6 s 
2023-03-21 13:18:24.729313:  
2023-03-21 13:18:24.729451: Epoch 323 
2023-03-21 13:18:24.729594: Current learning rate: 0.00704 
2023-03-21 13:18:38.509741: train_loss -0.9123 
2023-03-21 13:18:38.510031: val_loss -0.8415 
2023-03-21 13:18:38.510134: Pseudo dice [0.8956, 0.8776] 
2023-03-21 13:18:38.510235: Epoch time: 13.78 s 
2023-03-21 13:18:39.696353:  
2023-03-21 13:18:39.696491: Epoch 324 
2023-03-21 13:18:39.696633: Current learning rate: 0.00703 
2023-03-21 13:18:53.575219: train_loss -0.914 
2023-03-21 13:18:53.575551: val_loss -0.8406 
2023-03-21 13:18:53.575687: Pseudo dice [0.8931, 0.877] 
2023-03-21 13:18:53.575816: Epoch time: 13.88 s 
2023-03-21 13:18:54.963216:  
2023-03-21 13:18:54.963374: Epoch 325 
2023-03-21 13:18:54.963534: Current learning rate: 0.00702 
2023-03-21 13:19:08.634799: train_loss -0.9119 
2023-03-21 13:19:08.635193: val_loss -0.8411 
2023-03-21 13:19:08.635333: Pseudo dice [0.895, 0.8775] 
2023-03-21 13:19:08.635472: Epoch time: 13.67 s 
2023-03-21 13:19:09.906011:  
2023-03-21 13:19:09.906157: Epoch 326 
2023-03-21 13:19:09.906285: Current learning rate: 0.00701 
2023-03-21 13:19:23.624097: train_loss -0.9101 
2023-03-21 13:19:23.624364: val_loss -0.8438 
2023-03-21 13:19:23.624477: Pseudo dice [0.896, 0.88] 
2023-03-21 13:19:23.624583: Epoch time: 13.72 s 
2023-03-21 13:19:24.879550:  
2023-03-21 13:19:24.879691: Epoch 327 
2023-03-21 13:19:24.879833: Current learning rate: 0.007 
2023-03-21 13:19:38.674230: train_loss -0.9119 
2023-03-21 13:19:38.674511: val_loss -0.8383 
2023-03-21 13:19:38.674616: Pseudo dice [0.8918, 0.8771] 
2023-03-21 13:19:38.674744: Epoch time: 13.8 s 
2023-03-21 13:19:39.947448:  
2023-03-21 13:19:39.947597: Epoch 328 
2023-03-21 13:19:39.947752: Current learning rate: 0.00699 
2023-03-21 13:19:53.707120: train_loss -0.9112 
2023-03-21 13:19:53.707423: val_loss -0.8462 
2023-03-21 13:19:53.707538: Pseudo dice [0.8974, 0.8819] 
2023-03-21 13:19:53.707654: Epoch time: 13.76 s 
2023-03-21 13:19:54.916664:  
2023-03-21 13:19:54.916816: Epoch 329 
2023-03-21 13:19:54.916984: Current learning rate: 0.00698 
2023-03-21 13:20:08.826779: train_loss -0.9122 
2023-03-21 13:20:08.827313: val_loss -0.8427 
2023-03-21 13:20:08.827564: Pseudo dice [0.8949, 0.8801] 
2023-03-21 13:20:08.827848: Epoch time: 13.91 s 
2023-03-21 13:20:10.170477:  
2023-03-21 13:20:10.170645: Epoch 330 
2023-03-21 13:20:10.170818: Current learning rate: 0.00697 
2023-03-21 13:20:23.852277: train_loss -0.9126 
2023-03-21 13:20:23.853119: val_loss -0.8422 
2023-03-21 13:20:23.853223: Pseudo dice [0.8956, 0.8789] 
2023-03-21 13:20:23.853322: Epoch time: 13.68 s 
2023-03-21 13:20:25.038465:  
2023-03-21 13:20:25.038612: Epoch 331 
2023-03-21 13:20:25.038806: Current learning rate: 0.00696 
2023-03-21 13:20:38.353905: train_loss -0.9121 
2023-03-21 13:20:38.354106: val_loss -0.8384 
2023-03-21 13:20:38.354207: Pseudo dice [0.8936, 0.876] 
2023-03-21 13:20:38.354304: Epoch time: 13.32 s 
2023-03-21 13:20:39.542807:  
2023-03-21 13:20:39.542998: Epoch 332 
2023-03-21 13:20:39.543186: Current learning rate: 0.00696 
2023-03-21 13:20:53.124222: train_loss -0.9108 
2023-03-21 13:20:53.124530: val_loss -0.8436 
2023-03-21 13:20:53.124636: Pseudo dice [0.8954, 0.8791] 
2023-03-21 13:20:53.124738: Epoch time: 13.58 s 
2023-03-21 13:20:54.331652:  
2023-03-21 13:20:54.331796: Epoch 333 
2023-03-21 13:20:54.331939: Current learning rate: 0.00695 
2023-03-21 13:21:07.863026: train_loss -0.9126 
2023-03-21 13:21:07.863283: val_loss -0.8389 
2023-03-21 13:21:07.863394: Pseudo dice [0.8934, 0.8771] 
2023-03-21 13:21:07.863504: Epoch time: 13.53 s 
2023-03-21 13:21:09.261194:  
2023-03-21 13:21:09.261368: Epoch 334 
2023-03-21 13:21:09.261548: Current learning rate: 0.00694 
2023-03-21 13:21:22.662529: train_loss -0.9111 
2023-03-21 13:21:22.662821: val_loss -0.8461 
2023-03-21 13:21:22.662946: Pseudo dice [0.8976, 0.8817] 
2023-03-21 13:21:22.663063: Epoch time: 13.4 s 
2023-03-21 13:21:23.880021:  
2023-03-21 13:21:23.880162: Epoch 335 
2023-03-21 13:21:23.880300: Current learning rate: 0.00693 
2023-03-21 13:21:37.236756: train_loss -0.9111 
2023-03-21 13:21:37.237041: val_loss -0.8413 
2023-03-21 13:21:37.237144: Pseudo dice [0.8963, 0.876] 
2023-03-21 13:21:37.237248: Epoch time: 13.36 s 
2023-03-21 13:21:38.444556:  
2023-03-21 13:21:38.444698: Epoch 336 
2023-03-21 13:21:38.444843: Current learning rate: 0.00692 
2023-03-21 13:21:52.180498: train_loss -0.9124 
2023-03-21 13:21:52.180850: val_loss -0.8407 
2023-03-21 13:21:52.181068: Pseudo dice [0.8946, 0.8771] 
2023-03-21 13:21:52.181390: Epoch time: 13.74 s 
2023-03-21 13:21:53.406578:  
2023-03-21 13:21:53.406717: Epoch 337 
2023-03-21 13:21:53.406884: Current learning rate: 0.00691 
2023-03-21 13:22:07.289410: train_loss -0.9133 
2023-03-21 13:22:07.289696: val_loss -0.8418 
2023-03-21 13:22:07.289820: Pseudo dice [0.8951, 0.8781] 
2023-03-21 13:22:07.289948: Epoch time: 13.88 s 
2023-03-21 13:22:08.532465:  
2023-03-21 13:22:08.532604: Epoch 338 
2023-03-21 13:22:08.532752: Current learning rate: 0.0069 
2023-03-21 13:22:22.188709: train_loss -0.9139 
2023-03-21 13:22:22.188933: val_loss -0.8402 
2023-03-21 13:22:22.189036: Pseudo dice [0.8954, 0.8774] 
2023-03-21 13:22:22.189132: Epoch time: 13.66 s 
2023-03-21 13:22:23.566571:  
2023-03-21 13:22:23.566766: Epoch 339 
2023-03-21 13:22:23.566914: Current learning rate: 0.00689 
2023-03-21 13:22:37.327295: train_loss -0.9099 
2023-03-21 13:22:37.327660: val_loss -0.8336 
2023-03-21 13:22:37.327957: Pseudo dice [0.8893, 0.873] 
2023-03-21 13:22:37.328372: Epoch time: 13.76 s 
2023-03-21 13:22:38.596794:  
2023-03-21 13:22:38.596969: Epoch 340 
2023-03-21 13:22:38.597113: Current learning rate: 0.00688 
2023-03-21 13:22:52.221011: train_loss -0.9133 
2023-03-21 13:22:52.222562: val_loss -0.8438 
2023-03-21 13:22:52.222665: Pseudo dice [0.8964, 0.8794] 
2023-03-21 13:22:52.222836: Epoch time: 13.62 s 
2023-03-21 13:22:53.419813:  
2023-03-21 13:22:53.419976: Epoch 341 
2023-03-21 13:22:53.420107: Current learning rate: 0.00687 
2023-03-21 13:23:07.257867: train_loss -0.9114 
2023-03-21 13:23:07.258192: val_loss -0.8433 
2023-03-21 13:23:07.258326: Pseudo dice [0.8962, 0.8815] 
2023-03-21 13:23:07.258457: Epoch time: 13.84 s 
2023-03-21 13:23:08.572060:  
2023-03-21 13:23:08.572206: Epoch 342 
2023-03-21 13:23:08.572339: Current learning rate: 0.00686 
2023-03-21 13:23:22.344359: train_loss -0.9109 
2023-03-21 13:23:22.344839: val_loss -0.8433 
2023-03-21 13:23:22.344973: Pseudo dice [0.8952, 0.8796] 
2023-03-21 13:23:22.345096: Epoch time: 13.77 s 
2023-03-21 13:23:23.613292:  
2023-03-21 13:23:23.613442: Epoch 343 
2023-03-21 13:23:23.613589: Current learning rate: 0.00685 
2023-03-21 13:23:37.318454: train_loss -0.9112 
2023-03-21 13:23:37.318777: val_loss -0.8426 
2023-03-21 13:23:37.318909: Pseudo dice [0.8949, 0.8778] 
2023-03-21 13:23:37.319034: Epoch time: 13.71 s 
2023-03-21 13:23:38.702445:  
2023-03-21 13:23:38.702604: Epoch 344 
2023-03-21 13:23:38.702762: Current learning rate: 0.00684 
2023-03-21 13:23:51.907697: train_loss -0.9127 
2023-03-21 13:23:51.907891: val_loss -0.8389 
2023-03-21 13:23:51.908006: Pseudo dice [0.8932, 0.8773] 
2023-03-21 13:23:51.908101: Epoch time: 13.21 s 
2023-03-21 13:23:53.169827:  
2023-03-21 13:23:53.169975: Epoch 345 
2023-03-21 13:23:53.170135: Current learning rate: 0.00683 
2023-03-21 13:24:06.847857: train_loss -0.912 
2023-03-21 13:24:06.848243: val_loss -0.8418 
2023-03-21 13:24:06.848478: Pseudo dice [0.8943, 0.8791] 
2023-03-21 13:24:06.848768: Epoch time: 13.68 s 
2023-03-21 13:24:08.128509:  
2023-03-21 13:24:08.128646: Epoch 346 
2023-03-21 13:24:08.128804: Current learning rate: 0.00682 
2023-03-21 13:24:21.857569: train_loss -0.9116 
2023-03-21 13:24:21.857769: val_loss -0.8436 
2023-03-21 13:24:21.857887: Pseudo dice [0.8964, 0.8796] 
2023-03-21 13:24:21.857986: Epoch time: 13.73 s 
2023-03-21 13:24:23.125089:  
2023-03-21 13:24:23.125245: Epoch 347 
2023-03-21 13:24:23.125425: Current learning rate: 0.00681 
2023-03-21 13:24:36.707185: train_loss -0.9122 
2023-03-21 13:24:36.707460: val_loss -0.8398 
2023-03-21 13:24:36.707566: Pseudo dice [0.8944, 0.8755] 
2023-03-21 13:24:36.707670: Epoch time: 13.58 s 
2023-03-21 13:24:38.018334:  
2023-03-21 13:24:38.018486: Epoch 348 
2023-03-21 13:24:38.018652: Current learning rate: 0.0068 
2023-03-21 13:24:51.818148: train_loss -0.9121 
2023-03-21 13:24:51.818440: val_loss -0.8419 
2023-03-21 13:24:51.818549: Pseudo dice [0.8945, 0.8778] 
2023-03-21 13:24:51.818658: Epoch time: 13.8 s 
2023-03-21 13:24:53.157128:  
2023-03-21 13:24:53.157271: Epoch 349 
2023-03-21 13:24:53.157429: Current learning rate: 0.0068 
2023-03-21 13:25:06.659087: train_loss -0.912 
2023-03-21 13:25:06.659291: val_loss -0.8418 
2023-03-21 13:25:06.659412: Pseudo dice [0.894, 0.8787] 
2023-03-21 13:25:06.659512: Epoch time: 13.5 s 
2023-03-21 13:25:08.144921:  
2023-03-21 13:25:08.145182: Epoch 350 
2023-03-21 13:25:08.145354: Current learning rate: 0.00679 
2023-03-21 13:25:21.940595: train_loss -0.9134 
2023-03-21 13:25:21.941020: val_loss -0.8444 
2023-03-21 13:25:21.943469: Pseudo dice [0.8976, 0.8812] 
2023-03-21 13:25:21.943725: Epoch time: 13.8 s 
2023-03-21 13:25:23.224807:  
2023-03-21 13:25:23.224949: Epoch 351 
2023-03-21 13:25:23.225108: Current learning rate: 0.00678 
2023-03-21 13:25:36.730071: train_loss -0.9135 
2023-03-21 13:25:36.730348: val_loss -0.8468 
2023-03-21 13:25:36.730459: Pseudo dice [0.8979, 0.8813] 
2023-03-21 13:25:36.730573: Epoch time: 13.51 s 
2023-03-21 13:25:37.978131:  
2023-03-21 13:25:37.978283: Epoch 352 
2023-03-21 13:25:37.978462: Current learning rate: 0.00677 
2023-03-21 13:25:51.746272: train_loss -0.9129 
2023-03-21 13:25:51.746478: val_loss -0.8438 
2023-03-21 13:25:51.746583: Pseudo dice [0.8952, 0.88] 
2023-03-21 13:25:51.746683: Epoch time: 13.77 s 
2023-03-21 13:25:53.110184:  
2023-03-21 13:25:53.110364: Epoch 353 
2023-03-21 13:25:53.110567: Current learning rate: 0.00676 
2023-03-21 13:26:06.371612: train_loss -0.9142 
2023-03-21 13:26:06.372021: val_loss -0.8419 
2023-03-21 13:26:06.372220: Pseudo dice [0.8959, 0.8791] 
2023-03-21 13:26:06.372429: Epoch time: 13.26 s 
2023-03-21 13:26:07.594028:  
2023-03-21 13:26:07.594166: Epoch 354 
2023-03-21 13:26:07.594325: Current learning rate: 0.00675 
2023-03-21 13:26:21.236941: train_loss -0.9128 
2023-03-21 13:26:21.237220: val_loss -0.8436 
2023-03-21 13:26:21.237340: Pseudo dice [0.8966, 0.8813] 
2023-03-21 13:26:21.237446: Epoch time: 13.64 s 
2023-03-21 13:26:22.475344:  
2023-03-21 13:26:22.475490: Epoch 355 
2023-03-21 13:26:22.475649: Current learning rate: 0.00674 
2023-03-21 13:26:36.228608: train_loss -0.9126 
2023-03-21 13:26:36.228822: val_loss -0.8438 
2023-03-21 13:26:36.228945: Pseudo dice [0.8966, 0.8794] 
2023-03-21 13:26:36.229048: Epoch time: 13.75 s 
2023-03-21 13:26:37.489686:  
2023-03-21 13:26:37.489863: Epoch 356 
2023-03-21 13:26:37.490025: Current learning rate: 0.00673 
2023-03-21 13:26:51.251327: train_loss -0.9122 
2023-03-21 13:26:51.251626: val_loss -0.8356 
2023-03-21 13:26:51.251758: Pseudo dice [0.8906, 0.8728] 
2023-03-21 13:26:51.251870: Epoch time: 13.76 s 
2023-03-21 13:26:52.472681:  
2023-03-21 13:26:52.472814: Epoch 357 
2023-03-21 13:26:52.472958: Current learning rate: 0.00672 
2023-03-21 13:27:06.256404: train_loss -0.9121 
2023-03-21 13:27:06.256668: val_loss -0.8459 
2023-03-21 13:27:06.256776: Pseudo dice [0.8977, 0.8811] 
2023-03-21 13:27:06.256880: Epoch time: 13.78 s 
2023-03-21 13:27:07.648581:  
2023-03-21 13:27:07.648734: Epoch 358 
2023-03-21 13:27:07.648912: Current learning rate: 0.00671 
2023-03-21 13:27:21.425396: train_loss -0.9113 
2023-03-21 13:27:21.425667: val_loss -0.8361 
2023-03-21 13:27:21.425802: Pseudo dice [0.8898, 0.8747] 
2023-03-21 13:27:21.426069: Epoch time: 13.78 s 
2023-03-21 13:27:22.663876:  
2023-03-21 13:27:22.664012: Epoch 359 
2023-03-21 13:27:22.664171: Current learning rate: 0.0067 
2023-03-21 13:27:36.439063: train_loss -0.9121 
2023-03-21 13:27:36.439402: val_loss -0.8443 
2023-03-21 13:27:36.439536: Pseudo dice [0.8955, 0.8802] 
2023-03-21 13:27:36.439675: Epoch time: 13.78 s 
2023-03-21 13:27:37.765677:  
2023-03-21 13:27:37.765836: Epoch 360 
2023-03-21 13:27:37.766004: Current learning rate: 0.00669 
2023-03-21 13:27:51.449172: train_loss -0.9122 
2023-03-21 13:27:51.449499: val_loss -0.8381 
2023-03-21 13:27:51.449632: Pseudo dice [0.8908, 0.8761] 
2023-03-21 13:27:51.449766: Epoch time: 13.68 s 
2023-03-21 13:27:52.735005:  
2023-03-21 13:27:52.735147: Epoch 361 
2023-03-21 13:27:52.735297: Current learning rate: 0.00668 
2023-03-21 13:28:06.621402: train_loss -0.9117 
2023-03-21 13:28:06.621687: val_loss -0.84 
2023-03-21 13:28:06.621809: Pseudo dice [0.8933, 0.8744] 
2023-03-21 13:28:06.622635: Epoch time: 13.89 s 
2023-03-21 13:28:07.917692:  
2023-03-21 13:28:07.917825: Epoch 362 
2023-03-21 13:28:07.917971: Current learning rate: 0.00667 
2023-03-21 13:28:21.719262: train_loss -0.9124 
2023-03-21 13:28:21.719672: val_loss -0.8422 
2023-03-21 13:28:21.720080: Pseudo dice [0.8954, 0.8783] 
2023-03-21 13:28:21.720353: Epoch time: 13.8 s 
2023-03-21 13:28:23.119044:  
2023-03-21 13:28:23.119197: Epoch 363 
2023-03-21 13:28:23.119360: Current learning rate: 0.00666 
2023-03-21 13:28:37.046797: train_loss -0.9132 
2023-03-21 13:28:37.047094: val_loss -0.8345 
2023-03-21 13:28:37.047211: Pseudo dice [0.8909, 0.8731] 
2023-03-21 13:28:37.047328: Epoch time: 13.93 s 
2023-03-21 13:28:38.307797:  
2023-03-21 13:28:38.307938: Epoch 364 
2023-03-21 13:28:38.308089: Current learning rate: 0.00665 
2023-03-21 13:28:51.542084: train_loss -0.9144 
2023-03-21 13:28:51.542347: val_loss -0.8434 
2023-03-21 13:28:51.542467: Pseudo dice [0.8963, 0.8796] 
2023-03-21 13:28:51.542569: Epoch time: 13.23 s 
2023-03-21 13:28:52.816752:  
2023-03-21 13:28:52.816887: Epoch 365 
2023-03-21 13:28:52.817051: Current learning rate: 0.00665 
2023-03-21 13:29:06.484899: train_loss -0.9138 
2023-03-21 13:29:06.485144: val_loss -0.8383 
2023-03-21 13:29:06.485247: Pseudo dice [0.8928, 0.8762] 
2023-03-21 13:29:06.485351: Epoch time: 13.67 s 
2023-03-21 13:29:07.735848:  
2023-03-21 13:29:07.735983: Epoch 366 
2023-03-21 13:29:07.736145: Current learning rate: 0.00664 
2023-03-21 13:29:21.437409: train_loss -0.9131 
2023-03-21 13:29:21.437628: val_loss -0.8454 
2023-03-21 13:29:21.437728: Pseudo dice [0.8972, 0.8805] 
2023-03-21 13:29:21.437825: Epoch time: 13.7 s 
2023-03-21 13:29:22.686188:  
2023-03-21 13:29:22.686342: Epoch 367 
2023-03-21 13:29:22.686489: Current learning rate: 0.00663 
2023-03-21 13:29:36.151929: train_loss -0.9121 
2023-03-21 13:29:36.152204: val_loss -0.8414 
2023-03-21 13:29:36.152326: Pseudo dice [0.8961, 0.8768] 
2023-03-21 13:29:36.152427: Epoch time: 13.47 s 
2023-03-21 13:29:37.544232:  
2023-03-21 13:29:37.544377: Epoch 368 
2023-03-21 13:29:37.544557: Current learning rate: 0.00662 
2023-03-21 13:29:51.332695: train_loss -0.9113 
2023-03-21 13:29:51.332990: val_loss -0.8395 
2023-03-21 13:29:51.333097: Pseudo dice [0.895, 0.8753] 
2023-03-21 13:29:51.333204: Epoch time: 13.79 s 
2023-03-21 13:29:52.579293:  
2023-03-21 13:29:52.579432: Epoch 369 
2023-03-21 13:29:52.579574: Current learning rate: 0.00661 
2023-03-21 13:30:06.338146: train_loss -0.9146 
2023-03-21 13:30:06.338441: val_loss -0.841 
2023-03-21 13:30:06.338653: Pseudo dice [0.8954, 0.8788] 
2023-03-21 13:30:06.338926: Epoch time: 13.76 s 
2023-03-21 13:30:07.533026:  
2023-03-21 13:30:07.533163: Epoch 370 
2023-03-21 13:30:07.533324: Current learning rate: 0.0066 
2023-03-21 13:30:21.064344: train_loss -0.9154 
2023-03-21 13:30:21.064650: val_loss -0.8408 
2023-03-21 13:30:21.064770: Pseudo dice [0.8952, 0.8759] 
2023-03-21 13:30:21.064871: Epoch time: 13.53 s 
2023-03-21 13:30:22.301044:  
2023-03-21 13:30:22.301193: Epoch 371 
2023-03-21 13:30:22.301336: Current learning rate: 0.00659 
2023-03-21 13:30:35.866488: train_loss -0.915 
2023-03-21 13:30:35.866847: val_loss -0.8435 
2023-03-21 13:30:35.867004: Pseudo dice [0.8971, 0.8789] 
2023-03-21 13:30:35.867150: Epoch time: 13.57 s 
2023-03-21 13:30:37.260938:  
2023-03-21 13:30:37.261097: Epoch 372 
2023-03-21 13:30:37.261276: Current learning rate: 0.00658 
2023-03-21 13:30:51.241863: train_loss -0.9131 
2023-03-21 13:30:51.242151: val_loss -0.8458 
2023-03-21 13:30:51.242265: Pseudo dice [0.8978, 0.8811] 
2023-03-21 13:30:51.242379: Epoch time: 13.98 s 
2023-03-21 13:30:52.500515:  
2023-03-21 13:30:52.500654: Epoch 373 
2023-03-21 13:30:52.500802: Current learning rate: 0.00657 
2023-03-21 13:31:06.283053: train_loss -0.916 
2023-03-21 13:31:06.283566: val_loss -0.8418 
2023-03-21 13:31:06.283800: Pseudo dice [0.8955, 0.8786] 
2023-03-21 13:31:06.284014: Epoch time: 13.78 s 
2023-03-21 13:31:07.522933:  
2023-03-21 13:31:07.523089: Epoch 374 
2023-03-21 13:31:07.523251: Current learning rate: 0.00656 
2023-03-21 13:31:20.956790: train_loss -0.9147 
2023-03-21 13:31:20.957072: val_loss -0.8339 
2023-03-21 13:31:20.957179: Pseudo dice [0.8895, 0.8744] 
2023-03-21 13:31:20.957286: Epoch time: 13.43 s 
2023-03-21 13:31:22.207205:  
2023-03-21 13:31:22.207380: Epoch 375 
2023-03-21 13:31:22.207516: Current learning rate: 0.00655 
2023-03-21 13:31:35.864242: train_loss -0.9152 
2023-03-21 13:31:35.864483: val_loss -0.8332 
2023-03-21 13:31:35.864612: Pseudo dice [0.8891, 0.8731] 
2023-03-21 13:31:35.864730: Epoch time: 13.66 s 
2023-03-21 13:31:37.099543:  
2023-03-21 13:31:37.099675: Epoch 376 
2023-03-21 13:31:37.099819: Current learning rate: 0.00654 
2023-03-21 13:31:50.835130: train_loss -0.9153 
2023-03-21 13:31:50.835417: val_loss -0.8395 
2023-03-21 13:31:50.835517: Pseudo dice [0.8925, 0.8777] 
2023-03-21 13:31:50.835618: Epoch time: 13.74 s 
2023-03-21 13:31:52.223952:  
2023-03-21 13:31:52.224097: Epoch 377 
2023-03-21 13:31:52.224226: Current learning rate: 0.00653 
2023-03-21 13:32:05.747953: train_loss -0.9137 
2023-03-21 13:32:05.748436: val_loss -0.8391 
2023-03-21 13:32:05.748659: Pseudo dice [0.893, 0.8765] 
2023-03-21 13:32:05.748897: Epoch time: 13.52 s 
2023-03-21 13:32:06.983963:  
2023-03-21 13:32:06.984120: Epoch 378 
2023-03-21 13:32:06.984271: Current learning rate: 0.00652 
2023-03-21 13:32:20.727464: train_loss -0.9152 
2023-03-21 13:32:20.727787: val_loss -0.8393 
2023-03-21 13:32:20.727920: Pseudo dice [0.8943, 0.8765] 
2023-03-21 13:32:20.728051: Epoch time: 13.74 s 
2023-03-21 13:32:22.024848:  
2023-03-21 13:32:22.025006: Epoch 379 
2023-03-21 13:32:22.025159: Current learning rate: 0.00651 
2023-03-21 13:32:35.613034: train_loss -0.9142 
2023-03-21 13:32:35.613305: val_loss -0.8342 
2023-03-21 13:32:35.613409: Pseudo dice [0.8906, 0.8748] 
2023-03-21 13:32:35.613528: Epoch time: 13.59 s 
2023-03-21 13:32:36.860862:  
2023-03-21 13:32:36.861014: Epoch 380 
2023-03-21 13:32:36.861161: Current learning rate: 0.0065 
2023-03-21 13:32:50.473364: train_loss -0.9155 
2023-03-21 13:32:50.473621: val_loss -0.8398 
2023-03-21 13:32:50.473720: Pseudo dice [0.8946, 0.8778] 
2023-03-21 13:32:50.473821: Epoch time: 13.61 s 
2023-03-21 13:32:51.763783:  
2023-03-21 13:32:51.763922: Epoch 381 
2023-03-21 13:32:51.764068: Current learning rate: 0.00649 
2023-03-21 13:33:05.597324: train_loss -0.9162 
2023-03-21 13:33:05.597538: val_loss -0.8422 
2023-03-21 13:33:05.597652: Pseudo dice [0.8964, 0.8814] 
2023-03-21 13:33:05.597747: Epoch time: 13.83 s 
2023-03-21 13:33:07.020810:  
2023-03-21 13:33:07.020982: Epoch 382 
2023-03-21 13:33:07.021187: Current learning rate: 0.00648 
2023-03-21 13:33:20.757794: train_loss -0.9149 
2023-03-21 13:33:20.758093: val_loss -0.8444 
2023-03-21 13:33:20.758223: Pseudo dice [0.8967, 0.8817] 
2023-03-21 13:33:20.758347: Epoch time: 13.74 s 
2023-03-21 13:33:22.052762:  
2023-03-21 13:33:22.052908: Epoch 383 
2023-03-21 13:33:22.053036: Current learning rate: 0.00648 
2023-03-21 13:33:35.665116: train_loss -0.9147 
2023-03-21 13:33:35.665355: val_loss -0.8405 
2023-03-21 13:33:35.665463: Pseudo dice [0.8951, 0.8778] 
2023-03-21 13:33:35.665566: Epoch time: 13.61 s 
2023-03-21 13:33:37.032242:  
2023-03-21 13:33:37.032391: Epoch 384 
2023-03-21 13:33:37.032520: Current learning rate: 0.00647 
2023-03-21 13:33:50.620774: train_loss -0.9151 
2023-03-21 13:33:50.621095: val_loss -0.8433 
2023-03-21 13:33:50.621305: Pseudo dice [0.899, 0.8792] 
2023-03-21 13:33:50.621513: Epoch time: 13.59 s 
2023-03-21 13:33:51.947741:  
2023-03-21 13:33:51.947881: Epoch 385 
2023-03-21 13:33:51.948009: Current learning rate: 0.00646 
2023-03-21 13:34:05.664353: train_loss -0.9153 
2023-03-21 13:34:05.664609: val_loss -0.8394 
2023-03-21 13:34:05.664709: Pseudo dice [0.8939, 0.8761] 
2023-03-21 13:34:05.664809: Epoch time: 13.72 s 
2023-03-21 13:34:06.910519:  
2023-03-21 13:34:06.910672: Epoch 386 
2023-03-21 13:34:06.910870: Current learning rate: 0.00645 
2023-03-21 13:34:20.766574: train_loss -0.9146 
2023-03-21 13:34:20.767051: val_loss -0.8426 
2023-03-21 13:34:20.767447: Pseudo dice [0.8943, 0.8799] 
2023-03-21 13:34:20.767614: Epoch time: 13.86 s 
2023-03-21 13:34:22.203593:  
2023-03-21 13:34:22.203751: Epoch 387 
2023-03-21 13:34:22.203900: Current learning rate: 0.00644 
2023-03-21 13:34:36.068165: train_loss -0.914 
2023-03-21 13:34:36.068577: val_loss -0.8414 
2023-03-21 13:34:36.068727: Pseudo dice [0.8956, 0.8779] 
2023-03-21 13:34:36.068847: Epoch time: 13.87 s 
2023-03-21 13:34:37.386327:  
2023-03-21 13:34:37.386494: Epoch 388 
2023-03-21 13:34:37.386643: Current learning rate: 0.00643 
2023-03-21 13:34:50.689098: train_loss -0.9153 
2023-03-21 13:34:50.689539: val_loss -0.8406 
2023-03-21 13:34:50.689649: Pseudo dice [0.8951, 0.8773] 
2023-03-21 13:34:50.689760: Epoch time: 13.3 s 
2023-03-21 13:34:51.960028:  
2023-03-21 13:34:51.960195: Epoch 389 
2023-03-21 13:34:51.960346: Current learning rate: 0.00642 
2023-03-21 13:35:05.567160: train_loss -0.9136 
2023-03-21 13:35:05.567469: val_loss -0.8417 
2023-03-21 13:35:05.567600: Pseudo dice [0.8954, 0.879] 
2023-03-21 13:35:05.567728: Epoch time: 13.61 s 
2023-03-21 13:35:06.858509:  
2023-03-21 13:35:06.858653: Epoch 390 
2023-03-21 13:35:06.858821: Current learning rate: 0.00641 
2023-03-21 13:35:20.512483: train_loss -0.9154 
2023-03-21 13:35:20.513980: val_loss -0.8402 
2023-03-21 13:35:20.514083: Pseudo dice [0.8943, 0.8773] 
2023-03-21 13:35:20.514207: Epoch time: 13.65 s 
2023-03-21 13:35:21.896628:  
2023-03-21 13:35:21.896904: Epoch 391 
2023-03-21 13:35:21.897168: Current learning rate: 0.0064 
2023-03-21 13:35:35.734074: train_loss -0.9144 
2023-03-21 13:35:35.734408: val_loss -0.8414 
2023-03-21 13:35:35.734640: Pseudo dice [0.8949, 0.8771] 
2023-03-21 13:35:35.734973: Epoch time: 13.84 s 
2023-03-21 13:35:37.063441:  
2023-03-21 13:35:37.063590: Epoch 392 
2023-03-21 13:35:37.063735: Current learning rate: 0.00639 
2023-03-21 13:35:50.526034: train_loss -0.9158 
2023-03-21 13:35:50.526489: val_loss -0.8432 
2023-03-21 13:35:50.526841: Pseudo dice [0.8944, 0.8803] 
2023-03-21 13:35:50.527195: Epoch time: 13.46 s 
2023-03-21 13:35:51.801712:  
2023-03-21 13:35:51.801885: Epoch 393 
2023-03-21 13:35:51.802062: Current learning rate: 0.00638 
2023-03-21 13:36:05.395156: train_loss -0.9156 
2023-03-21 13:36:05.395422: val_loss -0.8411 
2023-03-21 13:36:05.395529: Pseudo dice [0.8968, 0.8786] 
2023-03-21 13:36:05.395635: Epoch time: 13.59 s 
2023-03-21 13:36:06.716329:  
2023-03-21 13:36:06.716492: Epoch 394 
2023-03-21 13:36:06.716638: Current learning rate: 0.00637 
2023-03-21 13:36:20.357006: train_loss -0.915 
2023-03-21 13:36:20.357328: val_loss -0.8467 
2023-03-21 13:36:20.357461: Pseudo dice [0.8992, 0.882] 
2023-03-21 13:36:20.357595: Epoch time: 13.64 s 
2023-03-21 13:36:21.638575:  
2023-03-21 13:36:21.638754: Epoch 395 
2023-03-21 13:36:21.638907: Current learning rate: 0.00636 
2023-03-21 13:36:35.063994: train_loss -0.9145 
2023-03-21 13:36:35.064387: val_loss -0.8394 
2023-03-21 13:36:35.064653: Pseudo dice [0.8926, 0.8773] 
2023-03-21 13:36:35.064778: Epoch time: 13.43 s 
2023-03-21 13:36:36.488136:  
2023-03-21 13:36:36.488280: Epoch 396 
2023-03-21 13:36:36.488409: Current learning rate: 0.00635 
2023-03-21 13:36:50.121154: train_loss -0.9149 
2023-03-21 13:36:50.122009: val_loss -0.8472 
2023-03-21 13:36:50.122116: Pseudo dice [0.8973, 0.8834] 
2023-03-21 13:36:50.122224: Epoch time: 13.63 s 
2023-03-21 13:36:51.343442:  
2023-03-21 13:36:51.343590: Epoch 397 
2023-03-21 13:36:51.343730: Current learning rate: 0.00634 
2023-03-21 13:37:04.810598: train_loss -0.914 
2023-03-21 13:37:04.810951: val_loss -0.8451 
2023-03-21 13:37:04.811076: Pseudo dice [0.8974, 0.8824] 
2023-03-21 13:37:04.811183: Epoch time: 13.47 s 
2023-03-21 13:37:06.058333:  
2023-03-21 13:37:06.058484: Epoch 398 
2023-03-21 13:37:06.058633: Current learning rate: 0.00633 
2023-03-21 13:37:19.849234: train_loss -0.9152 
2023-03-21 13:37:19.849490: val_loss -0.8466 
2023-03-21 13:37:19.849588: Pseudo dice [0.8985, 0.8814] 
2023-03-21 13:37:19.849689: Epoch time: 13.79 s 
2023-03-21 13:37:21.132877:  
2023-03-21 13:37:21.133023: Epoch 399 
2023-03-21 13:37:21.133190: Current learning rate: 0.00632 
2023-03-21 13:37:34.573567: train_loss -0.9156 
2023-03-21 13:37:34.573750: val_loss -0.8395 
2023-03-21 13:37:34.573852: Pseudo dice [0.8935, 0.8773] 
2023-03-21 13:37:34.573948: Epoch time: 13.44 s 
2023-03-21 13:37:36.205436:  
2023-03-21 13:37:36.205581: Epoch 400 
2023-03-21 13:37:36.205734: Current learning rate: 0.00631 
2023-03-21 13:37:49.819366: train_loss -0.9148 
2023-03-21 13:37:49.819798: val_loss -0.8424 
2023-03-21 13:37:49.819967: Pseudo dice [0.8971, 0.88] 
2023-03-21 13:37:49.820196: Epoch time: 13.61 s 
2023-03-21 13:37:51.083467:  
2023-03-21 13:37:51.083628: Epoch 401 
2023-03-21 13:37:51.083793: Current learning rate: 0.0063 
2023-03-21 13:38:04.680966: train_loss -0.9159 
2023-03-21 13:38:04.681420: val_loss -0.8427 
2023-03-21 13:38:04.681640: Pseudo dice [0.8953, 0.8799] 
2023-03-21 13:38:04.681876: Epoch time: 13.6 s 
2023-03-21 13:38:05.919144:  
2023-03-21 13:38:05.919286: Epoch 402 
2023-03-21 13:38:05.919433: Current learning rate: 0.0063 
2023-03-21 13:38:19.451437: train_loss -0.9162 
2023-03-21 13:38:19.451826: val_loss -0.8374 
2023-03-21 13:38:19.452179: Pseudo dice [0.8922, 0.8759] 
2023-03-21 13:38:19.452350: Epoch time: 13.53 s 
2023-03-21 13:38:20.685662:  
2023-03-21 13:38:20.685810: Epoch 403 
2023-03-21 13:38:20.685942: Current learning rate: 0.00629 
2023-03-21 13:38:34.391323: train_loss -0.9176 
2023-03-21 13:38:34.391640: val_loss -0.8446 
2023-03-21 13:38:34.391770: Pseudo dice [0.897, 0.8815] 
2023-03-21 13:38:34.391898: Epoch time: 13.71 s 
2023-03-21 13:38:35.688083:  
2023-03-21 13:38:35.688237: Epoch 404 
2023-03-21 13:38:35.688388: Current learning rate: 0.00628 
2023-03-21 13:38:49.387005: train_loss -0.9152 
2023-03-21 13:38:49.387190: val_loss -0.8389 
2023-03-21 13:38:49.387292: Pseudo dice [0.8939, 0.8761] 
2023-03-21 13:38:49.387391: Epoch time: 13.7 s 
2023-03-21 13:38:50.732167:  
2023-03-21 13:38:50.732320: Epoch 405 
2023-03-21 13:38:50.732453: Current learning rate: 0.00627 
2023-03-21 13:39:04.498346: train_loss -0.9138 
2023-03-21 13:39:04.498656: val_loss -0.8452 
2023-03-21 13:39:04.498818: Pseudo dice [0.8968, 0.8789] 
2023-03-21 13:39:04.498959: Epoch time: 13.77 s 
2023-03-21 13:39:05.788403:  
2023-03-21 13:39:05.788613: Epoch 406 
2023-03-21 13:39:05.788780: Current learning rate: 0.00626 
2023-03-21 13:39:19.400988: train_loss -0.9144 
2023-03-21 13:39:19.401219: val_loss -0.8414 
2023-03-21 13:39:19.401348: Pseudo dice [0.8943, 0.8789] 
2023-03-21 13:39:19.401470: Epoch time: 13.61 s 
2023-03-21 13:39:20.657744:  
2023-03-21 13:39:20.657890: Epoch 407 
2023-03-21 13:39:20.658020: Current learning rate: 0.00625 
2023-03-21 13:39:34.174974: train_loss -0.9137 
2023-03-21 13:39:34.175238: val_loss -0.8417 
2023-03-21 13:39:34.175368: Pseudo dice [0.895, 0.8777] 
2023-03-21 13:39:34.175472: Epoch time: 13.52 s 
2023-03-21 13:39:35.426910:  
2023-03-21 13:39:35.427049: Epoch 408 
2023-03-21 13:39:35.427195: Current learning rate: 0.00624 
2023-03-21 13:39:49.025614: train_loss -0.9146 
2023-03-21 13:39:49.025813: val_loss -0.8402 
2023-03-21 13:39:49.025914: Pseudo dice [0.8937, 0.8779] 
2023-03-21 13:39:49.026010: Epoch time: 13.6 s 
2023-03-21 13:39:50.230775:  
2023-03-21 13:39:50.230928: Epoch 409 
2023-03-21 13:39:50.231070: Current learning rate: 0.00623 
2023-03-21 13:40:03.707341: train_loss -0.9173 
2023-03-21 13:40:03.707593: val_loss -0.8376 
2023-03-21 13:40:03.707700: Pseudo dice [0.8922, 0.8762] 
2023-03-21 13:40:03.707809: Epoch time: 13.48 s 
2023-03-21 13:40:05.071124:  
2023-03-21 13:40:05.071284: Epoch 410 
2023-03-21 13:40:05.071428: Current learning rate: 0.00622 
2023-03-21 13:40:18.742843: train_loss -0.9162 
2023-03-21 13:40:18.743355: val_loss -0.8435 
2023-03-21 13:40:18.743576: Pseudo dice [0.8958, 0.8809] 
2023-03-21 13:40:18.743786: Epoch time: 13.67 s 
2023-03-21 13:40:19.925054:  
2023-03-21 13:40:19.925204: Epoch 411 
2023-03-21 13:40:19.925349: Current learning rate: 0.00621 
2023-03-21 13:40:33.366468: train_loss -0.9148 
2023-03-21 13:40:33.366667: val_loss -0.844 
2023-03-21 13:40:33.366827: Pseudo dice [0.8979, 0.8804] 
2023-03-21 13:40:33.366939: Epoch time: 13.44 s 
2023-03-21 13:40:34.531743:  
2023-03-21 13:40:34.531886: Epoch 412 
2023-03-21 13:40:34.532022: Current learning rate: 0.0062 
2023-03-21 13:40:48.102914: train_loss -0.9144 
2023-03-21 13:40:48.103226: val_loss -0.8355 
2023-03-21 13:40:48.103329: Pseudo dice [0.8915, 0.877] 
2023-03-21 13:40:48.103432: Epoch time: 13.57 s 
2023-03-21 13:40:49.269488:  
2023-03-21 13:40:49.269630: Epoch 413 
2023-03-21 13:40:49.269802: Current learning rate: 0.00619 
2023-03-21 13:41:02.882646: train_loss -0.9161 
2023-03-21 13:41:02.882966: val_loss -0.8454 
2023-03-21 13:41:02.883080: Pseudo dice [0.8967, 0.8813] 
2023-03-21 13:41:02.883190: Epoch time: 13.61 s 
2023-03-21 13:41:04.051162:  
2023-03-21 13:41:04.051316: Epoch 414 
2023-03-21 13:41:04.051466: Current learning rate: 0.00618 
2023-03-21 13:41:17.506510: train_loss -0.9172 
2023-03-21 13:41:17.506705: val_loss -0.8421 
2023-03-21 13:41:17.506845: Pseudo dice [0.8955, 0.8792] 
2023-03-21 13:41:17.506943: Epoch time: 13.46 s 
2023-03-21 13:41:18.818393:  
2023-03-21 13:41:18.818543: Epoch 415 
2023-03-21 13:41:18.818678: Current learning rate: 0.00617 
2023-03-21 13:41:32.247430: train_loss -0.9153 
2023-03-21 13:41:32.247692: val_loss -0.8372 
2023-03-21 13:41:32.247794: Pseudo dice [0.8932, 0.8751] 
2023-03-21 13:41:32.247894: Epoch time: 13.43 s 
2023-03-21 13:41:33.457350:  
2023-03-21 13:41:33.457497: Epoch 416 
2023-03-21 13:41:33.457626: Current learning rate: 0.00616 
2023-03-21 13:41:47.204989: train_loss -0.9158 
2023-03-21 13:41:47.205264: val_loss -0.8375 
2023-03-21 13:41:47.205386: Pseudo dice [0.8925, 0.8755] 
2023-03-21 13:41:47.205492: Epoch time: 13.75 s 
2023-03-21 13:41:48.370435:  
2023-03-21 13:41:48.370589: Epoch 417 
2023-03-21 13:41:48.370761: Current learning rate: 0.00615 
2023-03-21 13:42:02.015052: train_loss -0.9144 
2023-03-21 13:42:02.015395: val_loss -0.8463 
2023-03-21 13:42:02.015622: Pseudo dice [0.8981, 0.8807] 
2023-03-21 13:42:02.015912: Epoch time: 13.65 s 
2023-03-21 13:42:03.235708:  
2023-03-21 13:42:03.235862: Epoch 418 
2023-03-21 13:42:03.236007: Current learning rate: 0.00614 
2023-03-21 13:42:16.855192: train_loss -0.9143 
2023-03-21 13:42:16.855583: val_loss -0.8387 
2023-03-21 13:42:16.855883: Pseudo dice [0.894, 0.8766] 
2023-03-21 13:42:16.856369: Epoch time: 13.62 s 
2023-03-21 13:42:18.051922:  
2023-03-21 13:42:18.052071: Epoch 419 
2023-03-21 13:42:18.052199: Current learning rate: 0.00613 
2023-03-21 13:42:31.848859: train_loss -0.9144 
2023-03-21 13:42:31.849195: val_loss -0.8401 
2023-03-21 13:42:31.849302: Pseudo dice [0.8948, 0.8777] 
2023-03-21 13:42:31.849402: Epoch time: 13.8 s 
2023-03-21 13:42:33.141662:  
2023-03-21 13:42:33.141812: Epoch 420 
2023-03-21 13:42:33.141942: Current learning rate: 0.00612 
2023-03-21 13:42:46.966764: train_loss -0.9155 
2023-03-21 13:42:46.967075: val_loss -0.8438 
2023-03-21 13:42:46.967204: Pseudo dice [0.8979, 0.8807] 
2023-03-21 13:42:46.967329: Epoch time: 13.83 s 
2023-03-21 13:42:48.155349:  
2023-03-21 13:42:48.155506: Epoch 421 
2023-03-21 13:42:48.155647: Current learning rate: 0.00612 
2023-03-21 13:43:01.514091: train_loss -0.9158 
2023-03-21 13:43:01.514293: val_loss -0.8388 
2023-03-21 13:43:01.514397: Pseudo dice [0.8938, 0.8778] 
2023-03-21 13:43:01.514495: Epoch time: 13.36 s 
2023-03-21 13:43:02.655327:  
2023-03-21 13:43:02.655483: Epoch 422 
2023-03-21 13:43:02.655629: Current learning rate: 0.00611 
2023-03-21 13:43:16.337906: train_loss -0.9153 
2023-03-21 13:43:16.338184: val_loss -0.8428 
2023-03-21 13:43:16.338291: Pseudo dice [0.8968, 0.8806] 
2023-03-21 13:43:16.338398: Epoch time: 13.68 s 
2023-03-21 13:43:17.513500:  
2023-03-21 13:43:17.513649: Epoch 423 
2023-03-21 13:43:17.513781: Current learning rate: 0.0061 
2023-03-21 13:43:31.433648: train_loss -0.9172 
2023-03-21 13:43:31.433964: val_loss -0.8396 
2023-03-21 13:43:31.434098: Pseudo dice [0.8946, 0.8766] 
2023-03-21 13:43:31.434231: Epoch time: 13.92 s 
2023-03-21 13:43:32.745762:  
2023-03-21 13:43:32.745902: Epoch 424 
2023-03-21 13:43:32.746031: Current learning rate: 0.00609 
2023-03-21 13:43:46.650493: train_loss -0.9167 
2023-03-21 13:43:46.650713: val_loss -0.8404 
2023-03-21 13:43:46.650875: Pseudo dice [0.8956, 0.8775] 
2023-03-21 13:43:46.650998: Epoch time: 13.91 s 
2023-03-21 13:43:47.826897:  
2023-03-21 13:43:47.827054: Epoch 425 
2023-03-21 13:43:47.827227: Current learning rate: 0.00608 
2023-03-21 13:44:01.649864: train_loss -0.9152 
2023-03-21 13:44:01.650176: val_loss -0.8414 
2023-03-21 13:44:01.650308: Pseudo dice [0.8945, 0.8796] 
2023-03-21 13:44:01.650435: Epoch time: 13.82 s 
2023-03-21 13:44:02.993511:  
2023-03-21 13:44:02.993651: Epoch 426 
2023-03-21 13:44:02.993780: Current learning rate: 0.00607 
2023-03-21 13:44:16.602921: train_loss -0.9168 
2023-03-21 13:44:16.603148: val_loss -0.8389 
2023-03-21 13:44:16.603259: Pseudo dice [0.8936, 0.8784] 
2023-03-21 13:44:16.603364: Epoch time: 13.61 s 
2023-03-21 13:44:17.835391:  
2023-03-21 13:44:17.835607: Epoch 427 
2023-03-21 13:44:17.835787: Current learning rate: 0.00606 
2023-03-21 13:44:31.590753: train_loss -0.9156 
2023-03-21 13:44:31.591202: val_loss -0.844 
2023-03-21 13:44:31.591363: Pseudo dice [0.8975, 0.8806] 
2023-03-21 13:44:31.591628: Epoch time: 13.76 s 
2023-03-21 13:44:32.741020:  
2023-03-21 13:44:32.741156: Epoch 428 
2023-03-21 13:44:32.741282: Current learning rate: 0.00605 
2023-03-21 13:44:46.332257: train_loss -0.9166 
2023-03-21 13:44:46.332577: val_loss -0.8415 
2023-03-21 13:44:46.332705: Pseudo dice [0.895, 0.8786] 
2023-03-21 13:44:46.332830: Epoch time: 13.59 s 
2023-03-21 13:44:47.533283:  
2023-03-21 13:44:47.533432: Epoch 429 
2023-03-21 13:44:47.533558: Current learning rate: 0.00604 
2023-03-21 13:45:01.374125: train_loss -0.9158 
2023-03-21 13:45:01.374330: val_loss -0.839 
2023-03-21 13:45:01.374433: Pseudo dice [0.8935, 0.8783] 
2023-03-21 13:45:01.374529: Epoch time: 13.84 s 
2023-03-21 13:45:02.531375:  
2023-03-21 13:45:02.531523: Epoch 430 
2023-03-21 13:45:02.531651: Current learning rate: 0.00603 
2023-03-21 13:45:16.220488: train_loss -0.9161 
2023-03-21 13:45:16.220743: val_loss -0.8456 
2023-03-21 13:45:16.220899: Pseudo dice [0.8983, 0.8817] 
2023-03-21 13:45:16.221011: Epoch time: 13.69 s 
2023-03-21 13:45:17.438345:  
2023-03-21 13:45:17.438497: Epoch 431 
2023-03-21 13:45:17.438629: Current learning rate: 0.00602 
2023-03-21 13:45:31.070857: train_loss -0.9165 
2023-03-21 13:45:31.071206: val_loss -0.8432 
2023-03-21 13:45:31.071322: Pseudo dice [0.8961, 0.8793] 
2023-03-21 13:45:31.071438: Epoch time: 13.63 s 
2023-03-21 13:45:32.260537:  
2023-03-21 13:45:32.260676: Epoch 432 
2023-03-21 13:45:32.260804: Current learning rate: 0.00601 
2023-03-21 13:45:46.036951: train_loss -0.9159 
2023-03-21 13:45:46.037160: val_loss -0.8421 
2023-03-21 13:45:46.037265: Pseudo dice [0.8951, 0.8785] 
2023-03-21 13:45:46.037364: Epoch time: 13.78 s 
2023-03-21 13:45:47.206997:  
2023-03-21 13:45:47.207135: Epoch 433 
2023-03-21 13:45:47.207263: Current learning rate: 0.006 
2023-03-21 13:46:00.785769: train_loss -0.9161 
2023-03-21 13:46:00.786067: val_loss -0.8414 
2023-03-21 13:46:00.786186: Pseudo dice [0.8953, 0.8775] 
2023-03-21 13:46:00.786295: Epoch time: 13.58 s 
2023-03-21 13:46:01.978548:  
2023-03-21 13:46:01.978714: Epoch 434 
2023-03-21 13:46:01.978955: Current learning rate: 0.00599 
2023-03-21 13:46:15.908122: train_loss -0.9155 
2023-03-21 13:46:15.908645: val_loss -0.8424 
2023-03-21 13:46:15.908751: Pseudo dice [0.8972, 0.88] 
2023-03-21 13:46:15.908972: Epoch time: 13.93 s 
2023-03-21 13:46:17.082638:  
2023-03-21 13:46:17.082803: Epoch 435 
2023-03-21 13:46:17.082952: Current learning rate: 0.00598 
2023-03-21 13:46:30.951779: train_loss -0.9157 
2023-03-21 13:46:30.952114: val_loss -0.8374 
2023-03-21 13:46:30.952353: Pseudo dice [0.8938, 0.8782] 
2023-03-21 13:46:30.952459: Epoch time: 13.87 s 
2023-03-21 13:46:32.124519:  
2023-03-21 13:46:32.124662: Epoch 436 
2023-03-21 13:46:32.124793: Current learning rate: 0.00597 
2023-03-21 13:46:45.964808: train_loss -0.9166 
2023-03-21 13:46:45.965075: val_loss -0.8416 
2023-03-21 13:46:45.965181: Pseudo dice [0.8955, 0.8785] 
2023-03-21 13:46:45.965290: Epoch time: 13.84 s 
2023-03-21 13:46:47.130912:  
2023-03-21 13:46:47.131063: Epoch 437 
2023-03-21 13:46:47.131217: Current learning rate: 0.00596 
2023-03-21 13:47:01.116566: train_loss -0.9174 
2023-03-21 13:47:01.116939: val_loss -0.8354 
2023-03-21 13:47:01.117176: Pseudo dice [0.8911, 0.8763] 
2023-03-21 13:47:01.117454: Epoch time: 13.99 s 
2023-03-21 13:47:02.355474:  
2023-03-21 13:47:02.355625: Epoch 438 
2023-03-21 13:47:02.355775: Current learning rate: 0.00595 
2023-03-21 13:47:15.799944: train_loss -0.9163 
2023-03-21 13:47:15.800229: val_loss -0.8369 
2023-03-21 13:47:15.800336: Pseudo dice [0.8934, 0.8759] 
2023-03-21 13:47:15.800442: Epoch time: 13.45 s 
2023-03-21 13:47:16.993091:  
2023-03-21 13:47:16.993261: Epoch 439 
2023-03-21 13:47:16.993424: Current learning rate: 0.00594 
2023-03-21 13:47:30.615731: train_loss -0.9163 
2023-03-21 13:47:30.616033: val_loss -0.8372 
2023-03-21 13:47:30.616162: Pseudo dice [0.8917, 0.8757] 
2023-03-21 13:47:30.616286: Epoch time: 13.62 s 
2023-03-21 13:47:31.841313:  
2023-03-21 13:47:31.841466: Epoch 440 
2023-03-21 13:47:31.841614: Current learning rate: 0.00593 
2023-03-21 13:47:45.297739: train_loss -0.9163 
2023-03-21 13:47:45.297957: val_loss -0.8373 
2023-03-21 13:47:45.298059: Pseudo dice [0.8926, 0.8768] 
2023-03-21 13:47:45.298157: Epoch time: 13.46 s 
2023-03-21 13:47:46.463087:  
2023-03-21 13:47:46.463247: Epoch 441 
2023-03-21 13:47:46.463414: Current learning rate: 0.00592 
2023-03-21 13:47:59.842268: train_loss -0.9152 
2023-03-21 13:47:59.842480: val_loss -0.8402 
2023-03-21 13:47:59.842583: Pseudo dice [0.8936, 0.8769] 
2023-03-21 13:47:59.842680: Epoch time: 13.38 s 
2023-03-21 13:48:01.015504:  
2023-03-21 13:48:01.015650: Epoch 442 
2023-03-21 13:48:01.015797: Current learning rate: 0.00592 
2023-03-21 13:48:14.496241: train_loss -0.9155 
2023-03-21 13:48:14.496501: val_loss -0.8397 
2023-03-21 13:48:14.496603: Pseudo dice [0.8928, 0.8768] 
2023-03-21 13:48:14.496701: Epoch time: 13.48 s 
2023-03-21 13:48:15.629624:  
2023-03-21 13:48:15.629788: Epoch 443 
2023-03-21 13:48:15.629949: Current learning rate: 0.00591 
2023-03-21 13:48:29.462170: train_loss -0.9145 
2023-03-21 13:48:29.462668: val_loss -0.8384 
2023-03-21 13:48:29.463104: Pseudo dice [0.893, 0.8764] 
2023-03-21 13:48:29.463412: Epoch time: 13.83 s 
2023-03-21 13:48:30.624300:  
2023-03-21 13:48:30.624447: Epoch 444 
2023-03-21 13:48:30.624590: Current learning rate: 0.0059 
2023-03-21 13:48:44.084606: train_loss -0.917 
2023-03-21 13:48:44.084900: val_loss -0.8403 
2023-03-21 13:48:44.085005: Pseudo dice [0.8936, 0.8794] 
2023-03-21 13:48:44.085105: Epoch time: 13.46 s 
2023-03-21 13:48:45.406445:  
2023-03-21 13:48:45.406594: Epoch 445 
2023-03-21 13:48:45.406729: Current learning rate: 0.00589 
2023-03-21 13:48:58.940540: train_loss -0.9171 
2023-03-21 13:48:58.940866: val_loss -0.8357 
2023-03-21 13:48:58.940997: Pseudo dice [0.8909, 0.8749] 
2023-03-21 13:48:58.941124: Epoch time: 13.53 s 
2023-03-21 13:49:00.096019:  
2023-03-21 13:49:00.096194: Epoch 446 
2023-03-21 13:49:00.096357: Current learning rate: 0.00588 
2023-03-21 13:49:13.755004: train_loss -0.9175 
2023-03-21 13:49:13.755294: val_loss -0.8414 
2023-03-21 13:49:13.755408: Pseudo dice [0.8937, 0.88] 
2023-03-21 13:49:13.755526: Epoch time: 13.66 s 
2023-03-21 13:49:14.914001:  
2023-03-21 13:49:14.914144: Epoch 447 
2023-03-21 13:49:14.914289: Current learning rate: 0.00587 
2023-03-21 13:49:28.454182: train_loss -0.9191 
2023-03-21 13:49:28.454527: val_loss -0.8414 
2023-03-21 13:49:28.454808: Pseudo dice [0.8958, 0.8796] 
2023-03-21 13:49:28.455197: Epoch time: 13.54 s 
2023-03-21 13:49:29.614616:  
2023-03-21 13:49:29.614810: Epoch 448 
2023-03-21 13:49:29.614964: Current learning rate: 0.00586 
2023-03-21 13:49:43.254982: train_loss -0.9181 
2023-03-21 13:49:43.255166: val_loss -0.8413 
2023-03-21 13:49:43.255268: Pseudo dice [0.895, 0.8781] 
2023-03-21 13:49:43.255364: Epoch time: 13.64 s 
2023-03-21 13:49:44.423665:  
2023-03-21 13:49:44.423955: Epoch 449 
2023-03-21 13:49:44.424122: Current learning rate: 0.00585 
2023-03-21 13:49:58.248394: train_loss -0.9158 
2023-03-21 13:49:58.248710: val_loss -0.8397 
2023-03-21 13:49:58.248855: Pseudo dice [0.8955, 0.8774] 
2023-03-21 13:49:58.248982: Epoch time: 13.83 s 
2023-03-21 13:49:59.829080:  
2023-03-21 13:49:59.829235: Epoch 450 
2023-03-21 13:49:59.829379: Current learning rate: 0.00584 
2023-03-21 13:50:13.514840: train_loss -0.9171 
2023-03-21 13:50:13.515223: val_loss -0.8432 
2023-03-21 13:50:13.515430: Pseudo dice [0.8972, 0.8791] 
2023-03-21 13:50:13.515608: Epoch time: 13.69 s 
2023-03-21 13:50:14.678369:  
2023-03-21 13:50:14.678503: Epoch 451 
2023-03-21 13:50:14.678662: Current learning rate: 0.00583 
2023-03-21 13:50:27.944597: train_loss -0.9167 
2023-03-21 13:50:27.944921: val_loss -0.8418 
2023-03-21 13:50:27.945057: Pseudo dice [0.8939, 0.8793] 
2023-03-21 13:50:27.945185: Epoch time: 13.27 s 
2023-03-21 13:50:29.129774:  
2023-03-21 13:50:29.129949: Epoch 452 
2023-03-21 13:50:29.130115: Current learning rate: 0.00582 
2023-03-21 13:50:42.736618: train_loss -0.9186 
2023-03-21 13:50:42.736876: val_loss -0.8403 
2023-03-21 13:50:42.736975: Pseudo dice [0.8949, 0.8757] 
2023-03-21 13:50:42.737075: Epoch time: 13.61 s 
2023-03-21 13:50:43.899833:  
2023-03-21 13:50:43.899989: Epoch 453 
2023-03-21 13:50:43.900167: Current learning rate: 0.00581 
2023-03-21 13:50:57.833614: train_loss -0.9168 
2023-03-21 13:50:57.833870: val_loss -0.8448 
2023-03-21 13:50:57.833971: Pseudo dice [0.8983, 0.8813] 
2023-03-21 13:50:57.834076: Epoch time: 13.93 s 
2023-03-21 13:50:58.995489:  
2023-03-21 13:50:58.995883: Epoch 454 
2023-03-21 13:50:58.996176: Current learning rate: 0.0058 
2023-03-21 13:51:12.824708: train_loss -0.9165 
2023-03-21 13:51:12.825025: val_loss -0.8443 
2023-03-21 13:51:12.825278: Pseudo dice [0.8962, 0.8802] 
2023-03-21 13:51:12.825598: Epoch time: 13.83 s 
2023-03-21 13:51:14.158592:  
2023-03-21 13:51:14.158786: Epoch 455 
2023-03-21 13:51:14.158944: Current learning rate: 0.00579 
2023-03-21 13:51:27.906326: train_loss -0.9166 
2023-03-21 13:51:27.906600: val_loss -0.8377 
2023-03-21 13:51:27.906710: Pseudo dice [0.8917, 0.878] 
2023-03-21 13:51:27.906863: Epoch time: 13.75 s 
2023-03-21 13:51:29.059332:  
2023-03-21 13:51:29.059474: Epoch 456 
2023-03-21 13:51:29.059606: Current learning rate: 0.00578 
2023-03-21 13:51:42.534369: train_loss -0.9173 
2023-03-21 13:51:42.534621: val_loss -0.8392 
2023-03-21 13:51:42.534718: Pseudo dice [0.8947, 0.8766] 
2023-03-21 13:51:42.534861: Epoch time: 13.48 s 
2023-03-21 13:51:43.712039:  
2023-03-21 13:51:43.712187: Epoch 457 
2023-03-21 13:51:43.712332: Current learning rate: 0.00577 
2023-03-21 13:51:57.269952: train_loss -0.9173 
2023-03-21 13:51:57.270204: val_loss -0.8414 
2023-03-21 13:51:57.270344: Pseudo dice [0.8952, 0.8796] 
2023-03-21 13:51:57.270480: Epoch time: 13.56 s 
2023-03-21 13:51:58.430325:  
2023-03-21 13:51:58.430474: Epoch 458 
2023-03-21 13:51:58.430622: Current learning rate: 0.00576 
2023-03-21 13:52:12.293269: train_loss -0.9174 
2023-03-21 13:52:12.293848: val_loss -0.842 
2023-03-21 13:52:12.294333: Pseudo dice [0.8959, 0.8802] 
2023-03-21 13:52:12.294571: Epoch time: 13.86 s 
2023-03-21 13:52:13.459545:  
2023-03-21 13:52:13.459682: Epoch 459 
2023-03-21 13:52:13.459815: Current learning rate: 0.00575 
2023-03-21 13:52:26.999692: train_loss -0.9188 
2023-03-21 13:52:26.999905: val_loss -0.8396 
2023-03-21 13:52:27.000004: Pseudo dice [0.8944, 0.8784] 
2023-03-21 13:52:27.000100: Epoch time: 13.54 s 
2023-03-21 13:52:28.291958:  
2023-03-21 13:52:28.292120: Epoch 460 
2023-03-21 13:52:28.292265: Current learning rate: 0.00574 
2023-03-21 13:52:41.856444: train_loss -0.9191 
2023-03-21 13:52:41.856699: val_loss -0.8424 
2023-03-21 13:52:41.856808: Pseudo dice [0.8955, 0.879] 
2023-03-21 13:52:41.856915: Epoch time: 13.57 s 
2023-03-21 13:52:43.022801:  
2023-03-21 13:52:43.022967: Epoch 461 
2023-03-21 13:52:43.023133: Current learning rate: 0.00573 
2023-03-21 13:52:56.717475: train_loss -0.9182 
2023-03-21 13:52:56.717769: val_loss -0.8431 
2023-03-21 13:52:56.717871: Pseudo dice [0.8958, 0.8784] 
2023-03-21 13:52:56.717984: Epoch time: 13.7 s 
2023-03-21 13:52:57.956514:  
2023-03-21 13:52:57.956668: Epoch 462 
2023-03-21 13:52:57.956807: Current learning rate: 0.00572 
2023-03-21 13:53:11.715380: train_loss -0.9186 
2023-03-21 13:53:11.715680: val_loss -0.8417 
2023-03-21 13:53:11.715787: Pseudo dice [0.8974, 0.8788] 
2023-03-21 13:53:11.715895: Epoch time: 13.76 s 
2023-03-21 13:53:12.854860:  
2023-03-21 13:53:12.855028: Epoch 463 
2023-03-21 13:53:12.855206: Current learning rate: 0.00571 
2023-03-21 13:53:26.755373: train_loss -0.9184 
2023-03-21 13:53:26.755664: val_loss -0.8427 
2023-03-21 13:53:26.755766: Pseudo dice [0.8965, 0.8803] 
2023-03-21 13:53:26.755868: Epoch time: 13.9 s 
2023-03-21 13:53:27.916198:  
2023-03-21 13:53:27.916343: Epoch 464 
2023-03-21 13:53:27.916509: Current learning rate: 0.0057 
2023-03-21 13:53:41.623422: train_loss -0.9168 
2023-03-21 13:53:41.623939: val_loss -0.8403 
2023-03-21 13:53:41.624250: Pseudo dice [0.8946, 0.8779] 
2023-03-21 13:53:41.624357: Epoch time: 13.71 s 
2023-03-21 13:53:42.896312:  
2023-03-21 13:53:42.896455: Epoch 465 
2023-03-21 13:53:42.896583: Current learning rate: 0.0057 
2023-03-21 13:53:56.644823: train_loss -0.9177 
2023-03-21 13:53:56.645102: val_loss -0.8444 
2023-03-21 13:53:56.645210: Pseudo dice [0.8969, 0.8793] 
2023-03-21 13:53:56.645316: Epoch time: 13.75 s 
2023-03-21 13:53:58.118961:  
2023-03-21 13:53:58.119108: Epoch 466 
2023-03-21 13:53:58.119255: Current learning rate: 0.00569 
2023-03-21 13:54:11.961803: train_loss -0.9167 
2023-03-21 13:54:11.962075: val_loss -0.844 
2023-03-21 13:54:11.962182: Pseudo dice [0.8974, 0.8804] 
2023-03-21 13:54:11.962291: Epoch time: 13.84 s 
2023-03-21 13:54:13.140859:  
2023-03-21 13:54:13.141012: Epoch 467 
2023-03-21 13:54:13.141162: Current learning rate: 0.00568 
2023-03-21 13:54:26.766350: train_loss -0.9173 
2023-03-21 13:54:26.766568: val_loss -0.8366 
2023-03-21 13:54:26.766678: Pseudo dice [0.8906, 0.876] 
2023-03-21 13:54:26.766846: Epoch time: 13.63 s 
2023-03-21 13:54:27.944282:  
2023-03-21 13:54:27.944447: Epoch 468 
2023-03-21 13:54:27.944595: Current learning rate: 0.00567 
2023-03-21 13:54:41.787703: train_loss -0.9195 
2023-03-21 13:54:41.788030: val_loss -0.8402 
2023-03-21 13:54:41.788163: Pseudo dice [0.8935, 0.8787] 
2023-03-21 13:54:41.788292: Epoch time: 13.84 s 
2023-03-21 13:54:42.981297:  
2023-03-21 13:54:42.981453: Epoch 469 
2023-03-21 13:54:42.981620: Current learning rate: 0.00566 
2023-03-21 13:54:56.684645: train_loss -0.9183 
2023-03-21 13:54:56.684865: val_loss -0.8411 
2023-03-21 13:54:56.684996: Pseudo dice [0.8954, 0.8792] 
2023-03-21 13:54:56.685118: Epoch time: 13.7 s 
2023-03-21 13:54:58.008094:  
2023-03-21 13:54:58.008242: Epoch 470 
2023-03-21 13:54:58.008374: Current learning rate: 0.00565 
2023-03-21 13:55:11.600982: train_loss -0.9199 
2023-03-21 13:55:11.601250: val_loss -0.8458 
2023-03-21 13:55:11.601350: Pseudo dice [0.8985, 0.8815] 
2023-03-21 13:55:11.601449: Epoch time: 13.59 s 
2023-03-21 13:55:12.747788:  
2023-03-21 13:55:12.747945: Epoch 471 
2023-03-21 13:55:12.748090: Current learning rate: 0.00564 
2023-03-21 13:55:26.615695: train_loss -0.918 
2023-03-21 13:55:26.616163: val_loss -0.8384 
2023-03-21 13:55:26.616307: Pseudo dice [0.8918, 0.8775] 
2023-03-21 13:55:26.616444: Epoch time: 13.87 s 
2023-03-21 13:55:27.769960:  
2023-03-21 13:55:27.770106: Epoch 472 
2023-03-21 13:55:27.770252: Current learning rate: 0.00563 
2023-03-21 13:55:41.329802: train_loss -0.919 
2023-03-21 13:55:41.330058: val_loss -0.8417 
2023-03-21 13:55:41.330160: Pseudo dice [0.8945, 0.8781] 
2023-03-21 13:55:41.330261: Epoch time: 13.56 s 
2023-03-21 13:55:42.480772:  
2023-03-21 13:55:42.480971: Epoch 473 
2023-03-21 13:55:42.481205: Current learning rate: 0.00562 
2023-03-21 13:55:56.286934: train_loss -0.9186 
2023-03-21 13:55:56.287208: val_loss -0.841 
2023-03-21 13:55:56.287406: Pseudo dice [0.8941, 0.8783] 
2023-03-21 13:55:56.287602: Epoch time: 13.81 s 
2023-03-21 13:55:57.412711:  
2023-03-21 13:55:57.412856: Epoch 474 
2023-03-21 13:55:57.412982: Current learning rate: 0.00561 
2023-03-21 13:56:11.081715: train_loss -0.9151 
2023-03-21 13:56:11.082240: val_loss -0.8405 
2023-03-21 13:56:11.083839: Pseudo dice [0.8948, 0.8783] 
2023-03-21 13:56:11.084276: Epoch time: 13.67 s 
2023-03-21 13:56:12.392632:  
2023-03-21 13:56:12.392785: Epoch 475 
2023-03-21 13:56:12.392912: Current learning rate: 0.0056 
2023-03-21 13:56:26.210108: train_loss -0.9158 
2023-03-21 13:56:26.210406: val_loss -0.844 
2023-03-21 13:56:26.210536: Pseudo dice [0.8981, 0.8804] 
2023-03-21 13:56:26.210661: Epoch time: 13.82 s 
2023-03-21 13:56:27.411879:  
2023-03-21 13:56:27.412035: Epoch 476 
2023-03-21 13:56:27.412200: Current learning rate: 0.00559 
2023-03-21 13:56:40.973635: train_loss -0.9193 
2023-03-21 13:56:40.973886: val_loss -0.8431 
2023-03-21 13:56:40.974021: Pseudo dice [0.8951, 0.8817] 
2023-03-21 13:56:40.974148: Epoch time: 13.56 s 
2023-03-21 13:56:42.155859:  
2023-03-21 13:56:42.156003: Epoch 477 
2023-03-21 13:56:42.156140: Current learning rate: 0.00558 
2023-03-21 13:56:55.914571: train_loss -0.918 
2023-03-21 13:56:55.914891: val_loss -0.8417 
2023-03-21 13:56:55.915003: Pseudo dice [0.895, 0.8801] 
2023-03-21 13:56:55.915124: Epoch time: 13.76 s 
2023-03-21 13:56:57.173101:  
2023-03-21 13:56:57.173242: Epoch 478 
2023-03-21 13:56:57.173395: Current learning rate: 0.00557 
2023-03-21 13:57:10.814400: train_loss -0.9167 
2023-03-21 13:57:10.814679: val_loss -0.8429 
2023-03-21 13:57:10.814879: Pseudo dice [0.8975, 0.8788] 
2023-03-21 13:57:10.815046: Epoch time: 13.64 s 
2023-03-21 13:57:12.026518:  
2023-03-21 13:57:12.026673: Epoch 479 
2023-03-21 13:57:12.026873: Current learning rate: 0.00556 
2023-03-21 13:57:25.250220: train_loss -0.9185 
2023-03-21 13:57:25.250494: val_loss -0.8392 
2023-03-21 13:57:25.250599: Pseudo dice [0.8953, 0.8771] 
2023-03-21 13:57:25.250706: Epoch time: 13.22 s 
2023-03-21 13:57:26.586684:  
2023-03-21 13:57:26.586883: Epoch 480 
2023-03-21 13:57:26.587037: Current learning rate: 0.00555 
2023-03-21 13:57:40.472207: train_loss -0.9184 
2023-03-21 13:57:40.472403: val_loss -0.844 
2023-03-21 13:57:40.472506: Pseudo dice [0.8954, 0.8825] 
2023-03-21 13:57:40.472604: Epoch time: 13.89 s 
2023-03-21 13:57:41.662989:  
2023-03-21 13:57:41.663152: Epoch 481 
2023-03-21 13:57:41.663288: Current learning rate: 0.00554 
2023-03-21 13:57:55.400557: train_loss -0.9194 
2023-03-21 13:57:55.400891: val_loss -0.8422 
2023-03-21 13:57:55.401022: Pseudo dice [0.8958, 0.8805] 
2023-03-21 13:57:55.401152: Epoch time: 13.74 s 
2023-03-21 13:57:56.648423:  
2023-03-21 13:57:56.648573: Epoch 482 
2023-03-21 13:57:56.648702: Current learning rate: 0.00553 
2023-03-21 13:58:10.289478: train_loss -0.9208 
2023-03-21 13:58:10.289722: val_loss -0.8399 
2023-03-21 13:58:10.289850: Pseudo dice [0.8937, 0.878] 
2023-03-21 13:58:10.289970: Epoch time: 13.64 s 
2023-03-21 13:58:11.485925:  
2023-03-21 13:58:11.486080: Epoch 483 
2023-03-21 13:58:11.486227: Current learning rate: 0.00552 
2023-03-21 13:58:25.028120: train_loss -0.9176 
2023-03-21 13:58:25.028332: val_loss -0.8381 
2023-03-21 13:58:25.028432: Pseudo dice [0.8916, 0.8766] 
2023-03-21 13:58:25.028526: Epoch time: 13.54 s 
2023-03-21 13:58:26.188966:  
2023-03-21 13:58:26.189107: Epoch 484 
2023-03-21 13:58:26.189252: Current learning rate: 0.00551 
2023-03-21 13:58:39.769300: train_loss -0.9178 
2023-03-21 13:58:39.769567: val_loss -0.8462 
2023-03-21 13:58:39.769685: Pseudo dice [0.8987, 0.8821] 
2023-03-21 13:58:39.769825: Epoch time: 13.58 s 
2023-03-21 13:58:41.125483:  
2023-03-21 13:58:41.125632: Epoch 485 
2023-03-21 13:58:41.125760: Current learning rate: 0.0055 
2023-03-21 13:58:54.798562: train_loss -0.9198 
2023-03-21 13:58:54.798858: val_loss -0.8393 
2023-03-21 13:58:54.798976: Pseudo dice [0.8944, 0.879] 
2023-03-21 13:58:54.799084: Epoch time: 13.67 s 
2023-03-21 13:58:56.000752:  
2023-03-21 13:58:56.000923: Epoch 486 
2023-03-21 13:58:56.001077: Current learning rate: 0.00549 
2023-03-21 13:59:09.864095: train_loss -0.9193 
2023-03-21 13:59:09.864334: val_loss -0.8345 
2023-03-21 13:59:09.864461: Pseudo dice [0.8918, 0.875] 
2023-03-21 13:59:09.864571: Epoch time: 13.86 s 
2023-03-21 13:59:11.063863:  
2023-03-21 13:59:11.064064: Epoch 487 
2023-03-21 13:59:11.064228: Current learning rate: 0.00548 
2023-03-21 13:59:24.657045: train_loss -0.9194 
2023-03-21 13:59:24.657280: val_loss -0.8367 
2023-03-21 13:59:24.657391: Pseudo dice [0.8927, 0.8752] 
2023-03-21 13:59:24.657495: Epoch time: 13.59 s 
2023-03-21 13:59:25.833801:  
2023-03-21 13:59:25.833952: Epoch 488 
2023-03-21 13:59:25.834100: Current learning rate: 0.00547 
2023-03-21 13:59:39.229780: train_loss -0.9174 
2023-03-21 13:59:39.231333: val_loss -0.8396 
2023-03-21 13:59:39.231436: Pseudo dice [0.8929, 0.8779] 
2023-03-21 13:59:39.231542: Epoch time: 13.4 s 
2023-03-21 13:59:40.448979:  
2023-03-21 13:59:40.449132: Epoch 489 
2023-03-21 13:59:40.449260: Current learning rate: 0.00546 
2023-03-21 13:59:54.066658: train_loss -0.9185 
2023-03-21 13:59:54.066904: val_loss -0.8389 
2023-03-21 13:59:54.067009: Pseudo dice [0.8936, 0.8778] 
2023-03-21 13:59:54.067110: Epoch time: 13.62 s 
2023-03-21 13:59:55.410178:  
2023-03-21 13:59:55.410336: Epoch 490 
2023-03-21 13:59:55.410475: Current learning rate: 0.00546 
2023-03-21 14:00:08.939087: train_loss -0.9186 
2023-03-21 14:00:08.940688: val_loss -0.842 
2023-03-21 14:00:08.940803: Pseudo dice [0.8949, 0.8814] 
2023-03-21 14:00:08.940906: Epoch time: 13.53 s 
2023-03-21 14:00:10.125103:  
2023-03-21 14:00:10.125254: Epoch 491 
2023-03-21 14:00:10.125398: Current learning rate: 0.00545 
2023-03-21 14:00:23.836855: train_loss -0.919 
2023-03-21 14:00:23.837108: val_loss -0.8381 
2023-03-21 14:00:23.837227: Pseudo dice [0.894, 0.8772] 
2023-03-21 14:00:23.837330: Epoch time: 13.71 s 
2023-03-21 14:00:25.034622:  
2023-03-21 14:00:25.034809: Epoch 492 
2023-03-21 14:00:25.035005: Current learning rate: 0.00544 
2023-03-21 14:00:38.725785: train_loss -0.919 
2023-03-21 14:00:38.725982: val_loss -0.8449 
2023-03-21 14:00:38.726081: Pseudo dice [0.8992, 0.882] 
2023-03-21 14:00:38.726176: Epoch time: 13.69 s 
2023-03-21 14:00:39.894768:  
2023-03-21 14:00:39.894944: Epoch 493 
2023-03-21 14:00:39.895107: Current learning rate: 0.00543 
2023-03-21 14:00:53.467214: train_loss -0.9179 
2023-03-21 14:00:53.467524: val_loss -0.8401 
2023-03-21 14:00:53.467628: Pseudo dice [0.8954, 0.8789] 
2023-03-21 14:00:53.467733: Epoch time: 13.57 s 
2023-03-21 14:00:54.656934:  
2023-03-21 14:00:54.657083: Epoch 494 
2023-03-21 14:00:54.657227: Current learning rate: 0.00542 
2023-03-21 14:01:08.368354: train_loss -0.9179 
2023-03-21 14:01:08.368556: val_loss -0.8412 
2023-03-21 14:01:08.368659: Pseudo dice [0.8958, 0.878] 
2023-03-21 14:01:08.368757: Epoch time: 13.71 s 
2023-03-21 14:01:09.677263:  
2023-03-21 14:01:09.677431: Epoch 495 
2023-03-21 14:01:09.677578: Current learning rate: 0.00541 
2023-03-21 14:01:23.416746: train_loss -0.9189 
2023-03-21 14:01:23.417020: val_loss -0.8438 
2023-03-21 14:01:23.417123: Pseudo dice [0.8978, 0.8807] 
2023-03-21 14:01:23.417226: Epoch time: 13.74 s 
2023-03-21 14:01:24.586966:  
2023-03-21 14:01:24.587144: Epoch 496 
2023-03-21 14:01:24.587287: Current learning rate: 0.0054 
2023-03-21 14:01:38.190228: train_loss -0.9169 
2023-03-21 14:01:38.190429: val_loss -0.8413 
2023-03-21 14:01:38.190542: Pseudo dice [0.8937, 0.8799] 
2023-03-21 14:01:38.190639: Epoch time: 13.6 s 
2023-03-21 14:01:39.343637:  
2023-03-21 14:01:39.343783: Epoch 497 
2023-03-21 14:01:39.343909: Current learning rate: 0.00539 
