
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [3, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2023-03-21 14:26:25.294938: unpacking dataset... 
2023-03-21 14:26:28.328600: unpacking done... 
2023-03-21 14:26:28.329550: do_dummy_2d_data_aug: False 
2023-03-21 14:26:28.331450: Using splits from existing split file: /data_hdd/users/zengzhilin/nnUNet_preprocessed/Dataset004_Hippocampus/splits_final.json 
2023-03-21 14:26:28.331791: The split file contains 5 splits. 
2023-03-21 14:26:28.331852: Desired fold for training: 0 
2023-03-21 14:26:28.331904: This split has 208 training and 52 validation cases. 
2023-03-21 14:26:28.394311: Unable to plot network architecture: 
2023-03-21 14:26:28.394465: No module named 'hiddenlayer' 
2023-03-21 14:26:28.445042:  
2023-03-21 14:26:28.445209: Epoch 450 
2023-03-21 14:26:28.445472: Current learning rate: 0.00584 
2023-03-21 14:26:46.872069: train_loss -0.9176 
2023-03-21 14:26:46.872357: val_loss -0.8412 
2023-03-21 14:26:46.872474: Pseudo dice [0.897, 0.879] 
2023-03-21 14:26:46.872591: Epoch time: 18.43 s 
2023-03-21 14:26:48.144855:  
2023-03-21 14:26:48.145030: Epoch 451 
2023-03-21 14:26:48.145183: Current learning rate: 0.00583 
2023-03-21 14:27:01.722420: train_loss -0.9183 
2023-03-21 14:27:01.722688: val_loss -0.8404 
2023-03-21 14:27:01.722863: Pseudo dice [0.8945, 0.8766] 
2023-03-21 14:27:01.722977: Epoch time: 13.58 s 
2023-03-21 14:27:02.942024:  
2023-03-21 14:27:02.942250: Epoch 452 
2023-03-21 14:27:02.942383: Current learning rate: 0.00582 
2023-03-21 14:27:16.322352: train_loss -0.9171 
2023-03-21 14:27:16.322776: val_loss -0.8363 
2023-03-21 14:27:16.322886: Pseudo dice [0.8932, 0.8745] 
2023-03-21 14:27:16.323126: Epoch time: 13.38 s 
2023-03-21 14:27:17.628392:  
2023-03-21 14:27:17.628513: Epoch 453 
2023-03-21 14:27:17.628651: Current learning rate: 0.00581 
2023-03-21 14:27:31.382036: train_loss -0.9154 
2023-03-21 14:27:31.382523: val_loss -0.838 
2023-03-21 14:27:31.382773: Pseudo dice [0.8932, 0.8753] 
2023-03-21 14:27:31.383112: Epoch time: 13.75 s 
2023-03-21 14:27:32.588022:  
2023-03-21 14:27:32.588150: Epoch 454 
2023-03-21 14:27:32.588300: Current learning rate: 0.0058 
2023-03-21 14:27:46.884342: train_loss -0.9181 
2023-03-21 14:27:46.884578: val_loss -0.8404 
2023-03-21 14:27:46.885163: Pseudo dice [0.8944, 0.8793] 
2023-03-21 14:27:46.885425: Epoch time: 14.3 s 
2023-03-21 14:27:48.152056:  
2023-03-21 14:27:48.152442: Epoch 455 
2023-03-21 14:27:48.152793: Current learning rate: 0.00579 
2023-03-21 14:28:01.546578: train_loss -0.9173 
2023-03-21 14:28:01.546847: val_loss -0.8405 
2023-03-21 14:28:01.546958: Pseudo dice [0.8952, 0.8787] 
2023-03-21 14:28:01.547066: Epoch time: 13.4 s 
2023-03-21 14:28:02.743674:  
2023-03-21 14:28:02.743984: Epoch 456 
2023-03-21 14:28:02.744156: Current learning rate: 0.00578 
2023-03-21 14:28:16.516219: train_loss -0.9166 
2023-03-21 14:28:16.516509: val_loss -0.8394 
2023-03-21 14:28:16.516635: Pseudo dice [0.8947, 0.8751] 
2023-03-21 14:28:16.516759: Epoch time: 13.77 s 
2023-03-21 14:28:17.721032:  
2023-03-21 14:28:17.721158: Epoch 457 
2023-03-21 14:28:17.721320: Current learning rate: 0.00577 
2023-03-21 14:28:31.490572: train_loss -0.9172 
2023-03-21 14:28:31.491017: val_loss -0.8416 
2023-03-21 14:28:31.491151: Pseudo dice [0.8958, 0.8799] 
2023-03-21 14:28:31.491281: Epoch time: 13.77 s 
2023-03-21 14:28:32.800550:  
2023-03-21 14:28:32.800707: Epoch 458 
2023-03-21 14:28:32.800869: Current learning rate: 0.00576 
2023-03-21 14:28:46.407267: train_loss -0.9182 
2023-03-21 14:28:46.407482: val_loss -0.8384 
2023-03-21 14:28:46.407597: Pseudo dice [0.8935, 0.877] 
2023-03-21 14:28:46.407692: Epoch time: 13.61 s 
2023-03-21 14:28:47.571318:  
2023-03-21 14:28:47.571464: Epoch 459 
2023-03-21 14:28:47.571641: Current learning rate: 0.00575 
2023-03-21 14:29:01.346041: train_loss -0.9176 
2023-03-21 14:29:01.346292: val_loss -0.8382 
2023-03-21 14:29:01.346410: Pseudo dice [0.8949, 0.8767] 
2023-03-21 14:29:01.346508: Epoch time: 13.78 s 
2023-03-21 14:29:02.637679:  
2023-03-21 14:29:02.637803: Epoch 460 
2023-03-21 14:29:02.637946: Current learning rate: 0.00574 
2023-03-21 14:29:16.077651: train_loss -0.9185 
2023-03-21 14:29:16.077894: val_loss -0.8401 
2023-03-21 14:29:16.077996: Pseudo dice [0.895, 0.8797] 
2023-03-21 14:29:16.078095: Epoch time: 13.44 s 
2023-03-21 14:29:17.259647:  
2023-03-21 14:29:17.259770: Epoch 461 
2023-03-21 14:29:17.259929: Current learning rate: 0.00573 
2023-03-21 14:29:30.582510: train_loss -0.9154 
2023-03-21 14:29:30.582805: val_loss -0.8376 
2023-03-21 14:29:30.582916: Pseudo dice [0.8935, 0.8769] 
2023-03-21 14:29:30.583039: Epoch time: 13.32 s 
2023-03-21 14:29:31.831834:  
2023-03-21 14:29:31.831954: Epoch 462 
2023-03-21 14:29:31.832116: Current learning rate: 0.00572 
2023-03-21 14:29:45.541667: train_loss -0.9155 
2023-03-21 14:29:45.541908: val_loss -0.8401 
2023-03-21 14:29:45.542044: Pseudo dice [0.8942, 0.8777] 
2023-03-21 14:29:45.542164: Epoch time: 13.71 s 
2023-03-21 14:29:46.871006:  
2023-03-21 14:29:46.871159: Epoch 463 
2023-03-21 14:29:46.871304: Current learning rate: 0.00571 
2023-03-21 14:30:00.669416: train_loss -0.9175 
2023-03-21 14:30:00.669650: val_loss -0.8302 
2023-03-21 14:30:00.669765: Pseudo dice [0.8885, 0.8697] 
2023-03-21 14:30:00.669863: Epoch time: 13.8 s 
2023-03-21 14:30:01.840323:  
2023-03-21 14:30:01.840446: Epoch 464 
2023-03-21 14:30:01.840591: Current learning rate: 0.0057 
2023-03-21 14:30:15.525297: train_loss -0.9159 
2023-03-21 14:30:15.525579: val_loss -0.8375 
2023-03-21 14:30:15.525691: Pseudo dice [0.893, 0.8771] 
2023-03-21 14:30:15.525799: Epoch time: 13.69 s 
2023-03-21 14:30:16.727237:  
2023-03-21 14:30:16.727375: Epoch 465 
2023-03-21 14:30:16.727569: Current learning rate: 0.0057 
2023-03-21 14:30:30.360061: train_loss -0.9184 
2023-03-21 14:30:30.360308: val_loss -0.8376 
2023-03-21 14:30:30.360434: Pseudo dice [0.8947, 0.8765] 
2023-03-21 14:30:30.360545: Epoch time: 13.63 s 
2023-03-21 14:30:31.587099:  
2023-03-21 14:30:31.587226: Epoch 466 
2023-03-21 14:30:31.587371: Current learning rate: 0.00569 
2023-03-21 14:30:45.261971: train_loss -0.9191 
2023-03-21 14:30:45.263933: val_loss -0.8402 
2023-03-21 14:30:45.264217: Pseudo dice [0.8946, 0.8798] 
2023-03-21 14:30:45.264410: Epoch time: 13.68 s 
2023-03-21 14:30:46.432799:  
2023-03-21 14:30:46.432922: Epoch 467 
2023-03-21 14:30:46.433068: Current learning rate: 0.00568 
2023-03-21 14:30:59.851845: train_loss -0.9167 
2023-03-21 14:30:59.852225: val_loss -0.846 
2023-03-21 14:30:59.852467: Pseudo dice [0.8979, 0.8811] 
2023-03-21 14:30:59.852837: Epoch time: 13.42 s 
2023-03-21 14:31:01.067126:  
2023-03-21 14:31:01.067257: Epoch 468 
2023-03-21 14:31:01.067408: Current learning rate: 0.00567 
2023-03-21 14:31:15.013963: train_loss -0.9176 
2023-03-21 14:31:15.014310: val_loss -0.846 
2023-03-21 14:31:15.014635: Pseudo dice [0.8973, 0.8826] 
2023-03-21 14:31:15.014883: Epoch time: 13.95 s 
2023-03-21 14:31:16.201555:  
2023-03-21 14:31:16.201684: Epoch 469 
2023-03-21 14:31:16.201827: Current learning rate: 0.00566 
2023-03-21 14:31:29.909444: train_loss -0.9188 
2023-03-21 14:31:29.909671: val_loss -0.8418 
2023-03-21 14:31:29.909785: Pseudo dice [0.8956, 0.8783] 
2023-03-21 14:31:29.909884: Epoch time: 13.71 s 
2023-03-21 14:31:31.096464:  
2023-03-21 14:31:31.096708: Epoch 470 
2023-03-21 14:31:31.096922: Current learning rate: 0.00565 
2023-03-21 14:31:44.798421: train_loss -0.917 
2023-03-21 14:31:44.798872: val_loss -0.8459 
2023-03-21 14:31:44.799125: Pseudo dice [0.8985, 0.8813] 
2023-03-21 14:31:44.799297: Epoch time: 13.7 s 
2023-03-21 14:31:46.020144:  
2023-03-21 14:31:46.020339: Epoch 471 
2023-03-21 14:31:46.020515: Current learning rate: 0.00564 
2023-03-21 14:31:59.847826: train_loss -0.9179 
2023-03-21 14:31:59.848040: val_loss -0.8391 
2023-03-21 14:31:59.848174: Pseudo dice [0.8934, 0.8762] 
2023-03-21 14:31:59.848273: Epoch time: 13.83 s 
2023-03-21 14:32:01.058862:  
2023-03-21 14:32:01.059011: Epoch 472 
2023-03-21 14:32:01.059175: Current learning rate: 0.00563 
2023-03-21 14:32:14.893461: train_loss -0.9169 
2023-03-21 14:32:14.893894: val_loss -0.8338 
2023-03-21 14:32:14.894100: Pseudo dice [0.8912, 0.873] 
2023-03-21 14:32:14.894298: Epoch time: 13.84 s 
2023-03-21 14:32:16.128384:  
2023-03-21 14:32:16.128541: Epoch 473 
2023-03-21 14:32:16.128702: Current learning rate: 0.00562 
2023-03-21 14:32:29.621067: train_loss -0.9163 
2023-03-21 14:32:29.621533: val_loss -0.8445 
2023-03-21 14:32:29.621974: Pseudo dice [0.897, 0.8803] 
2023-03-21 14:32:29.622283: Epoch time: 13.49 s 
2023-03-21 14:32:31.004612:  
2023-03-21 14:32:31.004919: Epoch 474 
2023-03-21 14:32:31.005184: Current learning rate: 0.00561 
2023-03-21 14:32:44.748572: train_loss -0.9174 
2023-03-21 14:32:44.749083: val_loss -0.8435 
2023-03-21 14:32:44.749281: Pseudo dice [0.8974, 0.88] 
2023-03-21 14:32:44.749515: Epoch time: 13.74 s 
2023-03-21 14:32:45.981522:  
2023-03-21 14:32:45.981645: Epoch 475 
2023-03-21 14:32:45.981802: Current learning rate: 0.0056 
2023-03-21 14:32:59.762580: train_loss -0.9154 
2023-03-21 14:32:59.762861: val_loss -0.842 
2023-03-21 14:32:59.762986: Pseudo dice [0.8951, 0.8788] 
2023-03-21 14:32:59.763094: Epoch time: 13.78 s 
2023-03-21 14:33:00.966795:  
2023-03-21 14:33:00.966941: Epoch 476 
2023-03-21 14:33:00.967087: Current learning rate: 0.00559 
2023-03-21 14:33:14.645524: train_loss -0.9169 
2023-03-21 14:33:14.645826: val_loss -0.8435 
2023-03-21 14:33:14.645978: Pseudo dice [0.8969, 0.8802] 
2023-03-21 14:33:14.646112: Epoch time: 13.68 s 
2023-03-21 14:33:15.839833:  
2023-03-21 14:33:15.839957: Epoch 477 
2023-03-21 14:33:15.840099: Current learning rate: 0.00558 
2023-03-21 14:33:29.761193: train_loss -0.9194 
2023-03-21 14:33:29.761491: val_loss -0.8377 
2023-03-21 14:33:29.761618: Pseudo dice [0.8937, 0.8765] 
2023-03-21 14:33:29.761743: Epoch time: 13.92 s 
2023-03-21 14:33:30.982949:  
2023-03-21 14:33:30.983094: Epoch 478 
2023-03-21 14:33:30.983239: Current learning rate: 0.00557 
2023-03-21 14:33:44.711925: train_loss -0.9183 
2023-03-21 14:33:44.712222: val_loss -0.8412 
2023-03-21 14:33:44.712337: Pseudo dice [0.8952, 0.8781] 
2023-03-21 14:33:44.712436: Epoch time: 13.73 s 
2023-03-21 14:33:46.066044:  
2023-03-21 14:33:46.066171: Epoch 479 
2023-03-21 14:33:46.066315: Current learning rate: 0.00556 
2023-03-21 14:33:59.853909: train_loss -0.919 
2023-03-21 14:33:59.854164: val_loss -0.8375 
2023-03-21 14:33:59.854274: Pseudo dice [0.8935, 0.8786] 
2023-03-21 14:33:59.854375: Epoch time: 13.79 s 
2023-03-21 14:34:01.035767:  
2023-03-21 14:34:01.035889: Epoch 480 
2023-03-21 14:34:01.036031: Current learning rate: 0.00555 
2023-03-21 14:34:14.700746: train_loss -0.9191 
2023-03-21 14:34:14.700936: val_loss -0.8377 
2023-03-21 14:34:14.701050: Pseudo dice [0.8924, 0.8752] 
2023-03-21 14:34:14.701145: Epoch time: 13.67 s 
2023-03-21 14:34:15.957036:  
2023-03-21 14:34:15.957175: Epoch 481 
2023-03-21 14:34:15.957320: Current learning rate: 0.00554 
2023-03-21 14:34:29.601380: train_loss -0.9161 
2023-03-21 14:34:29.601616: val_loss -0.8341 
2023-03-21 14:34:29.601729: Pseudo dice [0.8914, 0.8744] 
2023-03-21 14:34:29.601831: Epoch time: 13.64 s 
2023-03-21 14:34:30.784959:  
2023-03-21 14:34:30.785099: Epoch 482 
2023-03-21 14:34:30.785274: Current learning rate: 0.00553 
2023-03-21 14:34:44.206932: train_loss -0.9186 
2023-03-21 14:34:44.207264: val_loss -0.8371 
2023-03-21 14:34:44.207392: Pseudo dice [0.8917, 0.8743] 
2023-03-21 14:34:44.207514: Epoch time: 13.42 s 
2023-03-21 14:34:45.413145:  
2023-03-21 14:34:45.413266: Epoch 483 
2023-03-21 14:34:45.413576: Current learning rate: 0.00552 
2023-03-21 14:34:58.955057: train_loss -0.9166 
2023-03-21 14:34:58.955536: val_loss -0.833 
2023-03-21 14:34:58.955697: Pseudo dice [0.8907, 0.8737] 
2023-03-21 14:34:58.955950: Epoch time: 13.54 s 
2023-03-21 14:35:00.304067:  
2023-03-21 14:35:00.304211: Epoch 484 
2023-03-21 14:35:00.304374: Current learning rate: 0.00551 
2023-03-21 14:35:14.013896: train_loss -0.9196 
2023-03-21 14:35:14.014171: val_loss -0.8383 
2023-03-21 14:35:14.014280: Pseudo dice [0.8921, 0.8758] 
2023-03-21 14:35:14.014395: Epoch time: 13.71 s 
2023-03-21 14:35:15.221986:  
2023-03-21 14:35:15.222115: Epoch 485 
2023-03-21 14:35:15.222261: Current learning rate: 0.0055 
2023-03-21 14:35:28.562988: train_loss -0.9197 
2023-03-21 14:35:28.563280: val_loss -0.8432 
2023-03-21 14:35:28.563403: Pseudo dice [0.8991, 0.8796] 
2023-03-21 14:35:28.563509: Epoch time: 13.34 s 
2023-03-21 14:35:29.756833:  
2023-03-21 14:35:29.756967: Epoch 486 
2023-03-21 14:35:29.757113: Current learning rate: 0.00549 
2023-03-21 14:35:43.504240: train_loss -0.9202 
2023-03-21 14:35:43.504471: val_loss -0.8403 
2023-03-21 14:35:43.504583: Pseudo dice [0.8958, 0.8781] 
2023-03-21 14:35:43.504680: Epoch time: 13.75 s 
2023-03-21 14:35:44.763781:  
2023-03-21 14:35:44.763931: Epoch 487 
2023-03-21 14:35:44.764098: Current learning rate: 0.00548 
2023-03-21 14:35:58.242393: train_loss -0.9187 
2023-03-21 14:35:58.242653: val_loss -0.839 
2023-03-21 14:35:58.242813: Pseudo dice [0.8947, 0.876] 
2023-03-21 14:35:58.242949: Epoch time: 13.48 s 
2023-03-21 14:35:59.473460:  
2023-03-21 14:35:59.473604: Epoch 488 
2023-03-21 14:35:59.473773: Current learning rate: 0.00547 
2023-03-21 14:36:13.342120: train_loss -0.9206 
2023-03-21 14:36:13.342393: val_loss -0.8391 
2023-03-21 14:36:13.342515: Pseudo dice [0.896, 0.8775] 
2023-03-21 14:36:13.342648: Epoch time: 13.87 s 
2023-03-21 14:36:14.718426:  
2023-03-21 14:36:14.718557: Epoch 489 
2023-03-21 14:36:14.718706: Current learning rate: 0.00546 
2023-03-21 14:36:29.085049: train_loss -0.9186 
2023-03-21 14:36:29.085388: val_loss -0.8399 
2023-03-21 14:36:29.085536: Pseudo dice [0.8948, 0.879] 
2023-03-21 14:36:29.085680: Epoch time: 14.37 s 
2023-03-21 14:36:30.292145:  
2023-03-21 14:36:30.292270: Epoch 490 
2023-03-21 14:36:30.292427: Current learning rate: 0.00546 
2023-03-21 14:36:43.991315: train_loss -0.9164 
2023-03-21 14:36:43.991741: val_loss -0.8427 
2023-03-21 14:36:43.991925: Pseudo dice [0.8969, 0.8793] 
2023-03-21 14:36:43.992207: Epoch time: 13.7 s 
2023-03-21 14:36:45.211424:  
2023-03-21 14:36:45.211549: Epoch 491 
2023-03-21 14:36:45.211693: Current learning rate: 0.00545 
2023-03-21 14:36:58.852873: train_loss -0.9176 
2023-03-21 14:36:58.853104: val_loss -0.8432 
2023-03-21 14:36:58.853229: Pseudo dice [0.8973, 0.8787] 
2023-03-21 14:36:58.853343: Epoch time: 13.64 s 
2023-03-21 14:37:00.071813:  
2023-03-21 14:37:00.071959: Epoch 492 
2023-03-21 14:37:00.072142: Current learning rate: 0.00544 
2023-03-21 14:37:13.756217: train_loss -0.9199 
2023-03-21 14:37:13.756458: val_loss -0.8427 
2023-03-21 14:37:13.756574: Pseudo dice [0.8964, 0.8789] 
2023-03-21 14:37:13.756671: Epoch time: 13.69 s 
2023-03-21 14:37:14.944452:  
2023-03-21 14:37:14.944575: Epoch 493 
2023-03-21 14:37:14.944735: Current learning rate: 0.00543 
2023-03-21 14:37:28.494600: train_loss -0.9195 
2023-03-21 14:37:28.494841: val_loss -0.837 
2023-03-21 14:37:28.494977: Pseudo dice [0.8916, 0.8768] 
2023-03-21 14:37:28.495101: Epoch time: 13.55 s 
2023-03-21 14:37:29.890149:  
2023-03-21 14:37:29.890290: Epoch 494 
2023-03-21 14:37:29.890473: Current learning rate: 0.00542 
2023-03-21 14:37:43.078689: train_loss -0.9172 
2023-03-21 14:37:43.078962: val_loss -0.8408 
2023-03-21 14:37:43.079065: Pseudo dice [0.8965, 0.8803] 
2023-03-21 14:37:43.079164: Epoch time: 13.19 s 
2023-03-21 14:37:44.249467:  
2023-03-21 14:37:44.249596: Epoch 495 
2023-03-21 14:37:44.249733: Current learning rate: 0.00541 
2023-03-21 14:37:57.995875: train_loss -0.9189 
2023-03-21 14:37:57.996258: val_loss -0.8374 
2023-03-21 14:37:57.996437: Pseudo dice [0.8921, 0.8764] 
2023-03-21 14:37:57.996752: Epoch time: 13.75 s 
2023-03-21 14:37:59.233943:  
2023-03-21 14:37:59.234209: Epoch 496 
2023-03-21 14:37:59.234512: Current learning rate: 0.0054 
2023-03-21 14:38:13.708246: train_loss -0.9192 
2023-03-21 14:38:13.708674: val_loss -0.8374 
2023-03-21 14:38:13.708781: Pseudo dice [0.8937, 0.8749] 
2023-03-21 14:38:13.708899: Epoch time: 14.47 s 
2023-03-21 14:38:14.954033:  
2023-03-21 14:38:14.954182: Epoch 497 
2023-03-21 14:38:14.954360: Current learning rate: 0.00539 
2023-03-21 14:38:29.011653: train_loss -0.9192 
2023-03-21 14:38:29.012014: val_loss -0.8431 
2023-03-21 14:38:29.012344: Pseudo dice [0.8944, 0.8807] 
2023-03-21 14:38:29.012614: Epoch time: 14.06 s 
2023-03-21 14:38:30.349493:  
2023-03-21 14:38:30.349641: Epoch 498 
2023-03-21 14:38:30.349809: Current learning rate: 0.00538 
2023-03-21 14:38:44.134009: train_loss -0.9182 
2023-03-21 14:38:44.134265: val_loss -0.8373 
2023-03-21 14:38:44.134373: Pseudo dice [0.894, 0.8765] 
2023-03-21 14:38:44.134491: Epoch time: 13.79 s 
2023-03-21 14:38:45.492138:  
2023-03-21 14:38:45.492287: Epoch 499 
2023-03-21 14:38:45.492471: Current learning rate: 0.00537 
2023-03-21 14:38:59.213079: train_loss -0.9174 
2023-03-21 14:38:59.213327: val_loss -0.8386 
2023-03-21 14:38:59.213495: Pseudo dice [0.893, 0.8789] 
2023-03-21 14:38:59.213711: Epoch time: 13.72 s 
2023-03-21 14:39:00.700522:  
2023-03-21 14:39:00.700770: Epoch 500 
2023-03-21 14:39:00.701031: Current learning rate: 0.00536 
2023-03-21 14:39:14.734813: train_loss -0.9196 
2023-03-21 14:39:14.735302: val_loss -0.8359 
2023-03-21 14:39:14.735430: Pseudo dice [0.8933, 0.8744] 
2023-03-21 14:39:14.735539: Epoch time: 14.04 s 
2023-03-21 14:39:15.966271:  
2023-03-21 14:39:15.966441: Epoch 501 
2023-03-21 14:39:15.966617: Current learning rate: 0.00535 
2023-03-21 14:39:29.809330: train_loss -0.92 
2023-03-21 14:39:29.809726: val_loss -0.8391 
2023-03-21 14:39:29.809862: Pseudo dice [0.8932, 0.8777] 
2023-03-21 14:39:29.810000: Epoch time: 13.84 s 
2023-03-21 14:39:31.068959:  
2023-03-21 14:39:31.069105: Epoch 502 
2023-03-21 14:39:31.069284: Current learning rate: 0.00534 
2023-03-21 14:39:44.852394: train_loss -0.9196 
2023-03-21 14:39:44.852661: val_loss -0.8425 
2023-03-21 14:39:44.852780: Pseudo dice [0.8971, 0.8775] 
2023-03-21 14:39:44.852883: Epoch time: 13.78 s 
2023-03-21 14:39:46.187595:  
2023-03-21 14:39:46.187723: Epoch 503 
2023-03-21 14:39:46.187857: Current learning rate: 0.00533 
2023-03-21 14:40:00.075554: train_loss -0.921 
2023-03-21 14:40:00.075793: val_loss -0.843 
2023-03-21 14:40:00.075890: Pseudo dice [0.8967, 0.8809] 
2023-03-21 14:40:00.075985: Epoch time: 13.89 s 
2023-03-21 14:40:01.280204:  
2023-03-21 14:40:01.280343: Epoch 504 
2023-03-21 14:40:01.280488: Current learning rate: 0.00532 
2023-03-21 14:40:15.005908: train_loss -0.9179 
2023-03-21 14:40:15.006325: val_loss -0.8447 
2023-03-21 14:40:15.006684: Pseudo dice [0.899, 0.8817] 
2023-03-21 14:40:15.006844: Epoch time: 13.73 s 
2023-03-21 14:40:16.535453:  
2023-03-21 14:40:16.535601: Epoch 505 
2023-03-21 14:40:16.535754: Current learning rate: 0.00531 
2023-03-21 14:40:30.040736: train_loss -0.92 
2023-03-21 14:40:30.040984: val_loss -0.8396 
2023-03-21 14:40:30.041089: Pseudo dice [0.8955, 0.876] 
2023-03-21 14:40:30.041193: Epoch time: 13.51 s 
2023-03-21 14:40:31.285002:  
2023-03-21 14:40:31.285139: Epoch 506 
2023-03-21 14:40:31.285291: Current learning rate: 0.0053 
2023-03-21 14:40:45.384519: train_loss -0.9211 
2023-03-21 14:40:45.384799: val_loss -0.8398 
2023-03-21 14:40:45.384914: Pseudo dice [0.8945, 0.878] 
2023-03-21 14:40:45.385053: Epoch time: 14.1 s 
2023-03-21 14:40:46.604221:  
2023-03-21 14:40:46.604353: Epoch 507 
2023-03-21 14:40:46.604496: Current learning rate: 0.00529 
2023-03-21 14:41:01.235241: train_loss -0.9183 
2023-03-21 14:41:01.235513: val_loss -0.8399 
2023-03-21 14:41:01.235621: Pseudo dice [0.8947, 0.8776] 
2023-03-21 14:41:01.235741: Epoch time: 14.63 s 
2023-03-21 14:41:02.611978:  
2023-03-21 14:41:02.612140: Epoch 508 
2023-03-21 14:41:02.612299: Current learning rate: 0.00528 
2023-03-21 14:41:16.909739: train_loss -0.9171 
2023-03-21 14:41:16.910240: val_loss -0.8389 
2023-03-21 14:41:16.910706: Pseudo dice [0.8956, 0.8775] 
2023-03-21 14:41:16.911150: Epoch time: 14.3 s 
2023-03-21 14:41:18.162715:  
2023-03-21 14:41:18.162900: Epoch 509 
2023-03-21 14:41:18.163102: Current learning rate: 0.00527 
2023-03-21 14:41:32.611395: train_loss -0.9207 
2023-03-21 14:41:32.611653: val_loss -0.8406 
2023-03-21 14:41:32.611779: Pseudo dice [0.8959, 0.8793] 
2023-03-21 14:41:32.611887: Epoch time: 14.45 s 
2023-03-21 14:41:33.864551:  
2023-03-21 14:41:33.864680: Epoch 510 
2023-03-21 14:41:33.864825: Current learning rate: 0.00526 
2023-03-21 14:41:48.065640: train_loss -0.9199 
2023-03-21 14:41:48.066795: val_loss -0.8376 
2023-03-21 14:41:48.066930: Pseudo dice [0.8945, 0.8766] 
2023-03-21 14:41:48.067059: Epoch time: 14.2 s 
2023-03-21 14:41:49.358836:  
2023-03-21 14:41:49.359061: Epoch 511 
2023-03-21 14:41:49.359225: Current learning rate: 0.00525 
2023-03-21 14:42:03.169940: train_loss -0.9204 
2023-03-21 14:42:03.170281: val_loss -0.8423 
2023-03-21 14:42:03.170432: Pseudo dice [0.8968, 0.8802] 
2023-03-21 14:42:03.170608: Epoch time: 13.81 s 
2023-03-21 14:42:04.436330:  
2023-03-21 14:42:04.436471: Epoch 512 
2023-03-21 14:42:04.436634: Current learning rate: 0.00524 
2023-03-21 14:42:17.996813: train_loss -0.9206 
2023-03-21 14:42:17.996999: val_loss -0.8437 
2023-03-21 14:42:17.997097: Pseudo dice [0.8983, 0.8802] 
2023-03-21 14:42:17.997208: Epoch time: 13.56 s 
2023-03-21 14:42:19.342980:  
2023-03-21 14:42:19.343107: Epoch 513 
2023-03-21 14:42:19.343262: Current learning rate: 0.00523 
2023-03-21 14:42:32.863556: train_loss -0.9189 
2023-03-21 14:42:32.863956: val_loss -0.8384 
2023-03-21 14:42:32.864146: Pseudo dice [0.8931, 0.8774] 
2023-03-21 14:42:32.864345: Epoch time: 13.52 s 
2023-03-21 14:42:34.074389:  
2023-03-21 14:42:34.074518: Epoch 514 
2023-03-21 14:42:34.074664: Current learning rate: 0.00522 
2023-03-21 14:42:47.799309: train_loss -0.9225 
2023-03-21 14:42:47.799598: val_loss -0.837 
2023-03-21 14:42:47.799717: Pseudo dice [0.8926, 0.8761] 
2023-03-21 14:42:47.799819: Epoch time: 13.73 s 
2023-03-21 14:42:49.051504:  
2023-03-21 14:42:49.051632: Epoch 515 
2023-03-21 14:42:49.051778: Current learning rate: 0.00521 
2023-03-21 14:43:03.132098: train_loss -0.9202 
2023-03-21 14:43:03.132346: val_loss -0.8355 
2023-03-21 14:43:03.132446: Pseudo dice [0.8924, 0.8747] 
2023-03-21 14:43:03.132572: Epoch time: 14.08 s 
2023-03-21 14:43:04.337312:  
2023-03-21 14:43:04.337458: Epoch 516 
2023-03-21 14:43:04.337642: Current learning rate: 0.0052 
2023-03-21 14:43:18.405478: train_loss -0.9202 
2023-03-21 14:43:18.405730: val_loss -0.8373 
2023-03-21 14:43:18.405859: Pseudo dice [0.8925, 0.8753] 
2023-03-21 14:43:18.405968: Epoch time: 14.07 s 
2023-03-21 14:43:19.624080:  
2023-03-21 14:43:19.624202: Epoch 517 
2023-03-21 14:43:19.624343: Current learning rate: 0.00519 
2023-03-21 14:43:33.470700: train_loss -0.9209 
2023-03-21 14:43:33.471159: val_loss -0.8352 
2023-03-21 14:43:33.471399: Pseudo dice [0.8912, 0.8757] 
2023-03-21 14:43:33.471636: Epoch time: 13.85 s 
2023-03-21 14:43:34.862687:  
2023-03-21 14:43:34.862851: Epoch 518 
2023-03-21 14:43:34.862998: Current learning rate: 0.00518 
2023-03-21 14:43:49.055798: train_loss -0.9221 
2023-03-21 14:43:49.056191: val_loss -0.8389 
2023-03-21 14:43:49.056444: Pseudo dice [0.8964, 0.877] 
2023-03-21 14:43:49.056782: Epoch time: 14.19 s 
2023-03-21 14:43:50.294104:  
2023-03-21 14:43:50.294239: Epoch 519 
2023-03-21 14:43:50.294405: Current learning rate: 0.00518 
2023-03-21 14:44:04.637996: train_loss -0.9215 
2023-03-21 14:44:04.638308: val_loss -0.84 
2023-03-21 14:44:04.638441: Pseudo dice [0.8945, 0.8778] 
2023-03-21 14:44:04.638593: Epoch time: 14.34 s 
2023-03-21 14:44:05.867064:  
2023-03-21 14:44:05.867206: Epoch 520 
2023-03-21 14:44:05.867359: Current learning rate: 0.00517 
2023-03-21 14:44:19.871169: train_loss -0.9195 
2023-03-21 14:44:19.871435: val_loss -0.8373 
2023-03-21 14:44:19.871541: Pseudo dice [0.8946, 0.8748] 
2023-03-21 14:44:19.871643: Epoch time: 14.0 s 
2023-03-21 14:44:21.102627:  
2023-03-21 14:44:21.102771: Epoch 521 
2023-03-21 14:44:21.102906: Current learning rate: 0.00516 
2023-03-21 14:44:35.227985: train_loss -0.9186 
2023-03-21 14:44:35.228278: val_loss -0.8309 
2023-03-21 14:44:35.228407: Pseudo dice [0.8915, 0.8718] 
2023-03-21 14:44:35.228520: Epoch time: 14.13 s 
2023-03-21 14:44:36.507195:  
2023-03-21 14:44:36.507453: Epoch 522 
2023-03-21 14:44:36.507748: Current learning rate: 0.00515 
2023-03-21 14:44:50.873605: train_loss -0.9202 
2023-03-21 14:44:50.873850: val_loss -0.8411 
2023-03-21 14:44:50.873981: Pseudo dice [0.8957, 0.8792] 
2023-03-21 14:44:50.874095: Epoch time: 14.37 s 
2023-03-21 14:44:52.295535:  
2023-03-21 14:44:52.295688: Epoch 523 
2023-03-21 14:44:52.295857: Current learning rate: 0.00514 
2023-03-21 14:45:06.091488: train_loss -0.9214 
2023-03-21 14:45:06.091736: val_loss -0.8375 
2023-03-21 14:45:06.091876: Pseudo dice [0.8937, 0.8767] 
2023-03-21 14:45:06.091977: Epoch time: 13.8 s 
2023-03-21 14:45:07.317612:  
2023-03-21 14:45:07.317745: Epoch 524 
2023-03-21 14:45:07.317895: Current learning rate: 0.00513 
2023-03-21 14:45:21.275345: train_loss -0.9221 
2023-03-21 14:45:21.275640: val_loss -0.8386 
2023-03-21 14:45:21.275768: Pseudo dice [0.8941, 0.8783] 
2023-03-21 14:45:21.275905: Epoch time: 13.96 s 
2023-03-21 14:45:22.514406:  
2023-03-21 14:45:22.514590: Epoch 525 
2023-03-21 14:45:22.514817: Current learning rate: 0.00512 
2023-03-21 14:45:36.120270: train_loss -0.9207 
2023-03-21 14:45:36.120539: val_loss -0.839 
2023-03-21 14:45:36.120645: Pseudo dice [0.8954, 0.8765] 
2023-03-21 14:45:36.120748: Epoch time: 13.61 s 
2023-03-21 14:45:37.414756:  
2023-03-21 14:45:37.414892: Epoch 526 
2023-03-21 14:45:37.415053: Current learning rate: 0.00511 
2023-03-21 14:45:50.995720: train_loss -0.9203 
2023-03-21 14:45:50.995997: val_loss -0.841 
2023-03-21 14:45:50.996121: Pseudo dice [0.8956, 0.8799] 
2023-03-21 14:45:50.996237: Epoch time: 13.58 s 
2023-03-21 14:45:52.224301:  
2023-03-21 14:45:52.224429: Epoch 527 
2023-03-21 14:45:52.224588: Current learning rate: 0.0051 
2023-03-21 14:46:05.739196: train_loss -0.9199 
2023-03-21 14:46:05.739393: val_loss -0.8409 
2023-03-21 14:46:05.739494: Pseudo dice [0.8973, 0.878] 
2023-03-21 14:46:05.739595: Epoch time: 13.52 s 
2023-03-21 14:46:07.119988:  
2023-03-21 14:46:07.120131: Epoch 528 
2023-03-21 14:46:07.120290: Current learning rate: 0.00509 
2023-03-21 14:46:20.452526: train_loss -0.9191 
2023-03-21 14:46:20.452762: val_loss -0.8418 
2023-03-21 14:46:20.452876: Pseudo dice [0.8952, 0.8786] 
2023-03-21 14:46:20.452972: Epoch time: 13.33 s 
2023-03-21 14:46:21.668385:  
2023-03-21 14:46:21.668656: Epoch 529 
2023-03-21 14:46:21.668886: Current learning rate: 0.00508 
2023-03-21 14:46:35.606679: train_loss -0.9204 
2023-03-21 14:46:35.606982: val_loss -0.8384 
2023-03-21 14:46:35.607131: Pseudo dice [0.8944, 0.8762] 
2023-03-21 14:46:35.607253: Epoch time: 13.94 s 
2023-03-21 14:46:36.870110:  
2023-03-21 14:46:36.870257: Epoch 530 
2023-03-21 14:46:36.870423: Current learning rate: 0.00507 
2023-03-21 14:46:50.594433: train_loss -0.9213 
2023-03-21 14:46:50.594703: val_loss -0.8429 
2023-03-21 14:46:50.594833: Pseudo dice [0.8976, 0.8799] 
2023-03-21 14:46:50.594941: Epoch time: 13.73 s 
2023-03-21 14:46:51.789813:  
2023-03-21 14:46:51.789947: Epoch 531 
2023-03-21 14:46:51.790123: Current learning rate: 0.00506 
2023-03-21 14:47:05.548782: train_loss -0.9196 
2023-03-21 14:47:05.549042: val_loss -0.8396 
2023-03-21 14:47:05.549158: Pseudo dice [0.8949, 0.8775] 
2023-03-21 14:47:05.549257: Epoch time: 13.76 s 
2023-03-21 14:47:06.765322:  
2023-03-21 14:47:06.765813: Epoch 532 
2023-03-21 14:47:06.765951: Current learning rate: 0.00505 
2023-03-21 14:47:20.290274: train_loss -0.9203 
2023-03-21 14:47:20.290505: val_loss -0.841 
2023-03-21 14:47:20.290618: Pseudo dice [0.8955, 0.8793] 
2023-03-21 14:47:20.290766: Epoch time: 13.53 s 
2023-03-21 14:47:21.620749:  
2023-03-21 14:47:21.620881: Epoch 533 
2023-03-21 14:47:21.621031: Current learning rate: 0.00504 
2023-03-21 14:47:35.245646: train_loss -0.9185 
2023-03-21 14:47:35.246051: val_loss -0.8394 
2023-03-21 14:47:35.246268: Pseudo dice [0.8948, 0.8768] 
2023-03-21 14:47:35.246369: Epoch time: 13.63 s 
2023-03-21 14:47:36.487045:  
2023-03-21 14:47:36.487190: Epoch 534 
2023-03-21 14:47:36.487369: Current learning rate: 0.00503 
2023-03-21 14:47:49.949089: train_loss -0.9202 
2023-03-21 14:47:49.949270: val_loss -0.8432 
2023-03-21 14:47:49.949370: Pseudo dice [0.8966, 0.8804] 
2023-03-21 14:47:49.949465: Epoch time: 13.46 s 
2023-03-21 14:47:51.158571:  
2023-03-21 14:47:51.158717: Epoch 535 
2023-03-21 14:47:51.158898: Current learning rate: 0.00502 
2023-03-21 14:48:04.707932: train_loss -0.9207 
2023-03-21 14:48:04.708183: val_loss -0.8367 
2023-03-21 14:48:04.708285: Pseudo dice [0.8919, 0.8758] 
2023-03-21 14:48:04.708383: Epoch time: 13.55 s 
2023-03-21 14:48:05.903914:  
2023-03-21 14:48:05.904041: Epoch 536 
2023-03-21 14:48:05.904188: Current learning rate: 0.00501 
2023-03-21 14:48:19.775954: train_loss -0.9203 
2023-03-21 14:48:19.776316: val_loss -0.8369 
2023-03-21 14:48:19.776512: Pseudo dice [0.8933, 0.8761] 
2023-03-21 14:48:19.776700: Epoch time: 13.87 s 
2023-03-21 14:48:21.025846:  
2023-03-21 14:48:21.025993: Epoch 537 
2023-03-21 14:48:21.026164: Current learning rate: 0.005 
2023-03-21 14:48:34.935418: train_loss -0.9209 
2023-03-21 14:48:34.935782: val_loss -0.8414 
2023-03-21 14:48:34.936008: Pseudo dice [0.8954, 0.8787] 
2023-03-21 14:48:34.936228: Epoch time: 13.91 s 
2023-03-21 14:48:36.317617:  
2023-03-21 14:48:36.317875: Epoch 538 
2023-03-21 14:48:36.318014: Current learning rate: 0.00499 
2023-03-21 14:48:49.884915: train_loss -0.919 
2023-03-21 14:48:49.885342: val_loss -0.84 
2023-03-21 14:48:49.885450: Pseudo dice [0.8939, 0.8796] 
2023-03-21 14:48:49.885554: Epoch time: 13.57 s 
2023-03-21 14:48:51.107645:  
2023-03-21 14:48:51.107782: Epoch 539 
2023-03-21 14:48:51.107934: Current learning rate: 0.00498 
2023-03-21 14:49:05.684575: train_loss -0.9202 
2023-03-21 14:49:05.684948: val_loss -0.8379 
2023-03-21 14:49:05.685052: Pseudo dice [0.8939, 0.8753] 
2023-03-21 14:49:05.685188: Epoch time: 14.58 s 
2023-03-21 14:49:06.904780:  
2023-03-21 14:49:06.904925: Epoch 540 
2023-03-21 14:49:06.905117: Current learning rate: 0.00497 
2023-03-21 14:49:20.561752: train_loss -0.9197 
2023-03-21 14:49:20.562002: val_loss -0.8385 
2023-03-21 14:49:20.562110: Pseudo dice [0.8942, 0.877] 
2023-03-21 14:49:20.562227: Epoch time: 13.66 s 
2023-03-21 14:49:21.817466:  
2023-03-21 14:49:21.817611: Epoch 541 
2023-03-21 14:49:21.817740: Current learning rate: 0.00496 
2023-03-21 14:49:35.563468: train_loss -0.9198 
2023-03-21 14:49:35.563725: val_loss -0.8382 
2023-03-21 14:49:35.563874: Pseudo dice [0.8941, 0.8751] 
2023-03-21 14:49:35.564012: Epoch time: 13.75 s 
2023-03-21 14:49:36.801147:  
2023-03-21 14:49:36.801455: Epoch 542 
2023-03-21 14:49:36.801680: Current learning rate: 0.00495 
2023-03-21 14:49:50.396986: train_loss -0.9193 
2023-03-21 14:49:50.397455: val_loss -0.8405 
2023-03-21 14:49:50.397837: Pseudo dice [0.8961, 0.8789] 
2023-03-21 14:49:50.398233: Epoch time: 13.6 s 
2023-03-21 14:49:51.793680:  
2023-03-21 14:49:51.793823: Epoch 543 
2023-03-21 14:49:51.793971: Current learning rate: 0.00494 
2023-03-21 14:50:05.612067: train_loss -0.9212 
2023-03-21 14:50:05.612317: val_loss -0.8362 
2023-03-21 14:50:05.612422: Pseudo dice [0.8922, 0.876] 
2023-03-21 14:50:05.612896: Epoch time: 13.82 s 
2023-03-21 14:50:06.936638:  
2023-03-21 14:50:06.936789: Epoch 544 
2023-03-21 14:50:06.936922: Current learning rate: 0.00493 
2023-03-21 14:50:20.559599: train_loss -0.9213 
2023-03-21 14:50:20.559899: val_loss -0.8422 
2023-03-21 14:50:20.560203: Pseudo dice [0.8965, 0.8786] 
2023-03-21 14:50:20.560415: Epoch time: 13.62 s 
2023-03-21 14:50:21.826967:  
2023-03-21 14:50:21.827120: Epoch 545 
2023-03-21 14:50:21.827249: Current learning rate: 0.00492 
2023-03-21 14:50:35.714340: train_loss -0.9225 
2023-03-21 14:50:35.714612: val_loss -0.8427 
2023-03-21 14:50:35.714953: Pseudo dice [0.898, 0.8781] 
2023-03-21 14:50:35.715052: Epoch time: 13.89 s 
2023-03-21 14:50:36.941967:  
2023-03-21 14:50:36.942100: Epoch 546 
2023-03-21 14:50:36.942229: Current learning rate: 0.00491 
2023-03-21 14:50:50.753188: train_loss -0.9213 
2023-03-21 14:50:50.753442: val_loss -0.8365 
2023-03-21 14:50:50.753541: Pseudo dice [0.8925, 0.874] 
2023-03-21 14:50:50.753639: Epoch time: 13.81 s 
2023-03-21 14:50:51.995433:  
2023-03-21 14:50:51.995569: Epoch 547 
2023-03-21 14:50:51.995703: Current learning rate: 0.0049 
2023-03-21 14:51:06.598109: train_loss -0.9227 
2023-03-21 14:51:06.598532: val_loss -0.8432 
2023-03-21 14:51:06.599002: Pseudo dice [0.897, 0.8797] 
2023-03-21 14:51:06.599332: Epoch time: 14.6 s 
2023-03-21 14:51:08.019864:  
2023-03-21 14:51:08.020001: Epoch 548 
2023-03-21 14:51:08.020142: Current learning rate: 0.00489 
2023-03-21 14:51:22.345526: train_loss -0.9205 
2023-03-21 14:51:22.345888: val_loss -0.8408 
2023-03-21 14:51:22.346299: Pseudo dice [0.8961, 0.8788] 
2023-03-21 14:51:22.346490: Epoch time: 14.33 s 
2023-03-21 14:51:23.568049:  
2023-03-21 14:51:23.568308: Epoch 549 
2023-03-21 14:51:23.568513: Current learning rate: 0.00488 
2023-03-21 14:51:37.901635: train_loss -0.92 
2023-03-21 14:51:37.901855: val_loss -0.8392 
2023-03-21 14:51:37.901951: Pseudo dice [0.8946, 0.8789] 
2023-03-21 14:51:37.902047: Epoch time: 14.33 s 
2023-03-21 14:51:39.394407:  
2023-03-21 14:51:39.394554: Epoch 550 
2023-03-21 14:51:39.394686: Current learning rate: 0.00487 
2023-03-21 14:51:53.191019: train_loss -0.9217 
2023-03-21 14:51:53.191217: val_loss -0.8398 
2023-03-21 14:51:53.191326: Pseudo dice [0.8957, 0.8774] 
2023-03-21 14:51:53.191417: Epoch time: 13.8 s 
2023-03-21 14:51:54.411939:  
2023-03-21 14:51:54.412269: Epoch 551 
2023-03-21 14:51:54.412432: Current learning rate: 0.00486 
2023-03-21 14:52:08.274103: train_loss -0.9212 
2023-03-21 14:52:08.274353: val_loss -0.843 
2023-03-21 14:52:08.274453: Pseudo dice [0.8981, 0.8791] 
2023-03-21 14:52:08.274553: Epoch time: 13.86 s 
2023-03-21 14:52:09.466783:  
2023-03-21 14:52:09.466933: Epoch 552 
2023-03-21 14:52:09.467067: Current learning rate: 0.00485 
2023-03-21 14:52:23.473654: train_loss -0.9205 
2023-03-21 14:52:23.473938: val_loss -0.8412 
2023-03-21 14:52:23.474046: Pseudo dice [0.8955, 0.8792] 
2023-03-21 14:52:23.474154: Epoch time: 14.01 s 
2023-03-21 14:52:24.817799:  
2023-03-21 14:52:24.817956: Epoch 553 
2023-03-21 14:52:24.818112: Current learning rate: 0.00484 
2023-03-21 14:52:39.312147: train_loss -0.9228 
2023-03-21 14:52:39.312528: val_loss -0.8436 
2023-03-21 14:52:39.312770: Pseudo dice [0.8984, 0.88] 
2023-03-21 14:52:39.313056: Epoch time: 14.49 s 
2023-03-21 14:52:40.512860:  
2023-03-21 14:52:40.513000: Epoch 554 
2023-03-21 14:52:40.513130: Current learning rate: 0.00484 
2023-03-21 14:52:54.379039: train_loss -0.9214 
2023-03-21 14:52:54.379405: val_loss -0.8402 
2023-03-21 14:52:54.379572: Pseudo dice [0.895, 0.8792] 
2023-03-21 14:52:54.379790: Epoch time: 13.87 s 
2023-03-21 14:52:55.548241:  
2023-03-21 14:52:55.548375: Epoch 555 
2023-03-21 14:52:55.548503: Current learning rate: 0.00483 
2023-03-21 14:53:09.293647: train_loss -0.9212 
2023-03-21 14:53:09.293917: val_loss -0.8432 
2023-03-21 14:53:09.294025: Pseudo dice [0.898, 0.8802] 
2023-03-21 14:53:09.294129: Epoch time: 13.75 s 
2023-03-21 14:53:10.524431:  
2023-03-21 14:53:10.524624: Epoch 556 
2023-03-21 14:53:10.524791: Current learning rate: 0.00482 
2023-03-21 14:53:23.784648: train_loss -0.9208 
2023-03-21 14:53:23.785200: val_loss -0.8371 
2023-03-21 14:53:23.785423: Pseudo dice [0.8923, 0.8743] 
2023-03-21 14:53:23.785625: Epoch time: 13.26 s 
2023-03-21 14:53:24.975883:  
2023-03-21 14:53:24.976011: Epoch 557 
2023-03-21 14:53:24.976163: Current learning rate: 0.00481 
2023-03-21 14:53:38.858969: train_loss -0.9211 
2023-03-21 14:53:38.859401: val_loss -0.8435 
2023-03-21 14:53:38.859617: Pseudo dice [0.8968, 0.8816] 
2023-03-21 14:53:38.859718: Epoch time: 13.88 s 
2023-03-21 14:53:40.166351:  
2023-03-21 14:53:40.166486: Epoch 558 
2023-03-21 14:53:40.166634: Current learning rate: 0.0048 
2023-03-21 14:53:53.896026: train_loss -0.9201 
2023-03-21 14:53:53.896312: val_loss -0.8401 
2023-03-21 14:53:53.896418: Pseudo dice [0.8952, 0.8782] 
2023-03-21 14:53:53.896527: Epoch time: 13.73 s 
2023-03-21 14:53:55.083111:  
2023-03-21 14:53:55.083251: Epoch 559 
2023-03-21 14:53:55.083412: Current learning rate: 0.00479 
2023-03-21 14:54:08.852489: train_loss -0.9219 
2023-03-21 14:54:08.852753: val_loss -0.843 
2023-03-21 14:54:08.852853: Pseudo dice [0.8983, 0.8826] 
2023-03-21 14:54:08.852951: Epoch time: 13.77 s 
2023-03-21 14:54:10.055246:  
2023-03-21 14:54:10.055447: Epoch 560 
2023-03-21 14:54:10.055684: Current learning rate: 0.00478 
2023-03-21 14:54:23.931551: train_loss -0.9207 
2023-03-21 14:54:23.931827: val_loss -0.8415 
2023-03-21 14:54:23.931942: Pseudo dice [0.8948, 0.8782] 
2023-03-21 14:54:23.932039: Epoch time: 13.88 s 
2023-03-21 14:54:25.135467:  
2023-03-21 14:54:25.135603: Epoch 561 
2023-03-21 14:54:25.135751: Current learning rate: 0.00477 
2023-03-21 14:54:39.111295: train_loss -0.9213 
2023-03-21 14:54:39.111692: val_loss -0.8371 
2023-03-21 14:54:39.111909: Pseudo dice [0.894, 0.8764] 
2023-03-21 14:54:39.112097: Epoch time: 13.98 s 
2023-03-21 14:54:40.329705:  
2023-03-21 14:54:40.329843: Epoch 562 
2023-03-21 14:54:40.329980: Current learning rate: 0.00476 
2023-03-21 14:54:54.261032: train_loss -0.9209 
2023-03-21 14:54:54.261329: val_loss -0.8379 
2023-03-21 14:54:54.261461: Pseudo dice [0.8932, 0.8771] 
2023-03-21 14:54:54.261601: Epoch time: 13.93 s 
2023-03-21 14:54:55.646389:  
2023-03-21 14:54:55.646531: Epoch 563 
2023-03-21 14:54:55.646683: Current learning rate: 0.00475 
2023-03-21 14:55:09.472775: train_loss -0.9215 
2023-03-21 14:55:09.473747: val_loss -0.8408 
2023-03-21 14:55:09.473960: Pseudo dice [0.8959, 0.8779] 
2023-03-21 14:55:09.474159: Epoch time: 13.83 s 
2023-03-21 14:55:10.690527:  
2023-03-21 14:55:10.690666: Epoch 564 
2023-03-21 14:55:10.690852: Current learning rate: 0.00474 
2023-03-21 14:55:24.997241: train_loss -0.9207 
2023-03-21 14:55:24.997507: val_loss -0.8403 
2023-03-21 14:55:24.997616: Pseudo dice [0.8951, 0.8787] 
2023-03-21 14:55:24.997725: Epoch time: 14.31 s 
2023-03-21 14:55:26.246577:  
2023-03-21 14:55:26.246715: Epoch 565 
2023-03-21 14:55:26.246903: Current learning rate: 0.00473 
2023-03-21 14:55:40.192063: train_loss -0.9222 
2023-03-21 14:55:40.192307: val_loss -0.8384 
2023-03-21 14:55:40.192447: Pseudo dice [0.8942, 0.8769] 
2023-03-21 14:55:40.192548: Epoch time: 13.95 s 
2023-03-21 14:55:41.409427:  
2023-03-21 14:55:41.409552: Epoch 566 
2023-03-21 14:55:41.409729: Current learning rate: 0.00472 
2023-03-21 14:55:55.223697: train_loss -0.9223 
2023-03-21 14:55:55.223997: val_loss -0.8415 
2023-03-21 14:55:55.224130: Pseudo dice [0.8979, 0.8791] 
2023-03-21 14:55:55.224259: Epoch time: 13.81 s 
2023-03-21 14:55:56.456164:  
2023-03-21 14:55:56.456303: Epoch 567 
2023-03-21 14:55:56.456470: Current learning rate: 0.00471 
2023-03-21 14:56:10.188109: train_loss -0.9222 
2023-03-21 14:56:10.188384: val_loss -0.8398 
2023-03-21 14:56:10.188512: Pseudo dice [0.8955, 0.878] 
2023-03-21 14:56:10.188637: Epoch time: 13.73 s 
2023-03-21 14:56:11.579103:  
2023-03-21 14:56:11.579339: Epoch 568 
2023-03-21 14:56:11.579607: Current learning rate: 0.0047 
2023-03-21 14:56:25.369237: train_loss -0.9211 
2023-03-21 14:56:25.369574: val_loss -0.8395 
2023-03-21 14:56:25.369695: Pseudo dice [0.8946, 0.8771] 
2023-03-21 14:56:25.369956: Epoch time: 13.79 s 
2023-03-21 14:56:26.620709:  
2023-03-21 14:56:26.620839: Epoch 569 
2023-03-21 14:56:26.621000: Current learning rate: 0.00469 
2023-03-21 14:56:40.376122: train_loss -0.9209 
2023-03-21 14:56:40.376519: val_loss -0.8375 
2023-03-21 14:56:40.376807: Pseudo dice [0.8934, 0.8766] 
2023-03-21 14:56:40.377051: Epoch time: 13.76 s 
2023-03-21 14:56:41.603272:  
2023-03-21 14:56:41.603400: Epoch 570 
2023-03-21 14:56:41.603546: Current learning rate: 0.00468 
2023-03-21 14:56:55.365480: train_loss -0.9221 
2023-03-21 14:56:55.365995: val_loss -0.8401 
2023-03-21 14:56:55.366347: Pseudo dice [0.8947, 0.8809] 
2023-03-21 14:56:55.366750: Epoch time: 13.76 s 
2023-03-21 14:56:56.567759:  
2023-03-21 14:56:56.567904: Epoch 571 
2023-03-21 14:56:56.568065: Current learning rate: 0.00467 
2023-03-21 14:57:10.383544: train_loss -0.9205 
2023-03-21 14:57:10.383766: val_loss -0.8406 
2023-03-21 14:57:10.383879: Pseudo dice [0.894, 0.8792] 
2023-03-21 14:57:10.383990: Epoch time: 13.82 s 
2023-03-21 14:57:11.594239:  
2023-03-21 14:57:11.594363: Epoch 572 
2023-03-21 14:57:11.594506: Current learning rate: 0.00466 
2023-03-21 14:57:25.307096: train_loss -0.9227 
2023-03-21 14:57:25.307350: val_loss -0.8379 
2023-03-21 14:57:25.307479: Pseudo dice [0.8944, 0.8776] 
2023-03-21 14:57:25.307611: Epoch time: 13.71 s 
2023-03-21 14:57:26.672451:  
2023-03-21 14:57:26.672600: Epoch 573 
2023-03-21 14:57:26.672760: Current learning rate: 0.00465 
2023-03-21 14:57:40.237082: train_loss -0.9223 
2023-03-21 14:57:40.237609: val_loss -0.8434 
2023-03-21 14:57:40.237864: Pseudo dice [0.8966, 0.8821] 
2023-03-21 14:57:40.238118: Epoch time: 13.57 s 
2023-03-21 14:57:41.473692:  
2023-03-21 14:57:41.473832: Epoch 574 
2023-03-21 14:57:41.473979: Current learning rate: 0.00464 
2023-03-21 14:57:55.339521: train_loss -0.9209 
2023-03-21 14:57:55.339766: val_loss -0.8381 
2023-03-21 14:57:55.340177: Pseudo dice [0.8945, 0.8767] 
2023-03-21 14:57:55.340307: Epoch time: 13.87 s 
2023-03-21 14:57:56.742011:  
2023-03-21 14:57:56.742148: Epoch 575 
2023-03-21 14:57:56.742316: Current learning rate: 0.00463 
2023-03-21 14:58:10.432233: train_loss -0.9201 
2023-03-21 14:58:10.432836: val_loss -0.836 
2023-03-21 14:58:10.432995: Pseudo dice [0.8926, 0.8752] 
2023-03-21 14:58:10.433122: Epoch time: 13.69 s 
2023-03-21 14:58:11.674874:  
2023-03-21 14:58:11.675034: Epoch 576 
2023-03-21 14:58:11.675239: Current learning rate: 0.00462 
2023-03-21 14:58:25.330598: train_loss -0.9214 
2023-03-21 14:58:25.330904: val_loss -0.8434 
2023-03-21 14:58:25.331042: Pseudo dice [0.8975, 0.8799] 
2023-03-21 14:58:25.331159: Epoch time: 13.66 s 
2023-03-21 14:58:26.585745:  
2023-03-21 14:58:26.585906: Epoch 577 
2023-03-21 14:58:26.586096: Current learning rate: 0.00461 
2023-03-21 14:58:40.301430: train_loss -0.9212 
2023-03-21 14:58:40.301722: val_loss -0.8383 
2023-03-21 14:58:40.301847: Pseudo dice [0.8929, 0.8779] 
2023-03-21 14:58:40.301970: Epoch time: 13.72 s 
2023-03-21 14:58:41.718148:  
2023-03-21 14:58:41.718286: Epoch 578 
2023-03-21 14:58:41.718432: Current learning rate: 0.0046 
2023-03-21 14:58:55.355665: train_loss -0.9218 
2023-03-21 14:58:55.355919: val_loss -0.836 
2023-03-21 14:58:55.356024: Pseudo dice [0.8937, 0.8755] 
2023-03-21 14:58:55.356125: Epoch time: 13.64 s 
2023-03-21 14:58:56.584920:  
2023-03-21 14:58:56.585062: Epoch 579 
2023-03-21 14:58:56.585238: Current learning rate: 0.00459 
2023-03-21 14:59:10.473941: train_loss -0.9229 
2023-03-21 14:59:10.474219: val_loss -0.8404 
2023-03-21 14:59:10.474349: Pseudo dice [0.8959, 0.8772] 
2023-03-21 14:59:10.474453: Epoch time: 13.89 s 
2023-03-21 14:59:11.691249:  
2023-03-21 14:59:11.691374: Epoch 580 
2023-03-21 14:59:11.691533: Current learning rate: 0.00458 
2023-03-21 14:59:26.207120: train_loss -0.9223 
2023-03-21 14:59:26.207420: val_loss -0.8357 
2023-03-21 14:59:26.207556: Pseudo dice [0.8937, 0.8756] 
2023-03-21 14:59:26.207687: Epoch time: 14.52 s 
2023-03-21 14:59:27.495185:  
2023-03-21 14:59:27.495350: Epoch 581 
2023-03-21 14:59:27.495508: Current learning rate: 0.00457 
2023-03-21 14:59:41.674430: train_loss -0.9241 
2023-03-21 14:59:41.674714: val_loss -0.838 
2023-03-21 14:59:41.674838: Pseudo dice [0.8946, 0.8762] 
2023-03-21 14:59:41.674969: Epoch time: 14.18 s 
2023-03-21 14:59:42.936984:  
2023-03-21 14:59:42.937139: Epoch 582 
2023-03-21 14:59:42.937311: Current learning rate: 0.00456 
2023-03-21 14:59:56.334156: train_loss -0.9224 
2023-03-21 14:59:56.334399: val_loss -0.8389 
2023-03-21 14:59:56.334519: Pseudo dice [0.8947, 0.8789] 
2023-03-21 14:59:56.334625: Epoch time: 13.4 s 
2023-03-21 14:59:57.726564:  
2023-03-21 14:59:57.726700: Epoch 583 
2023-03-21 14:59:57.726903: Current learning rate: 0.00455 
2023-03-21 15:00:11.565695: train_loss -0.9219 
2023-03-21 15:00:11.566059: val_loss -0.8435 
2023-03-21 15:00:11.566195: Pseudo dice [0.8958, 0.8782] 
2023-03-21 15:00:11.566319: Epoch time: 13.84 s 
2023-03-21 15:00:12.785889:  
2023-03-21 15:00:12.786021: Epoch 584 
2023-03-21 15:00:12.786183: Current learning rate: 0.00454 
2023-03-21 15:00:26.600850: train_loss -0.9233 
2023-03-21 15:00:26.601119: val_loss -0.8395 
2023-03-21 15:00:26.601221: Pseudo dice [0.8954, 0.8792] 
2023-03-21 15:00:26.601321: Epoch time: 13.82 s 
2023-03-21 15:00:27.899186:  
2023-03-21 15:00:27.899334: Epoch 585 
2023-03-21 15:00:27.899503: Current learning rate: 0.00453 
2023-03-21 15:00:41.985839: train_loss -0.923 
2023-03-21 15:00:41.986088: val_loss -0.8385 
2023-03-21 15:00:41.986205: Pseudo dice [0.8941, 0.8776] 
2023-03-21 15:00:41.986309: Epoch time: 14.09 s 
2023-03-21 15:00:43.229386:  
2023-03-21 15:00:43.229515: Epoch 586 
2023-03-21 15:00:43.229663: Current learning rate: 0.00452 
2023-03-21 15:00:57.387969: train_loss -0.923 
2023-03-21 15:00:57.388418: val_loss -0.8374 
2023-03-21 15:00:57.388721: Pseudo dice [0.8939, 0.8776] 
2023-03-21 15:00:57.388973: Epoch time: 14.16 s 
2023-03-21 15:00:58.682764:  
2023-03-21 15:00:58.682909: Epoch 587 
2023-03-21 15:00:58.683078: Current learning rate: 0.00451 
2023-03-21 15:01:12.880303: train_loss -0.9238 
2023-03-21 15:01:12.880540: val_loss -0.8361 
2023-03-21 15:01:12.880639: Pseudo dice [0.8915, 0.8748] 
2023-03-21 15:01:12.880739: Epoch time: 14.2 s 
2023-03-21 15:01:14.262659:  
2023-03-21 15:01:14.262818: Epoch 588 
2023-03-21 15:01:14.262967: Current learning rate: 0.0045 
2023-03-21 15:01:28.061090: train_loss -0.9233 
2023-03-21 15:01:28.061426: val_loss -0.8412 
2023-03-21 15:01:28.061531: Pseudo dice [0.8955, 0.8803] 
2023-03-21 15:01:28.061631: Epoch time: 13.8 s 
2023-03-21 15:01:29.299847:  
2023-03-21 15:01:29.299973: Epoch 589 
2023-03-21 15:01:29.300117: Current learning rate: 0.00449 
2023-03-21 15:01:42.881874: train_loss -0.9223 
2023-03-21 15:01:42.882065: val_loss -0.839 
2023-03-21 15:01:42.882184: Pseudo dice [0.8948, 0.8766] 
2023-03-21 15:01:42.882283: Epoch time: 13.58 s 
2023-03-21 15:01:44.101442:  
2023-03-21 15:01:44.101584: Epoch 590 
2023-03-21 15:01:44.101746: Current learning rate: 0.00448 
2023-03-21 15:01:57.784854: train_loss -0.924 
2023-03-21 15:01:57.785220: val_loss -0.838 
2023-03-21 15:01:57.785433: Pseudo dice [0.8939, 0.8774] 
2023-03-21 15:01:57.785671: Epoch time: 13.68 s 
2023-03-21 15:01:59.044187:  
2023-03-21 15:01:59.044313: Epoch 591 
2023-03-21 15:01:59.044474: Current learning rate: 0.00447 
2023-03-21 15:02:12.660470: train_loss -0.9226 
2023-03-21 15:02:12.660728: val_loss -0.8397 
2023-03-21 15:02:12.660866: Pseudo dice [0.8951, 0.8784] 
2023-03-21 15:02:12.660974: Epoch time: 13.62 s 
2023-03-21 15:02:14.037253:  
2023-03-21 15:02:14.037415: Epoch 592 
2023-03-21 15:02:14.037601: Current learning rate: 0.00446 
2023-03-21 15:02:28.055919: train_loss -0.9218 
2023-03-21 15:02:28.056182: val_loss -0.8379 
2023-03-21 15:02:28.056279: Pseudo dice [0.8939, 0.8769] 
2023-03-21 15:02:28.056375: Epoch time: 14.02 s 
2023-03-21 15:02:29.308510:  
2023-03-21 15:02:29.308650: Epoch 593 
2023-03-21 15:02:29.308785: Current learning rate: 0.00445 
2023-03-21 15:02:43.131243: train_loss -0.9228 
2023-03-21 15:02:43.131487: val_loss -0.8393 
2023-03-21 15:02:43.131621: Pseudo dice [0.8957, 0.8768] 
2023-03-21 15:02:43.131723: Epoch time: 13.82 s 
2023-03-21 15:02:44.382409:  
2023-03-21 15:02:44.382550: Epoch 594 
2023-03-21 15:02:44.382704: Current learning rate: 0.00444 
2023-03-21 15:02:58.192497: train_loss -0.9237 
2023-03-21 15:02:58.192793: val_loss -0.8407 
2023-03-21 15:02:58.192921: Pseudo dice [0.8967, 0.8811] 
2023-03-21 15:02:58.193043: Epoch time: 13.81 s 
2023-03-21 15:02:59.419362:  
2023-03-21 15:02:59.419484: Epoch 595 
2023-03-21 15:02:59.419628: Current learning rate: 0.00443 
2023-03-21 15:03:13.172839: train_loss -0.9234 
2023-03-21 15:03:13.173085: val_loss -0.8326 
2023-03-21 15:03:13.173200: Pseudo dice [0.89, 0.876] 
2023-03-21 15:03:13.173315: Epoch time: 13.75 s 
2023-03-21 15:03:14.441413:  
2023-03-21 15:03:14.441537: Epoch 596 
2023-03-21 15:03:14.441666: Current learning rate: 0.00442 
2023-03-21 15:03:28.168591: train_loss -0.9232 
2023-03-21 15:03:28.168927: val_loss -0.838 
2023-03-21 15:03:28.169180: Pseudo dice [0.8942, 0.8785] 
2023-03-21 15:03:28.169339: Epoch time: 13.73 s 
2023-03-21 15:03:29.500517:  
2023-03-21 15:03:29.500656: Epoch 597 
2023-03-21 15:03:29.500799: Current learning rate: 0.00441 
2023-03-21 15:03:43.214513: train_loss -0.924 
2023-03-21 15:03:43.214803: val_loss -0.8364 
2023-03-21 15:03:43.214911: Pseudo dice [0.8938, 0.8776] 
2023-03-21 15:03:43.215023: Epoch time: 13.71 s 
2023-03-21 15:03:44.475994:  
2023-03-21 15:03:44.476126: Epoch 598 
2023-03-21 15:03:44.476291: Current learning rate: 0.0044 
2023-03-21 15:03:57.818196: train_loss -0.9222 
2023-03-21 15:03:57.818406: val_loss -0.8352 
2023-03-21 15:03:57.818523: Pseudo dice [0.8914, 0.8738] 
2023-03-21 15:03:57.818620: Epoch time: 13.34 s 
2023-03-21 15:03:59.052069:  
2023-03-21 15:03:59.052227: Epoch 599 
2023-03-21 15:03:59.052380: Current learning rate: 0.00439 
2023-03-21 15:04:12.988933: train_loss -0.9239 
2023-03-21 15:04:12.989338: val_loss -0.8373 
2023-03-21 15:04:12.989570: Pseudo dice [0.8946, 0.8769] 
2023-03-21 15:04:12.989817: Epoch time: 13.94 s 
2023-03-21 15:04:14.546214:  
2023-03-21 15:04:14.546341: Epoch 600 
2023-03-21 15:04:14.546487: Current learning rate: 0.00438 
2023-03-21 15:04:28.313715: train_loss -0.9229 
2023-03-21 15:04:28.314123: val_loss -0.839 
2023-03-21 15:04:28.314486: Pseudo dice [0.8949, 0.8792] 
2023-03-21 15:04:28.314667: Epoch time: 13.77 s 
2023-03-21 15:04:29.608055:  
2023-03-21 15:04:29.608178: Epoch 601 
2023-03-21 15:04:29.608320: Current learning rate: 0.00437 
2023-03-21 15:04:43.381086: train_loss -0.9243 
2023-03-21 15:04:43.381501: val_loss -0.8425 
2023-03-21 15:04:43.381728: Pseudo dice [0.8974, 0.8807] 
2023-03-21 15:04:43.381868: Epoch time: 13.77 s 
2023-03-21 15:04:44.819912:  
2023-03-21 15:04:44.820055: Epoch 602 
2023-03-21 15:04:44.820206: Current learning rate: 0.00436 
2023-03-21 15:04:59.326673: train_loss -0.9233 
2023-03-21 15:04:59.326961: val_loss -0.8388 
2023-03-21 15:04:59.327076: Pseudo dice [0.8954, 0.8775] 
2023-03-21 15:04:59.327202: Epoch time: 14.51 s 
2023-03-21 15:05:00.584349:  
2023-03-21 15:05:00.584496: Epoch 603 
2023-03-21 15:05:00.584646: Current learning rate: 0.00435 
2023-03-21 15:05:14.275019: train_loss -0.9239 
2023-03-21 15:05:14.275270: val_loss -0.8395 
2023-03-21 15:05:14.275381: Pseudo dice [0.8959, 0.8793] 
2023-03-21 15:05:14.275476: Epoch time: 13.69 s 
2023-03-21 15:05:15.556347:  
2023-03-21 15:05:15.556478: Epoch 604 
2023-03-21 15:05:15.556641: Current learning rate: 0.00434 
2023-03-21 15:05:29.326428: train_loss -0.9234 
2023-03-21 15:05:29.326665: val_loss -0.8385 
2023-03-21 15:05:29.326810: Pseudo dice [0.8946, 0.8789] 
2023-03-21 15:05:29.326918: Epoch time: 13.77 s 
2023-03-21 15:05:30.539382:  
2023-03-21 15:05:30.539505: Epoch 605 
2023-03-21 15:05:30.539646: Current learning rate: 0.00433 
2023-03-21 15:05:44.235359: train_loss -0.9246 
2023-03-21 15:05:44.235802: val_loss -0.8316 
2023-03-21 15:05:44.235974: Pseudo dice [0.8911, 0.8725] 
2023-03-21 15:05:44.236326: Epoch time: 13.7 s 
2023-03-21 15:05:45.470141:  
2023-03-21 15:05:45.470269: Epoch 606 
2023-03-21 15:05:45.470437: Current learning rate: 0.00432 
2023-03-21 15:05:59.267974: train_loss -0.9252 
2023-03-21 15:05:59.268228: val_loss -0.8382 
2023-03-21 15:05:59.268336: Pseudo dice [0.8932, 0.8789] 
2023-03-21 15:05:59.268466: Epoch time: 13.8 s 
2023-03-21 15:06:00.684955:  
2023-03-21 15:06:00.685107: Epoch 607 
2023-03-21 15:06:00.685267: Current learning rate: 0.00431 
2023-03-21 15:06:14.630942: train_loss -0.9229 
2023-03-21 15:06:14.631299: val_loss -0.839 
2023-03-21 15:06:14.631559: Pseudo dice [0.8951, 0.8783] 
2023-03-21 15:06:14.631834: Epoch time: 13.95 s 
2023-03-21 15:06:15.964550:  
2023-03-21 15:06:15.964734: Epoch 608 
2023-03-21 15:06:15.964888: Current learning rate: 0.0043 
2023-03-21 15:06:29.846441: train_loss -0.9229 
2023-03-21 15:06:29.846912: val_loss -0.8347 
2023-03-21 15:06:29.847029: Pseudo dice [0.8916, 0.8738] 
2023-03-21 15:06:29.847235: Epoch time: 13.88 s 
2023-03-21 15:06:31.119028:  
2023-03-21 15:06:31.119161: Epoch 609 
2023-03-21 15:06:31.119311: Current learning rate: 0.00429 
2023-03-21 15:06:44.980517: train_loss -0.9226 
2023-03-21 15:06:44.980711: val_loss -0.8414 
2023-03-21 15:06:44.980811: Pseudo dice [0.8952, 0.8784] 
2023-03-21 15:06:44.980906: Epoch time: 13.86 s 
2023-03-21 15:06:46.308768:  
2023-03-21 15:06:46.308918: Epoch 610 
2023-03-21 15:06:46.309068: Current learning rate: 0.00429 
2023-03-21 15:07:00.163761: train_loss -0.9232 
2023-03-21 15:07:00.164117: val_loss -0.8371 
2023-03-21 15:07:00.164318: Pseudo dice [0.8935, 0.879] 
2023-03-21 15:07:00.164692: Epoch time: 13.86 s 
2023-03-21 15:07:01.469384:  
2023-03-21 15:07:01.469508: Epoch 611 
2023-03-21 15:07:01.469668: Current learning rate: 0.00428 
2023-03-21 15:07:15.089033: train_loss -0.9229 
2023-03-21 15:07:15.089436: val_loss -0.8472 
2023-03-21 15:07:15.089703: Pseudo dice [0.9002, 0.8834] 
2023-03-21 15:07:15.089906: Epoch time: 13.62 s 
2023-03-21 15:07:16.585911:  
2023-03-21 15:07:16.586049: Epoch 612 
2023-03-21 15:07:16.586217: Current learning rate: 0.00427 
2023-03-21 15:07:30.235831: train_loss -0.9228 
2023-03-21 15:07:30.236098: val_loss -0.8384 
2023-03-21 15:07:30.236216: Pseudo dice [0.8951, 0.879] 
2023-03-21 15:07:30.236314: Epoch time: 13.65 s 
2023-03-21 15:07:31.473225:  
2023-03-21 15:07:31.473352: Epoch 613 
2023-03-21 15:07:31.473495: Current learning rate: 0.00426 
2023-03-21 15:07:45.229803: train_loss -0.9233 
2023-03-21 15:07:45.230048: val_loss -0.8339 
2023-03-21 15:07:45.230170: Pseudo dice [0.8927, 0.8736] 
2023-03-21 15:07:45.230304: Epoch time: 13.76 s 
2023-03-21 15:07:46.467485:  
2023-03-21 15:07:46.467613: Epoch 614 
2023-03-21 15:07:46.467762: Current learning rate: 0.00425 
2023-03-21 15:08:00.188639: train_loss -0.9234 
2023-03-21 15:08:00.188931: val_loss -0.8393 
2023-03-21 15:08:00.189059: Pseudo dice [0.8962, 0.8795] 
2023-03-21 15:08:00.189181: Epoch time: 13.72 s 
2023-03-21 15:08:01.452901:  
2023-03-21 15:08:01.453026: Epoch 615 
2023-03-21 15:08:01.453186: Current learning rate: 0.00424 
2023-03-21 15:08:14.960052: train_loss -0.9231 
2023-03-21 15:08:14.960296: val_loss -0.8343 
2023-03-21 15:08:14.960395: Pseudo dice [0.8921, 0.8747] 
2023-03-21 15:08:14.960491: Epoch time: 13.51 s 
2023-03-21 15:08:16.165472:  
2023-03-21 15:08:16.165594: Epoch 616 
2023-03-21 15:08:16.165754: Current learning rate: 0.00423 
2023-03-21 15:08:30.030071: train_loss -0.9234 
2023-03-21 15:08:30.030387: val_loss -0.8403 
2023-03-21 15:08:30.030522: Pseudo dice [0.8935, 0.8781] 
2023-03-21 15:08:30.030660: Epoch time: 13.87 s 
2023-03-21 15:08:31.451140:  
2023-03-21 15:08:31.451283: Epoch 617 
2023-03-21 15:08:31.451435: Current learning rate: 0.00422 
2023-03-21 15:08:45.195241: train_loss -0.9243 
2023-03-21 15:08:45.195730: val_loss -0.8372 
2023-03-21 15:08:45.195970: Pseudo dice [0.8941, 0.8783] 
2023-03-21 15:08:45.196369: Epoch time: 13.74 s 
2023-03-21 15:08:46.459765:  
2023-03-21 15:08:46.459907: Epoch 618 
2023-03-21 15:08:46.460080: Current learning rate: 0.00421 
2023-03-21 15:09:00.336287: train_loss -0.922 
2023-03-21 15:09:00.336558: val_loss -0.8421 
2023-03-21 15:09:00.336739: Pseudo dice [0.8951, 0.8794] 
2023-03-21 15:09:00.337108: Epoch time: 13.88 s 
2023-03-21 15:09:01.562407:  
2023-03-21 15:09:01.562551: Epoch 619 
2023-03-21 15:09:01.562701: Current learning rate: 0.0042 
2023-03-21 15:09:15.260412: train_loss -0.9242 
2023-03-21 15:09:15.260662: val_loss -0.8425 
2023-03-21 15:09:15.260811: Pseudo dice [0.8976, 0.881] 
2023-03-21 15:09:15.260912: Epoch time: 13.7 s 
2023-03-21 15:09:16.483036:  
2023-03-21 15:09:16.483161: Epoch 620 
2023-03-21 15:09:16.483302: Current learning rate: 0.00419 
2023-03-21 15:09:30.700464: train_loss -0.9241 
2023-03-21 15:09:30.702173: val_loss -0.8403 
2023-03-21 15:09:30.702315: Pseudo dice [0.8959, 0.8801] 
2023-03-21 15:09:30.702438: Epoch time: 14.22 s 
2023-03-21 15:09:32.016927:  
2023-03-21 15:09:32.017085: Epoch 621 
2023-03-21 15:09:32.017293: Current learning rate: 0.00418 
2023-03-21 15:09:45.987152: train_loss -0.9231 
2023-03-21 15:09:45.987397: val_loss -0.8401 
2023-03-21 15:09:45.987617: Pseudo dice [0.8959, 0.8807] 
2023-03-21 15:09:45.987717: Epoch time: 13.97 s 
2023-03-21 15:09:47.400410:  
2023-03-21 15:09:47.400558: Epoch 622 
2023-03-21 15:09:47.400735: Current learning rate: 0.00417 
2023-03-21 15:10:01.150195: train_loss -0.9254 
2023-03-21 15:10:01.150443: val_loss -0.8347 
2023-03-21 15:10:01.150558: Pseudo dice [0.8904, 0.8768] 
2023-03-21 15:10:01.150655: Epoch time: 13.75 s 
2023-03-21 15:10:02.384351:  
2023-03-21 15:10:02.384496: Epoch 623 
2023-03-21 15:10:02.384659: Current learning rate: 0.00416 
2023-03-21 15:10:16.005619: train_loss -0.9231 
2023-03-21 15:10:16.005795: val_loss -0.8434 
2023-03-21 15:10:16.005913: Pseudo dice [0.8977, 0.8807] 
2023-03-21 15:10:16.006013: Epoch time: 13.62 s 
2023-03-21 15:10:17.288435:  
2023-03-21 15:10:17.288687: Epoch 624 
2023-03-21 15:10:17.288930: Current learning rate: 0.00415 
2023-03-21 15:10:30.588661: train_loss -0.9222 
2023-03-21 15:10:30.588904: val_loss -0.8416 
2023-03-21 15:10:30.589004: Pseudo dice [0.8946, 0.8803] 
2023-03-21 15:10:30.589104: Epoch time: 13.3 s 
2023-03-21 15:10:31.841403:  
2023-03-21 15:10:31.841569: Epoch 625 
2023-03-21 15:10:31.841787: Current learning rate: 0.00414 
2023-03-21 15:10:45.542079: train_loss -0.9218 
2023-03-21 15:10:45.542379: val_loss -0.8319 
2023-03-21 15:10:45.542481: Pseudo dice [0.8896, 0.8744] 
2023-03-21 15:10:45.542593: Epoch time: 13.7 s 
2023-03-21 15:10:46.786134:  
2023-03-21 15:10:46.786256: Epoch 626 
2023-03-21 15:10:46.786399: Current learning rate: 0.00413 
2023-03-21 15:11:00.657778: train_loss -0.924 
2023-03-21 15:11:00.658088: val_loss -0.8385 
2023-03-21 15:11:00.658217: Pseudo dice [0.8946, 0.8785] 
2023-03-21 15:11:00.658341: Epoch time: 13.87 s 
2023-03-21 15:11:02.024168:  
2023-03-21 15:11:02.024318: Epoch 627 
2023-03-21 15:11:02.024462: Current learning rate: 0.00412 
2023-03-21 15:11:15.677158: train_loss -0.9223 
2023-03-21 15:11:15.677421: val_loss -0.8383 
2023-03-21 15:11:15.677539: Pseudo dice [0.8945, 0.8787] 
2023-03-21 15:11:15.677670: Epoch time: 13.65 s 
2023-03-21 15:11:16.895342:  
2023-03-21 15:11:16.895493: Epoch 628 
2023-03-21 15:11:16.895639: Current learning rate: 0.00411 
2023-03-21 15:11:30.777641: train_loss -0.9245 
2023-03-21 15:11:30.779519: val_loss -0.8403 
2023-03-21 15:11:30.779754: Pseudo dice [0.8964, 0.8787] 
2023-03-21 15:11:30.779895: Epoch time: 13.88 s 
2023-03-21 15:11:32.052927:  
2023-03-21 15:11:32.053070: Epoch 629 
2023-03-21 15:11:32.053215: Current learning rate: 0.0041 
2023-03-21 15:11:45.764980: train_loss -0.9241 
2023-03-21 15:11:45.765691: val_loss -0.8427 
2023-03-21 15:11:45.765923: Pseudo dice [0.8972, 0.8818] 
2023-03-21 15:11:45.766209: Epoch time: 13.71 s 
2023-03-21 15:11:47.020588:  
2023-03-21 15:11:47.020730: Epoch 630 
2023-03-21 15:11:47.020884: Current learning rate: 0.00409 
2023-03-21 15:12:00.928260: train_loss -0.9227 
2023-03-21 15:12:00.928531: val_loss -0.8332 
2023-03-21 15:12:00.928632: Pseudo dice [0.8909, 0.8743] 
2023-03-21 15:12:00.928730: Epoch time: 13.91 s 
2023-03-21 15:12:02.286864:  
2023-03-21 15:12:02.287158: Epoch 631 
2023-03-21 15:12:02.287315: Current learning rate: 0.00408 
2023-03-21 15:12:16.538636: train_loss -0.9236 
2023-03-21 15:12:16.539013: val_loss -0.8355 
2023-03-21 15:12:16.539219: Pseudo dice [0.893, 0.8767] 
2023-03-21 15:12:16.539410: Epoch time: 14.25 s 
2023-03-21 15:12:17.842622:  
2023-03-21 15:12:17.842768: Epoch 632 
2023-03-21 15:12:17.842939: Current learning rate: 0.00407 
2023-03-21 15:12:31.562953: train_loss -0.9239 
2023-03-21 15:12:31.563410: val_loss -0.8377 
2023-03-21 15:12:31.563674: Pseudo dice [0.8943, 0.8764] 
2023-03-21 15:12:31.563813: Epoch time: 13.72 s 
2023-03-21 15:12:32.817563:  
2023-03-21 15:12:32.817722: Epoch 633 
2023-03-21 15:12:32.817968: Current learning rate: 0.00406 
2023-03-21 15:12:46.637635: train_loss -0.9232 
2023-03-21 15:12:46.637980: val_loss -0.8399 
2023-03-21 15:12:46.638140: Pseudo dice [0.8953, 0.8795] 
2023-03-21 15:12:46.638268: Epoch time: 13.82 s 
2023-03-21 15:12:47.908805:  
2023-03-21 15:12:47.908930: Epoch 634 
2023-03-21 15:12:47.909076: Current learning rate: 0.00405 
2023-03-21 15:13:01.621225: train_loss -0.9239 
2023-03-21 15:13:01.621474: val_loss -0.8328 
2023-03-21 15:13:01.621613: Pseudo dice [0.8906, 0.8727] 
2023-03-21 15:13:01.621773: Epoch time: 13.71 s 
2023-03-21 15:13:02.876398:  
2023-03-21 15:13:02.876519: Epoch 635 
2023-03-21 15:13:02.876663: Current learning rate: 0.00404 
2023-03-21 15:13:16.827560: train_loss -0.9236 
2023-03-21 15:13:16.827747: val_loss -0.8421 
2023-03-21 15:13:16.827845: Pseudo dice [0.897, 0.8805] 
2023-03-21 15:13:16.827951: Epoch time: 13.95 s 
2023-03-21 15:13:18.146027:  
2023-03-21 15:13:18.146166: Epoch 636 
2023-03-21 15:13:18.146324: Current learning rate: 0.00403 
2023-03-21 15:13:31.678014: train_loss -0.9235 
2023-03-21 15:13:31.678436: val_loss -0.8426 
2023-03-21 15:13:31.678605: Pseudo dice [0.8965, 0.8805] 
2023-03-21 15:13:31.678985: Epoch time: 13.53 s 
2023-03-21 15:13:32.896785:  
2023-03-21 15:13:32.896913: Epoch 637 
2023-03-21 15:13:32.897047: Current learning rate: 0.00402 
2023-03-21 15:13:46.749331: train_loss -0.9241 
2023-03-21 15:13:46.749570: val_loss -0.84 
2023-03-21 15:13:46.749672: Pseudo dice [0.8959, 0.878] 
2023-03-21 15:13:46.749770: Epoch time: 13.85 s 
2023-03-21 15:13:48.027110:  
2023-03-21 15:13:48.027249: Epoch 638 
2023-03-21 15:13:48.027412: Current learning rate: 0.00401 
2023-03-21 15:14:01.475543: train_loss -0.926 
2023-03-21 15:14:01.476242: val_loss -0.8348 
2023-03-21 15:14:01.476462: Pseudo dice [0.8919, 0.877] 
2023-03-21 15:14:01.476720: Epoch time: 13.45 s 
2023-03-21 15:14:02.739852:  
2023-03-21 15:14:02.739978: Epoch 639 
2023-03-21 15:14:02.740124: Current learning rate: 0.004 
2023-03-21 15:14:16.562480: train_loss -0.9223 
2023-03-21 15:14:16.562819: val_loss -0.838 
2023-03-21 15:14:16.562963: Pseudo dice [0.895, 0.8799] 
2023-03-21 15:14:16.563125: Epoch time: 13.82 s 
2023-03-21 15:14:17.810624:  
2023-03-21 15:14:17.810786: Epoch 640 
2023-03-21 15:14:17.810946: Current learning rate: 0.00399 
2023-03-21 15:14:31.541935: train_loss -0.9225 
2023-03-21 15:14:31.542209: val_loss -0.838 
2023-03-21 15:14:31.542335: Pseudo dice [0.8952, 0.8764] 
2023-03-21 15:14:31.542459: Epoch time: 13.73 s 
2023-03-21 15:14:32.929964:  
2023-03-21 15:14:32.930125: Epoch 641 
2023-03-21 15:14:32.930309: Current learning rate: 0.00398 
2023-03-21 15:14:46.313065: train_loss -0.9238 
2023-03-21 15:14:46.313454: val_loss -0.8386 
2023-03-21 15:14:46.313576: Pseudo dice [0.8948, 0.8785] 
2023-03-21 15:14:46.313674: Epoch time: 13.38 s 
2023-03-21 15:14:47.573423:  
2023-03-21 15:14:47.573564: Epoch 642 
2023-03-21 15:14:47.573725: Current learning rate: 0.00397 
2023-03-21 15:15:01.432359: train_loss -0.9249 
2023-03-21 15:15:01.432620: val_loss -0.8434 
2023-03-21 15:15:01.432723: Pseudo dice [0.8972, 0.88] 
2023-03-21 15:15:01.432822: Epoch time: 13.86 s 
2023-03-21 15:15:02.680730:  
2023-03-21 15:15:02.680856: Epoch 643 
2023-03-21 15:15:02.681013: Current learning rate: 0.00396 
2023-03-21 15:15:16.686954: train_loss -0.9244 
2023-03-21 15:15:16.687251: val_loss -0.8364 
2023-03-21 15:15:16.687361: Pseudo dice [0.8938, 0.8767] 
2023-03-21 15:15:16.687477: Epoch time: 14.01 s 
2023-03-21 15:15:17.921310:  
2023-03-21 15:15:17.921449: Epoch 644 
2023-03-21 15:15:17.921624: Current learning rate: 0.00395 
2023-03-21 15:15:31.671797: train_loss -0.9246 
2023-03-21 15:15:31.671999: val_loss -0.8345 
2023-03-21 15:15:31.672100: Pseudo dice [0.8924, 0.8745] 
2023-03-21 15:15:31.672193: Epoch time: 13.75 s 
2023-03-21 15:15:32.876400:  
2023-03-21 15:15:32.876523: Epoch 645 
2023-03-21 15:15:32.876666: Current learning rate: 0.00394 
2023-03-21 15:15:46.400304: train_loss -0.9238 
2023-03-21 15:15:46.400664: val_loss -0.843 
2023-03-21 15:15:46.400951: Pseudo dice [0.8964, 0.8802] 
2023-03-21 15:15:46.401339: Epoch time: 13.52 s 
2023-03-21 15:15:47.792062:  
2023-03-21 15:15:47.792267: Epoch 646 
2023-03-21 15:15:47.792432: Current learning rate: 0.00393 
2023-03-21 15:16:01.954371: train_loss -0.9245 
2023-03-21 15:16:01.954963: val_loss -0.8365 
2023-03-21 15:16:01.955653: Pseudo dice [0.893, 0.8787] 
2023-03-21 15:16:01.955853: Epoch time: 14.16 s 
2023-03-21 15:16:03.238306:  
2023-03-21 15:16:03.238440: Epoch 647 
2023-03-21 15:16:03.238591: Current learning rate: 0.00392 
2023-03-21 15:16:16.938046: train_loss -0.9233 
2023-03-21 15:16:16.938697: val_loss -0.8372 
2023-03-21 15:16:16.938865: Pseudo dice [0.8948, 0.8788] 
2023-03-21 15:16:16.939122: Epoch time: 13.7 s 
2023-03-21 15:16:18.210318:  
2023-03-21 15:16:18.210461: Epoch 648 
2023-03-21 15:16:18.210608: Current learning rate: 0.00391 
2023-03-21 15:16:31.728791: train_loss -0.9244 
2023-03-21 15:16:31.729052: val_loss -0.8365 
2023-03-21 15:16:31.729148: Pseudo dice [0.893, 0.8765] 
2023-03-21 15:16:31.729243: Epoch time: 13.52 s 
2023-03-21 15:16:33.038972:  
2023-03-21 15:16:33.039097: Epoch 649 
2023-03-21 15:16:33.039239: Current learning rate: 0.0039 
2023-03-21 15:16:46.917696: train_loss -0.9235 
2023-03-21 15:16:46.918019: val_loss -0.8438 
2023-03-21 15:16:46.918242: Pseudo dice [0.8986, 0.8833] 
2023-03-21 15:16:46.918501: Epoch time: 13.88 s 
2023-03-21 15:16:48.459418:  
2023-03-21 15:16:48.459544: Epoch 650 
2023-03-21 15:16:48.459687: Current learning rate: 0.00389 
2023-03-21 15:17:02.105298: train_loss -0.9245 
2023-03-21 15:17:02.105542: val_loss -0.8404 
2023-03-21 15:17:02.105637: Pseudo dice [0.8943, 0.8791] 
2023-03-21 15:17:02.105729: Epoch time: 13.65 s 
2023-03-21 15:17:03.485245:  
2023-03-21 15:17:03.485398: Epoch 651 
2023-03-21 15:17:03.485559: Current learning rate: 0.00388 
2023-03-21 15:17:17.304360: train_loss -0.9245 
2023-03-21 15:17:17.304610: val_loss -0.8392 
2023-03-21 15:17:17.304723: Pseudo dice [0.895, 0.8782] 
2023-03-21 15:17:17.304816: Epoch time: 13.82 s 
2023-03-21 15:17:18.591589:  
2023-03-21 15:17:18.591727: Epoch 652 
2023-03-21 15:17:18.591914: Current learning rate: 0.00387 
2023-03-21 15:17:32.690219: train_loss -0.9257 
2023-03-21 15:17:32.690473: val_loss -0.8386 
2023-03-21 15:17:32.690601: Pseudo dice [0.8944, 0.8776] 
2023-03-21 15:17:32.690701: Epoch time: 14.1 s 
2023-03-21 15:17:33.930624:  
2023-03-21 15:17:33.930766: Epoch 653 
2023-03-21 15:17:33.930929: Current learning rate: 0.00386 
2023-03-21 15:17:47.635963: train_loss -0.9242 
2023-03-21 15:17:47.636274: val_loss -0.8385 
2023-03-21 15:17:47.636411: Pseudo dice [0.8952, 0.8781] 
2023-03-21 15:17:47.636556: Epoch time: 13.71 s 
2023-03-21 15:17:48.961337:  
2023-03-21 15:17:48.961484: Epoch 654 
2023-03-21 15:17:48.961643: Current learning rate: 0.00385 
2023-03-21 15:18:02.968426: train_loss -0.9247 
2023-03-21 15:18:02.968611: val_loss -0.842 
2023-03-21 15:18:02.968709: Pseudo dice [0.8954, 0.8788] 
2023-03-21 15:18:02.968808: Epoch time: 14.01 s 
2023-03-21 15:18:04.210940:  
2023-03-21 15:18:04.211077: Epoch 655 
2023-03-21 15:18:04.211221: Current learning rate: 0.00384 
2023-03-21 15:18:17.966048: train_loss -0.9242 
2023-03-21 15:18:17.966487: val_loss -0.8383 
2023-03-21 15:18:17.966886: Pseudo dice [0.8938, 0.8778] 
2023-03-21 15:18:17.967140: Epoch time: 13.76 s 
2023-03-21 15:18:19.345059:  
2023-03-21 15:18:19.345205: Epoch 656 
2023-03-21 15:18:19.345341: Current learning rate: 0.00383 
2023-03-21 15:18:33.377095: train_loss -0.9251 
2023-03-21 15:18:33.377556: val_loss -0.8393 
2023-03-21 15:18:33.377836: Pseudo dice [0.8933, 0.8801] 
2023-03-21 15:18:33.378074: Epoch time: 14.03 s 
2023-03-21 15:18:34.621145:  
2023-03-21 15:18:34.621280: Epoch 657 
2023-03-21 15:18:34.621416: Current learning rate: 0.00382 
2023-03-21 15:18:48.490824: train_loss -0.9246 
2023-03-21 15:18:48.491027: val_loss -0.8382 
2023-03-21 15:18:48.491128: Pseudo dice [0.8932, 0.8774] 
2023-03-21 15:18:48.491223: Epoch time: 13.87 s 
2023-03-21 15:18:49.822473:  
2023-03-21 15:18:49.822609: Epoch 658 
2023-03-21 15:18:49.822777: Current learning rate: 0.00381 
2023-03-21 15:19:03.560807: train_loss -0.9235 
2023-03-21 15:19:03.561094: val_loss -0.8387 
2023-03-21 15:19:03.561213: Pseudo dice [0.8942, 0.8769] 
2023-03-21 15:19:03.561345: Epoch time: 13.74 s 
2023-03-21 15:19:04.822953:  
2023-03-21 15:19:04.823314: Epoch 659 
2023-03-21 15:19:04.823521: Current learning rate: 0.0038 
2023-03-21 15:19:18.634966: train_loss -0.9247 
2023-03-21 15:19:18.635266: val_loss -0.8441 
2023-03-21 15:19:18.635393: Pseudo dice [0.8983, 0.8823] 
2023-03-21 15:19:18.635514: Epoch time: 13.81 s 
2023-03-21 15:19:19.848816:  
2023-03-21 15:19:19.848954: Epoch 660 
2023-03-21 15:19:19.849111: Current learning rate: 0.00379 
2023-03-21 15:19:33.605551: train_loss -0.9253 
2023-03-21 15:19:33.605764: val_loss -0.836 
2023-03-21 15:19:33.605889: Pseudo dice [0.8927, 0.8753] 
2023-03-21 15:19:33.605987: Epoch time: 13.76 s 
2023-03-21 15:19:34.826828:  
2023-03-21 15:19:34.827101: Epoch 661 
2023-03-21 15:19:34.827319: Current learning rate: 0.00378 
2023-03-21 15:19:48.382540: train_loss -0.9259 
2023-03-21 15:19:48.382852: val_loss -0.8405 
2023-03-21 15:19:48.382959: Pseudo dice [0.8956, 0.8795] 
2023-03-21 15:19:48.383074: Epoch time: 13.56 s 
2023-03-21 15:19:49.604768:  
2023-03-21 15:19:49.604895: Epoch 662 
2023-03-21 15:19:49.605037: Current learning rate: 0.00377 
2023-03-21 15:20:03.264154: train_loss -0.926 
2023-03-21 15:20:03.264413: val_loss -0.835 
2023-03-21 15:20:03.264516: Pseudo dice [0.8947, 0.8763] 
2023-03-21 15:20:03.264620: Epoch time: 13.66 s 
2023-03-21 15:20:04.506309:  
2023-03-21 15:20:04.506436: Epoch 663 
2023-03-21 15:20:04.506578: Current learning rate: 0.00376 
2023-03-21 15:20:18.302196: train_loss -0.9251 
2023-03-21 15:20:18.302459: val_loss -0.8395 
2023-03-21 15:20:18.302563: Pseudo dice [0.8946, 0.8797] 
2023-03-21 15:20:18.302685: Epoch time: 13.8 s 
2023-03-21 15:20:19.588286:  
2023-03-21 15:20:19.588413: Epoch 664 
2023-03-21 15:20:19.588558: Current learning rate: 0.00375 
2023-03-21 15:20:33.266274: train_loss -0.9253 
2023-03-21 15:20:33.266592: val_loss -0.8411 
2023-03-21 15:20:33.266811: Pseudo dice [0.8959, 0.8812] 
2023-03-21 15:20:33.267075: Epoch time: 13.68 s 
2023-03-21 15:20:34.607862:  
2023-03-21 15:20:34.608003: Epoch 665 
2023-03-21 15:20:34.608149: Current learning rate: 0.00374 
2023-03-21 15:20:48.308036: train_loss -0.9263 
2023-03-21 15:20:48.308230: val_loss -0.834 
2023-03-21 15:20:48.308348: Pseudo dice [0.892, 0.875] 
2023-03-21 15:20:48.308440: Epoch time: 13.7 s 
2023-03-21 15:20:49.518397:  
2023-03-21 15:20:49.518524: Epoch 666 
2023-03-21 15:20:49.518688: Current learning rate: 0.00373 
2023-03-21 15:21:03.488487: train_loss -0.9255 
2023-03-21 15:21:03.488929: val_loss -0.8423 
2023-03-21 15:21:03.489456: Pseudo dice [0.8971, 0.8812] 
2023-03-21 15:21:03.489814: Epoch time: 13.97 s 
2023-03-21 15:21:04.786133:  
2023-03-21 15:21:04.786263: Epoch 667 
2023-03-21 15:21:04.786424: Current learning rate: 0.00372 
2023-03-21 15:21:18.591051: train_loss -0.9254 
2023-03-21 15:21:18.591287: val_loss -0.8394 
2023-03-21 15:21:18.591467: Pseudo dice [0.8956, 0.8789] 
2023-03-21 15:21:18.591656: Epoch time: 13.81 s 
2023-03-21 15:21:19.900276:  
2023-03-21 15:21:19.900414: Epoch 668 
2023-03-21 15:21:19.900568: Current learning rate: 0.00371 
2023-03-21 15:21:33.647192: train_loss -0.9257 
2023-03-21 15:21:33.648753: val_loss -0.8399 
2023-03-21 15:21:33.648889: Pseudo dice [0.8955, 0.8779] 
2023-03-21 15:21:33.649016: Epoch time: 13.75 s 
2023-03-21 15:21:34.909025:  
2023-03-21 15:21:34.909153: Epoch 669 
2023-03-21 15:21:34.909305: Current learning rate: 0.0037 
2023-03-21 15:21:48.749734: train_loss -0.9239 
2023-03-21 15:21:48.749969: val_loss -0.8433 
2023-03-21 15:21:48.750089: Pseudo dice [0.8976, 0.8807] 
2023-03-21 15:21:48.750201: Epoch time: 13.84 s 
2023-03-21 15:21:50.150892:  
2023-03-21 15:21:50.151039: Epoch 670 
2023-03-21 15:21:50.151209: Current learning rate: 0.00369 
2023-03-21 15:22:04.645942: train_loss -0.9264 
2023-03-21 15:22:04.646162: val_loss -0.844 
2023-03-21 15:22:04.646305: Pseudo dice [0.8976, 0.8811] 
2023-03-21 15:22:04.646413: Epoch time: 14.5 s 
2023-03-21 15:22:05.865883:  
2023-03-21 15:22:05.866103: Epoch 671 
2023-03-21 15:22:05.866341: Current learning rate: 0.00368 
2023-03-21 15:22:19.662288: train_loss -0.9271 
2023-03-21 15:22:19.662648: val_loss -0.834 
2023-03-21 15:22:19.663127: Pseudo dice [0.8906, 0.8768] 
2023-03-21 15:22:19.663295: Epoch time: 13.8 s 
2023-03-21 15:22:20.972986:  
2023-03-21 15:22:20.973117: Epoch 672 
2023-03-21 15:22:20.973258: Current learning rate: 0.00367 
2023-03-21 15:22:34.992658: train_loss -0.9271 
2023-03-21 15:22:34.992941: val_loss -0.84 
2023-03-21 15:22:34.993066: Pseudo dice [0.8953, 0.8807] 
2023-03-21 15:22:34.993175: Epoch time: 14.02 s 
2023-03-21 15:22:36.270311:  
2023-03-21 15:22:36.270442: Epoch 673 
2023-03-21 15:22:36.270594: Current learning rate: 0.00366 
2023-03-21 15:22:50.231347: train_loss -0.9271 
2023-03-21 15:22:50.231616: val_loss -0.8429 
2023-03-21 15:22:50.231731: Pseudo dice [0.8976, 0.8808] 
2023-03-21 15:22:50.231825: Epoch time: 13.96 s 
2023-03-21 15:22:51.506851:  
2023-03-21 15:22:51.506988: Epoch 674 
2023-03-21 15:22:51.507134: Current learning rate: 0.00365 
2023-03-21 15:23:04.815835: train_loss -0.9257 
2023-03-21 15:23:04.816090: val_loss -0.8351 
2023-03-21 15:23:04.816207: Pseudo dice [0.8934, 0.8763] 
2023-03-21 15:23:04.816306: Epoch time: 13.31 s 
2023-03-21 15:23:06.182000:  
2023-03-21 15:23:06.182143: Epoch 675 
2023-03-21 15:23:06.182292: Current learning rate: 0.00364 
2023-03-21 15:23:19.861604: train_loss -0.9264 
2023-03-21 15:23:19.861853: val_loss -0.8355 
2023-03-21 15:23:19.861972: Pseudo dice [0.8936, 0.8752] 
2023-03-21 15:23:19.862090: Epoch time: 13.68 s 
2023-03-21 15:23:21.143426:  
2023-03-21 15:23:21.143557: Epoch 676 
2023-03-21 15:23:21.143703: Current learning rate: 0.00363 
2023-03-21 15:23:34.978898: train_loss -0.9271 
2023-03-21 15:23:34.979218: val_loss -0.8357 
2023-03-21 15:23:34.979347: Pseudo dice [0.8936, 0.8781] 
2023-03-21 15:23:34.979453: Epoch time: 13.84 s 
2023-03-21 15:23:36.215491:  
2023-03-21 15:23:36.215747: Epoch 677 
2023-03-21 15:23:36.215950: Current learning rate: 0.00362 
2023-03-21 15:23:50.109042: train_loss -0.926 
2023-03-21 15:23:50.109304: val_loss -0.8361 
2023-03-21 15:23:50.109419: Pseudo dice [0.8927, 0.8785] 
2023-03-21 15:23:50.109518: Epoch time: 13.89 s 
2023-03-21 15:23:51.329341:  
2023-03-21 15:23:51.329478: Epoch 678 
2023-03-21 15:23:51.329636: Current learning rate: 0.00361 
2023-03-21 15:24:05.166460: train_loss -0.9259 
2023-03-21 15:24:05.166728: val_loss -0.8381 
2023-03-21 15:24:05.166882: Pseudo dice [0.8954, 0.8767] 
2023-03-21 15:24:05.166989: Epoch time: 13.84 s 
2023-03-21 15:24:06.478477:  
2023-03-21 15:24:06.478623: Epoch 679 
2023-03-21 15:24:06.478798: Current learning rate: 0.0036 
2023-03-21 15:24:20.087237: train_loss -0.927 
2023-03-21 15:24:20.087505: val_loss -0.838 
2023-03-21 15:24:20.087623: Pseudo dice [0.8943, 0.8756] 
2023-03-21 15:24:20.087724: Epoch time: 13.61 s 
2023-03-21 15:24:21.441184:  
2023-03-21 15:24:21.441321: Epoch 680 
2023-03-21 15:24:21.441482: Current learning rate: 0.00359 
2023-03-21 15:24:35.919142: train_loss -0.9262 
2023-03-21 15:24:35.919436: val_loss -0.8377 
2023-03-21 15:24:35.919566: Pseudo dice [0.8932, 0.8783] 
2023-03-21 15:24:35.919696: Epoch time: 14.48 s 
2023-03-21 15:24:37.199556:  
2023-03-21 15:24:37.199688: Epoch 681 
2023-03-21 15:24:37.199833: Current learning rate: 0.00358 
2023-03-21 15:24:50.829728: train_loss -0.928 
2023-03-21 15:24:50.830004: val_loss -0.8404 
2023-03-21 15:24:50.830118: Pseudo dice [0.8947, 0.8801] 
2023-03-21 15:24:50.830220: Epoch time: 13.63 s 
2023-03-21 15:24:52.161791:  
2023-03-21 15:24:52.161922: Epoch 682 
2023-03-21 15:24:52.162065: Current learning rate: 0.00357 
2023-03-21 15:25:06.054659: train_loss -0.9266 
2023-03-21 15:25:06.054981: val_loss -0.8381 
2023-03-21 15:25:06.055227: Pseudo dice [0.895, 0.8785] 
2023-03-21 15:25:06.055501: Epoch time: 13.89 s 
2023-03-21 15:25:07.285311:  
2023-03-21 15:25:07.285435: Epoch 683 
2023-03-21 15:25:07.285589: Current learning rate: 0.00356 
2023-03-21 15:25:20.713327: train_loss -0.9264 
2023-03-21 15:25:20.713582: val_loss -0.8401 
2023-03-21 15:25:20.713779: Pseudo dice [0.896, 0.8794] 
2023-03-21 15:25:20.714081: Epoch time: 13.43 s 
2023-03-21 15:25:21.940372:  
2023-03-21 15:25:21.940496: Epoch 684 
2023-03-21 15:25:21.940640: Current learning rate: 0.00355 
2023-03-21 15:25:35.648081: train_loss -0.9255 
2023-03-21 15:25:35.648350: val_loss -0.8376 
2023-03-21 15:25:35.648452: Pseudo dice [0.8951, 0.8779] 
2023-03-21 15:25:35.648547: Epoch time: 13.71 s 
2023-03-21 15:25:36.897946:  
2023-03-21 15:25:36.898072: Epoch 685 
2023-03-21 15:25:36.898213: Current learning rate: 0.00354 
2023-03-21 15:25:50.333757: train_loss -0.9276 
2023-03-21 15:25:50.333990: val_loss -0.839 
2023-03-21 15:25:50.334104: Pseudo dice [0.8951, 0.8798] 
2023-03-21 15:25:50.334199: Epoch time: 13.44 s 
2023-03-21 15:25:51.570165:  
2023-03-21 15:25:51.570290: Epoch 686 
2023-03-21 15:25:51.570431: Current learning rate: 0.00353 
2023-03-21 15:26:05.258662: train_loss -0.926 
2023-03-21 15:26:05.260058: val_loss -0.8367 
2023-03-21 15:26:05.260279: Pseudo dice [0.894, 0.8757] 
2023-03-21 15:26:05.260417: Epoch time: 13.69 s 
2023-03-21 15:26:06.484639:  
2023-03-21 15:26:06.484768: Epoch 687 
2023-03-21 15:26:06.484911: Current learning rate: 0.00352 
2023-03-21 15:26:20.347178: train_loss -0.9269 
2023-03-21 15:26:20.347422: val_loss -0.8403 
2023-03-21 15:26:20.347536: Pseudo dice [0.8958, 0.8777] 
2023-03-21 15:26:20.347634: Epoch time: 13.86 s 
2023-03-21 15:26:21.626303:  
2023-03-21 15:26:21.626426: Epoch 688 
2023-03-21 15:26:21.626569: Current learning rate: 0.00351 
2023-03-21 15:26:35.070168: train_loss -0.9267 
2023-03-21 15:26:35.070444: val_loss -0.8362 
2023-03-21 15:26:35.070555: Pseudo dice [0.8943, 0.8789] 
2023-03-21 15:26:35.070657: Epoch time: 13.44 s 
2023-03-21 15:26:36.517829:  
2023-03-21 15:26:36.517968: Epoch 689 
2023-03-21 15:26:36.518114: Current learning rate: 0.0035 
2023-03-21 15:26:50.307479: train_loss -0.9258 
2023-03-21 15:26:50.307751: val_loss -0.8393 
2023-03-21 15:26:50.307874: Pseudo dice [0.8941, 0.8795] 
2023-03-21 15:26:50.307979: Epoch time: 13.79 s 
2023-03-21 15:26:51.581137:  
2023-03-21 15:26:51.581271: Epoch 690 
2023-03-21 15:26:51.581422: Current learning rate: 0.00349 
2023-03-21 15:27:05.080655: train_loss -0.9265 
2023-03-21 15:27:05.080904: val_loss -0.8375 
2023-03-21 15:27:05.081002: Pseudo dice [0.8952, 0.8784] 
2023-03-21 15:27:05.081098: Epoch time: 13.5 s 
2023-03-21 15:27:06.317834:  
2023-03-21 15:27:06.317965: Epoch 691 
2023-03-21 15:27:06.318110: Current learning rate: 0.00348 
2023-03-21 15:27:19.926988: train_loss -0.9264 
2023-03-21 15:27:19.927302: val_loss -0.8412 
2023-03-21 15:27:19.927441: Pseudo dice [0.8959, 0.8791] 
2023-03-21 15:27:19.927565: Epoch time: 13.61 s 
2023-03-21 15:27:21.194954:  
2023-03-21 15:27:21.195082: Epoch 692 
2023-03-21 15:27:21.195228: Current learning rate: 0.00346 
2023-03-21 15:27:34.638538: train_loss -0.9266 
2023-03-21 15:27:34.638834: val_loss -0.8391 
2023-03-21 15:27:34.638944: Pseudo dice [0.8957, 0.8788] 
2023-03-21 15:27:34.639063: Epoch time: 13.44 s 
2023-03-21 15:27:35.899169:  
2023-03-21 15:27:35.899291: Epoch 693 
2023-03-21 15:27:35.899433: Current learning rate: 0.00345 
2023-03-21 15:27:49.602483: train_loss -0.9268 
2023-03-21 15:27:49.602672: val_loss -0.8428 
2023-03-21 15:27:49.602825: Pseudo dice [0.8971, 0.8809] 
2023-03-21 15:27:49.602925: Epoch time: 13.7 s 
2023-03-21 15:27:50.984738:  
2023-03-21 15:27:50.984867: Epoch 694 
2023-03-21 15:27:50.985028: Current learning rate: 0.00344 
2023-03-21 15:28:04.527737: train_loss -0.9264 
2023-03-21 15:28:04.528046: val_loss -0.8342 
2023-03-21 15:28:04.528181: Pseudo dice [0.8926, 0.8748] 
2023-03-21 15:28:04.528318: Epoch time: 13.54 s 
2023-03-21 15:28:05.811422:  
2023-03-21 15:28:05.811552: Epoch 695 
2023-03-21 15:28:05.811698: Current learning rate: 0.00343 
2023-03-21 15:28:19.344564: train_loss -0.9263 
2023-03-21 15:28:19.344813: val_loss -0.8366 
2023-03-21 15:28:19.344929: Pseudo dice [0.894, 0.8766] 
2023-03-21 15:28:19.345027: Epoch time: 13.53 s 
2023-03-21 15:28:20.627326:  
2023-03-21 15:28:20.627451: Epoch 696 
2023-03-21 15:28:20.627680: Current learning rate: 0.00342 
2023-03-21 15:28:34.295958: train_loss -0.9274 
2023-03-21 15:28:34.296279: val_loss -0.8418 
2023-03-21 15:28:34.296688: Pseudo dice [0.8964, 0.8804] 
2023-03-21 15:28:34.296801: Epoch time: 13.67 s 
2023-03-21 15:28:35.608232:  
2023-03-21 15:28:35.608359: Epoch 697 
2023-03-21 15:28:35.608510: Current learning rate: 0.00341 
2023-03-21 15:28:49.423000: train_loss -0.928 
2023-03-21 15:28:49.423280: val_loss -0.8338 
2023-03-21 15:28:49.423397: Pseudo dice [0.891, 0.876] 
2023-03-21 15:28:49.423510: Epoch time: 13.82 s 
2023-03-21 15:28:50.653790:  
2023-03-21 15:28:50.654035: Epoch 698 
2023-03-21 15:28:50.654294: Current learning rate: 0.0034 
2023-03-21 15:29:03.904173: train_loss -0.9261 
2023-03-21 15:29:03.904401: val_loss -0.8364 
2023-03-21 15:29:03.904498: Pseudo dice [0.892, 0.8748] 
2023-03-21 15:29:03.904597: Epoch time: 13.25 s 
2023-03-21 15:29:05.368565:  
2023-03-21 15:29:05.368711: Epoch 699 
2023-03-21 15:29:05.368861: Current learning rate: 0.00339 
2023-03-21 15:29:18.998538: train_loss -0.9284 
2023-03-21 15:29:18.998994: val_loss -0.8369 
2023-03-21 15:29:18.999365: Pseudo dice [0.8926, 0.8783] 
2023-03-21 15:29:18.999615: Epoch time: 13.63 s 
2023-03-21 15:29:20.581696:  
2023-03-21 15:29:20.581912: Epoch 700 
2023-03-21 15:29:20.582172: Current learning rate: 0.00338 
2023-03-21 15:29:34.993739: train_loss -0.927 
2023-03-21 15:29:34.993944: val_loss -0.8391 
2023-03-21 15:29:34.994075: Pseudo dice [0.8953, 0.8801] 
2023-03-21 15:29:34.994231: Epoch time: 14.41 s 
2023-03-21 15:29:36.229238:  
2023-03-21 15:29:36.229369: Epoch 701 
2023-03-21 15:29:36.229525: Current learning rate: 0.00337 
2023-03-21 15:29:50.444503: train_loss -0.9276 
2023-03-21 15:29:50.444736: val_loss -0.8397 
2023-03-21 15:29:50.444839: Pseudo dice [0.896, 0.8798] 
2023-03-21 15:29:50.444940: Epoch time: 14.22 s 
2023-03-21 15:29:51.740716:  
2023-03-21 15:29:51.740844: Epoch 702 
2023-03-21 15:29:51.741002: Current learning rate: 0.00336 
2023-03-21 15:30:05.623590: train_loss -0.9272 
2023-03-21 15:30:05.623806: val_loss -0.836 
2023-03-21 15:30:05.624157: Pseudo dice [0.894, 0.8779] 
2023-03-21 15:30:05.624256: Epoch time: 13.88 s 
2023-03-21 15:30:06.857962:  
2023-03-21 15:30:06.858093: Epoch 703 
2023-03-21 15:30:06.858249: Current learning rate: 0.00335 
2023-03-21 15:30:20.839588: train_loss -0.9273 
2023-03-21 15:30:20.839882: val_loss -0.8352 
2023-03-21 15:30:20.840019: Pseudo dice [0.8939, 0.8757] 
2023-03-21 15:30:20.840153: Epoch time: 13.98 s 
2023-03-21 15:30:22.289666:  
2023-03-21 15:30:22.289961: Epoch 704 
2023-03-21 15:30:22.290185: Current learning rate: 0.00334 
2023-03-21 15:30:35.893354: train_loss -0.9294 
2023-03-21 15:30:35.893535: val_loss -0.8401 
2023-03-21 15:30:35.893645: Pseudo dice [0.8945, 0.8806] 
2023-03-21 15:30:35.893733: Epoch time: 13.6 s 
2023-03-21 15:30:37.091649:  
2023-03-21 15:30:37.091778: Epoch 705 
2023-03-21 15:30:37.091944: Current learning rate: 0.00333 
2023-03-21 15:30:51.107337: train_loss -0.9276 
2023-03-21 15:30:51.107689: val_loss -0.8325 
2023-03-21 15:30:51.107795: Pseudo dice [0.8921, 0.8739] 
2023-03-21 15:30:51.108081: Epoch time: 14.02 s 
2023-03-21 15:30:52.357875:  
2023-03-21 15:30:52.358107: Epoch 706 
2023-03-21 15:30:52.358414: Current learning rate: 0.00332 
2023-03-21 15:31:06.066944: train_loss -0.927 
2023-03-21 15:31:06.067178: val_loss -0.8388 
2023-03-21 15:31:06.067278: Pseudo dice [0.8939, 0.8791] 
2023-03-21 15:31:06.067389: Epoch time: 13.71 s 
2023-03-21 15:31:07.307530:  
2023-03-21 15:31:07.307654: Epoch 707 
2023-03-21 15:31:07.307798: Current learning rate: 0.00331 
2023-03-21 15:31:20.895397: train_loss -0.9267 
2023-03-21 15:31:20.895673: val_loss -0.8377 
2023-03-21 15:31:20.895774: Pseudo dice [0.8954, 0.8786] 
2023-03-21 15:31:20.895871: Epoch time: 13.59 s 
2023-03-21 15:31:22.281726:  
2023-03-21 15:31:22.281861: Epoch 708 
2023-03-21 15:31:22.282004: Current learning rate: 0.0033 
2023-03-21 15:31:35.963603: train_loss -0.9274 
2023-03-21 15:31:35.963986: val_loss -0.834 
2023-03-21 15:31:35.964208: Pseudo dice [0.8908, 0.876] 
2023-03-21 15:31:35.964405: Epoch time: 13.68 s 
2023-03-21 15:31:37.240539:  
2023-03-21 15:31:37.241015: Epoch 709 
2023-03-21 15:31:37.241261: Current learning rate: 0.00329 
2023-03-21 15:31:50.937056: train_loss -0.9287 
2023-03-21 15:31:50.937325: val_loss -0.8372 
2023-03-21 15:31:50.937443: Pseudo dice [0.8935, 0.8773] 
2023-03-21 15:31:50.937549: Epoch time: 13.7 s 
2023-03-21 15:31:52.186055:  
2023-03-21 15:31:52.186311: Epoch 710 
2023-03-21 15:31:52.186562: Current learning rate: 0.00328 
2023-03-21 15:32:05.960099: train_loss -0.926 
2023-03-21 15:32:05.960358: val_loss -0.8316 
2023-03-21 15:32:05.960458: Pseudo dice [0.8904, 0.8748] 
2023-03-21 15:32:05.960556: Epoch time: 13.77 s 
2023-03-21 15:32:07.211705:  
2023-03-21 15:32:07.211935: Epoch 711 
2023-03-21 15:32:07.212156: Current learning rate: 0.00327 
2023-03-21 15:32:20.944764: train_loss -0.9263 
2023-03-21 15:32:20.945146: val_loss -0.8371 
2023-03-21 15:32:20.945567: Pseudo dice [0.8939, 0.8793] 
2023-03-21 15:32:20.945749: Epoch time: 13.73 s 
2023-03-21 15:32:22.186435:  
2023-03-21 15:32:22.186556: Epoch 712 
2023-03-21 15:32:22.186699: Current learning rate: 0.00326 
2023-03-21 15:32:35.837513: train_loss -0.9269 
2023-03-21 15:32:35.837899: val_loss -0.8348 
2023-03-21 15:32:35.838244: Pseudo dice [0.8918, 0.8755] 
2023-03-21 15:32:35.838435: Epoch time: 13.65 s 
2023-03-21 15:32:37.303464:  
2023-03-21 15:32:37.303601: Epoch 713 
2023-03-21 15:32:37.303748: Current learning rate: 0.00325 
2023-03-21 15:32:51.078579: train_loss -0.927 
2023-03-21 15:32:51.078898: val_loss -0.8431 
2023-03-21 15:32:51.079015: Pseudo dice [0.8959, 0.8813] 
2023-03-21 15:32:51.079151: Epoch time: 13.78 s 
2023-03-21 15:32:52.365979:  
2023-03-21 15:32:52.366270: Epoch 714 
2023-03-21 15:32:52.366451: Current learning rate: 0.00324 
2023-03-21 15:33:06.043135: train_loss -0.9273 
2023-03-21 15:33:06.043606: val_loss -0.8359 
2023-03-21 15:33:06.043976: Pseudo dice [0.8928, 0.8766] 
2023-03-21 15:33:06.044323: Epoch time: 13.68 s 
2023-03-21 15:33:07.342341:  
2023-03-21 15:33:07.342487: Epoch 715 
2023-03-21 15:33:07.342638: Current learning rate: 0.00323 
2023-03-21 15:33:21.165980: train_loss -0.9276 
2023-03-21 15:33:21.166219: val_loss -0.8436 
2023-03-21 15:33:21.166352: Pseudo dice [0.8981, 0.8814] 
2023-03-21 15:33:21.166474: Epoch time: 13.82 s 
2023-03-21 15:33:22.544425:  
2023-03-21 15:33:22.544567: Epoch 716 
2023-03-21 15:33:22.544697: Current learning rate: 0.00322 
2023-03-21 15:33:36.085998: train_loss -0.9274 
2023-03-21 15:33:36.086268: val_loss -0.8388 
2023-03-21 15:33:36.086368: Pseudo dice [0.8941, 0.8787] 
2023-03-21 15:33:36.086466: Epoch time: 13.54 s 
2023-03-21 15:33:37.420804:  
2023-03-21 15:33:37.420928: Epoch 717 
2023-03-21 15:33:37.421069: Current learning rate: 0.00321 
2023-03-21 15:33:51.034462: train_loss -0.9273 
2023-03-21 15:33:51.034717: val_loss -0.8412 
2023-03-21 15:33:51.034864: Pseudo dice [0.8965, 0.879] 
2023-03-21 15:33:51.034977: Epoch time: 13.61 s 
2023-03-21 15:33:52.477683:  
2023-03-21 15:33:52.477815: Epoch 718 
2023-03-21 15:33:52.477966: Current learning rate: 0.0032 
2023-03-21 15:34:06.316919: train_loss -0.9272 
2023-03-21 15:34:06.317126: val_loss -0.8371 
2023-03-21 15:34:06.317245: Pseudo dice [0.8939, 0.8764] 
2023-03-21 15:34:06.317344: Epoch time: 13.84 s 
2023-03-21 15:34:07.578993:  
2023-03-21 15:34:07.579137: Epoch 719 
2023-03-21 15:34:07.579287: Current learning rate: 0.00319 
2023-03-21 15:34:21.341100: train_loss -0.9263 
2023-03-21 15:34:21.341375: val_loss -0.8395 
2023-03-21 15:34:21.341501: Pseudo dice [0.8957, 0.8807] 
2023-03-21 15:34:21.341595: Epoch time: 13.76 s 
2023-03-21 15:34:22.567094:  
2023-03-21 15:34:22.567219: Epoch 720 
2023-03-21 15:34:22.567379: Current learning rate: 0.00318 
2023-03-21 15:34:36.094177: train_loss -0.926 
2023-03-21 15:34:36.094443: val_loss -0.8352 
2023-03-21 15:34:36.094544: Pseudo dice [0.8924, 0.8765] 
2023-03-21 15:34:36.094662: Epoch time: 13.53 s 
2023-03-21 15:34:37.327762:  
2023-03-21 15:34:37.327890: Epoch 721 
2023-03-21 15:34:37.328034: Current learning rate: 0.00317 
2023-03-21 15:34:51.020125: train_loss -0.9277 
2023-03-21 15:34:51.020554: val_loss -0.839 
2023-03-21 15:34:51.020720: Pseudo dice [0.8939, 0.8787] 
2023-03-21 15:34:51.020842: Epoch time: 13.69 s 
2023-03-21 15:34:52.272666:  
2023-03-21 15:34:52.272819: Epoch 722 
2023-03-21 15:34:52.272988: Current learning rate: 0.00316 
2023-03-21 15:35:06.702705: train_loss -0.9271 
2023-03-21 15:35:06.703123: val_loss -0.838 
2023-03-21 15:35:06.703357: Pseudo dice [0.8941, 0.8779] 
2023-03-21 15:35:06.703573: Epoch time: 14.43 s 
2023-03-21 15:35:08.164072:  
2023-03-21 15:35:08.164212: Epoch 723 
2023-03-21 15:35:08.164369: Current learning rate: 0.00315 
2023-03-21 15:35:22.082997: train_loss -0.9277 
2023-03-21 15:35:22.083254: val_loss -0.8368 
2023-03-21 15:35:22.083364: Pseudo dice [0.8954, 0.8759] 
2023-03-21 15:35:22.083473: Epoch time: 13.92 s 
2023-03-21 15:35:23.326827:  
2023-03-21 15:35:23.326959: Epoch 724 
2023-03-21 15:35:23.327105: Current learning rate: 0.00314 
2023-03-21 15:35:37.266158: train_loss -0.928 
2023-03-21 15:35:37.266473: val_loss -0.8367 
2023-03-21 15:35:37.266585: Pseudo dice [0.8947, 0.8778] 
2023-03-21 15:35:37.266769: Epoch time: 13.94 s 
2023-03-21 15:35:38.517154:  
2023-03-21 15:35:38.517288: Epoch 725 
2023-03-21 15:35:38.517435: Current learning rate: 0.00313 
2023-03-21 15:35:52.363930: train_loss -0.9292 
2023-03-21 15:35:52.364239: val_loss -0.8343 
2023-03-21 15:35:52.364366: Pseudo dice [0.8933, 0.8757] 
2023-03-21 15:35:52.364490: Epoch time: 13.85 s 
2023-03-21 15:35:53.621461:  
2023-03-21 15:35:53.621600: Epoch 726 
2023-03-21 15:35:53.621747: Current learning rate: 0.00312 
2023-03-21 15:36:07.110850: train_loss -0.9291 
2023-03-21 15:36:07.111096: val_loss -0.8384 
2023-03-21 15:36:07.111240: Pseudo dice [0.8961, 0.8801] 
2023-03-21 15:36:07.111382: Epoch time: 13.49 s 
2023-03-21 15:36:08.347207:  
2023-03-21 15:36:08.347395: Epoch 727 
2023-03-21 15:36:08.347591: Current learning rate: 0.00311 
2023-03-21 15:36:22.201971: train_loss -0.9273 
2023-03-21 15:36:22.202197: val_loss -0.8366 
2023-03-21 15:36:22.202298: Pseudo dice [0.8947, 0.878] 
2023-03-21 15:36:22.202390: Epoch time: 13.86 s 
2023-03-21 15:36:23.564449:  
2023-03-21 15:36:23.564619: Epoch 728 
2023-03-21 15:36:23.564780: Current learning rate: 0.0031 
2023-03-21 15:36:37.344654: train_loss -0.9268 
2023-03-21 15:36:37.345120: val_loss -0.8392 
2023-03-21 15:36:37.345404: Pseudo dice [0.8946, 0.878] 
2023-03-21 15:36:37.345658: Epoch time: 13.78 s 
2023-03-21 15:36:38.615153:  
2023-03-21 15:36:38.615513: Epoch 729 
2023-03-21 15:36:38.615769: Current learning rate: 0.00309 
2023-03-21 15:36:52.694398: train_loss -0.9288 
2023-03-21 15:36:52.694843: val_loss -0.8396 
2023-03-21 15:36:52.695213: Pseudo dice [0.8957, 0.8781] 
2023-03-21 15:36:52.695367: Epoch time: 14.08 s 
2023-03-21 15:36:53.964948:  
2023-03-21 15:36:53.965097: Epoch 730 
2023-03-21 15:36:53.965236: Current learning rate: 0.00308 
2023-03-21 15:37:07.912016: train_loss -0.9286 
2023-03-21 15:37:07.912277: val_loss -0.8351 
2023-03-21 15:37:07.912395: Pseudo dice [0.8933, 0.877] 
2023-03-21 15:37:07.912498: Epoch time: 13.95 s 
2023-03-21 15:37:09.204619:  
2023-03-21 15:37:09.204757: Epoch 731 
2023-03-21 15:37:09.204908: Current learning rate: 0.00307 
2023-03-21 15:37:22.747330: train_loss -0.9288 
2023-03-21 15:37:22.747567: val_loss -0.837 
2023-03-21 15:37:22.747666: Pseudo dice [0.895, 0.8779] 
2023-03-21 15:37:22.747763: Epoch time: 13.54 s 
2023-03-21 15:37:24.026450:  
2023-03-21 15:37:24.026573: Epoch 732 
2023-03-21 15:37:24.026716: Current learning rate: 0.00306 
2023-03-21 15:37:37.570997: train_loss -0.9284 
2023-03-21 15:37:37.571245: val_loss -0.8339 
2023-03-21 15:37:37.571357: Pseudo dice [0.8923, 0.8768] 
2023-03-21 15:37:37.571462: Epoch time: 13.55 s 
2023-03-21 15:37:38.999060:  
2023-03-21 15:37:38.999190: Epoch 733 
2023-03-21 15:37:38.999334: Current learning rate: 0.00305 
2023-03-21 15:37:52.778111: train_loss -0.9273 
2023-03-21 15:37:52.778521: val_loss -0.8374 
2023-03-21 15:37:52.778920: Pseudo dice [0.8954, 0.8799] 
2023-03-21 15:37:52.779361: Epoch time: 13.78 s 
2023-03-21 15:37:54.019811:  
2023-03-21 15:37:54.019952: Epoch 734 
2023-03-21 15:37:54.020113: Current learning rate: 0.00304 
2023-03-21 15:38:07.698023: train_loss -0.9285 
2023-03-21 15:38:07.698402: val_loss -0.8397 
2023-03-21 15:38:07.698613: Pseudo dice [0.8962, 0.8786] 
2023-03-21 15:38:07.698936: Epoch time: 13.68 s 
2023-03-21 15:38:08.943289:  
2023-03-21 15:38:08.943415: Epoch 735 
2023-03-21 15:38:08.943557: Current learning rate: 0.00303 
2023-03-21 15:38:22.655273: train_loss -0.9279 
2023-03-21 15:38:22.655616: val_loss -0.8397 
2023-03-21 15:38:22.655845: Pseudo dice [0.8961, 0.879] 
2023-03-21 15:38:22.656087: Epoch time: 13.71 s 
2023-03-21 15:38:23.984366:  
2023-03-21 15:38:23.984510: Epoch 736 
2023-03-21 15:38:23.984654: Current learning rate: 0.00302 
2023-03-21 15:38:37.467430: train_loss -0.9278 
2023-03-21 15:38:37.467609: val_loss -0.8367 
2023-03-21 15:38:37.467719: Pseudo dice [0.8935, 0.8784] 
2023-03-21 15:38:37.467807: Epoch time: 13.48 s 
2023-03-21 15:38:38.709543:  
2023-03-21 15:38:38.709692: Epoch 737 
2023-03-21 15:38:38.709859: Current learning rate: 0.00301 
2023-03-21 15:38:52.595357: train_loss -0.9271 
2023-03-21 15:38:52.595655: val_loss -0.8368 
2023-03-21 15:38:52.595805: Pseudo dice [0.8926, 0.8776] 
2023-03-21 15:38:52.595933: Epoch time: 13.89 s 
2023-03-21 15:38:53.973525:  
2023-03-21 15:38:53.973659: Epoch 738 
2023-03-21 15:38:53.973822: Current learning rate: 0.003 
2023-03-21 15:39:07.907256: train_loss -0.9286 
2023-03-21 15:39:07.907523: val_loss -0.8445 
2023-03-21 15:39:07.907625: Pseudo dice [0.8985, 0.8822] 
2023-03-21 15:39:07.907727: Epoch time: 13.93 s 
2023-03-21 15:39:09.182944:  
2023-03-21 15:39:09.183308: Epoch 739 
2023-03-21 15:39:09.183501: Current learning rate: 0.00299 
2023-03-21 15:39:22.823501: train_loss -0.929 
2023-03-21 15:39:22.823861: val_loss -0.8317 
2023-03-21 15:39:22.824198: Pseudo dice [0.8909, 0.8737] 
2023-03-21 15:39:22.824416: Epoch time: 13.64 s 
2023-03-21 15:39:24.099927:  
2023-03-21 15:39:24.100400: Epoch 740 
2023-03-21 15:39:24.100666: Current learning rate: 0.00297 
2023-03-21 15:39:37.927554: train_loss -0.9295 
2023-03-21 15:39:37.927805: val_loss -0.8384 
2023-03-21 15:39:37.927923: Pseudo dice [0.894, 0.8794] 
2023-03-21 15:39:37.928037: Epoch time: 13.83 s 
2023-03-21 15:39:39.237792:  
2023-03-21 15:39:39.237917: Epoch 741 
2023-03-21 15:39:39.238059: Current learning rate: 0.00296 
2023-03-21 15:39:52.914774: train_loss -0.9305 
2023-03-21 15:39:52.915425: val_loss -0.8382 
2023-03-21 15:39:52.915628: Pseudo dice [0.8956, 0.8801] 
2023-03-21 15:39:52.915872: Epoch time: 13.68 s 
2023-03-21 15:39:54.308713:  
2023-03-21 15:39:54.308890: Epoch 742 
2023-03-21 15:39:54.309043: Current learning rate: 0.00295 
2023-03-21 15:40:07.836592: train_loss -0.929 
2023-03-21 15:40:07.836834: val_loss -0.8416 
2023-03-21 15:40:07.836947: Pseudo dice [0.8971, 0.8796] 
2023-03-21 15:40:07.837056: Epoch time: 13.53 s 
2023-03-21 15:40:09.101970:  
2023-03-21 15:40:09.102200: Epoch 743 
2023-03-21 15:40:09.102421: Current learning rate: 0.00294 
2023-03-21 15:40:22.895472: train_loss -0.9292 
2023-03-21 15:40:22.895756: val_loss -0.8372 
2023-03-21 15:40:22.895932: Pseudo dice [0.8943, 0.8787] 
2023-03-21 15:40:22.896468: Epoch time: 13.79 s 
2023-03-21 15:40:24.150029:  
2023-03-21 15:40:24.150154: Epoch 744 
2023-03-21 15:40:24.150331: Current learning rate: 0.00293 
2023-03-21 15:40:37.771200: train_loss -0.9279 
2023-03-21 15:40:37.771423: val_loss -0.8361 
2023-03-21 15:40:37.771535: Pseudo dice [0.8954, 0.8769] 
2023-03-21 15:40:37.771629: Epoch time: 13.62 s 
2023-03-21 15:40:39.001782:  
2023-03-21 15:40:39.002005: Epoch 745 
2023-03-21 15:40:39.002248: Current learning rate: 0.00292 
2023-03-21 15:40:52.836469: train_loss -0.928 
2023-03-21 15:40:52.836689: val_loss -0.837 
2023-03-21 15:40:52.836813: Pseudo dice [0.8919, 0.8785] 
2023-03-21 15:40:52.836917: Epoch time: 13.84 s 
2023-03-21 15:40:54.156495:  
2023-03-21 15:40:54.156707: Epoch 746 
2023-03-21 15:40:54.157009: Current learning rate: 0.00291 
2023-03-21 15:41:07.862401: train_loss -0.929 
2023-03-21 15:41:07.862652: val_loss -0.8327 
2023-03-21 15:41:07.862793: Pseudo dice [0.8909, 0.8752] 
2023-03-21 15:41:07.862899: Epoch time: 13.71 s 
2023-03-21 15:41:09.238580:  
2023-03-21 15:41:09.238715: Epoch 747 
2023-03-21 15:41:09.238906: Current learning rate: 0.0029 
2023-03-21 15:41:22.839980: train_loss -0.9291 
2023-03-21 15:41:22.840220: val_loss -0.8339 
2023-03-21 15:41:22.840323: Pseudo dice [0.8928, 0.8745] 
2023-03-21 15:41:22.840421: Epoch time: 13.6 s 
2023-03-21 15:41:24.097367:  
2023-03-21 15:41:24.097530: Epoch 748 
2023-03-21 15:41:24.097723: Current learning rate: 0.00289 
2023-03-21 15:41:38.012893: train_loss -0.928 
2023-03-21 15:41:38.013144: val_loss -0.8372 
2023-03-21 15:41:38.013367: Pseudo dice [0.8943, 0.8763] 
2023-03-21 15:41:38.013586: Epoch time: 13.92 s 
2023-03-21 15:41:39.213123:  
2023-03-21 15:41:39.213245: Epoch 749 
2023-03-21 15:41:39.213404: Current learning rate: 0.00288 
2023-03-21 15:41:52.700501: train_loss -0.9292 
2023-03-21 15:41:52.700876: val_loss -0.8365 
2023-03-21 15:41:52.701375: Pseudo dice [0.8927, 0.876] 
2023-03-21 15:41:52.701547: Epoch time: 13.49 s 
2023-03-21 15:41:54.240428:  
2023-03-21 15:41:54.240555: Epoch 750 
2023-03-21 15:41:54.240698: Current learning rate: 0.00287 
2023-03-21 15:42:08.081356: train_loss -0.9293 
2023-03-21 15:42:08.081671: val_loss -0.8387 
2023-03-21 15:42:08.081810: Pseudo dice [0.8948, 0.8797] 
2023-03-21 15:42:08.081956: Epoch time: 13.84 s 
2023-03-21 15:42:09.310923:  
2023-03-21 15:42:09.311045: Epoch 751 
2023-03-21 15:42:09.311191: Current learning rate: 0.00286 
2023-03-21 15:42:22.791331: train_loss -0.9294 
2023-03-21 15:42:22.791616: val_loss -0.841 
2023-03-21 15:42:22.791724: Pseudo dice [0.8974, 0.8806] 
2023-03-21 15:42:22.791847: Epoch time: 13.48 s 
2023-03-21 15:42:24.231363:  
2023-03-21 15:42:24.231496: Epoch 752 
2023-03-21 15:42:24.231650: Current learning rate: 0.00285 
2023-03-21 15:42:38.457482: train_loss -0.9293 
2023-03-21 15:42:38.457861: val_loss -0.8398 
2023-03-21 15:42:38.457999: Pseudo dice [0.8946, 0.8797] 
2023-03-21 15:42:38.458162: Epoch time: 14.23 s 
2023-03-21 15:42:39.700088:  
2023-03-21 15:42:39.700228: Epoch 753 
2023-03-21 15:42:39.700373: Current learning rate: 0.00284 
2023-03-21 15:42:53.253592: train_loss -0.9283 
2023-03-21 15:42:53.253795: val_loss -0.8386 
2023-03-21 15:42:53.253893: Pseudo dice [0.8946, 0.8799] 
2023-03-21 15:42:53.253985: Epoch time: 13.55 s 
2023-03-21 15:42:54.539384:  
2023-03-21 15:42:54.539584: Epoch 754 
2023-03-21 15:42:54.539812: Current learning rate: 0.00283 
2023-03-21 15:43:08.218796: train_loss -0.9288 
2023-03-21 15:43:08.218998: val_loss -0.8375 
2023-03-21 15:43:08.219114: Pseudo dice [0.8955, 0.8778] 
2023-03-21 15:43:08.219211: Epoch time: 13.68 s 
2023-03-21 15:43:09.437695:  
2023-03-21 15:43:09.437834: Epoch 755 
2023-03-21 15:43:09.437995: Current learning rate: 0.00282 
2023-03-21 15:43:22.829542: train_loss -0.9297 
2023-03-21 15:43:22.829908: val_loss -0.8359 
2023-03-21 15:43:22.830088: Pseudo dice [0.8941, 0.8786] 
2023-03-21 15:43:22.830217: Epoch time: 13.39 s 
2023-03-21 15:43:24.223732:  
2023-03-21 15:43:24.223871: Epoch 756 
2023-03-21 15:43:24.224022: Current learning rate: 0.00281 
2023-03-21 15:43:38.378197: train_loss -0.9286 
2023-03-21 15:43:38.378656: val_loss -0.8383 
2023-03-21 15:43:38.378935: Pseudo dice [0.8932, 0.8801] 
2023-03-21 15:43:38.379237: Epoch time: 14.16 s 
2023-03-21 15:43:39.774031:  
2023-03-21 15:43:39.774157: Epoch 757 
2023-03-21 15:43:39.774303: Current learning rate: 0.0028 
2023-03-21 15:43:53.670906: train_loss -0.9271 
2023-03-21 15:43:53.671162: val_loss -0.8406 
2023-03-21 15:43:53.671289: Pseudo dice [0.8967, 0.88] 
2023-03-21 15:43:53.671419: Epoch time: 13.9 s 
2023-03-21 15:43:54.947690:  
2023-03-21 15:43:54.947845: Epoch 758 
2023-03-21 15:43:54.948014: Current learning rate: 0.00279 
2023-03-21 15:44:08.911152: train_loss -0.9302 
2023-03-21 15:44:08.911591: val_loss -0.8419 
2023-03-21 15:44:08.911787: Pseudo dice [0.8977, 0.8807] 
2023-03-21 15:44:08.911972: Epoch time: 13.96 s 
2023-03-21 15:44:10.149443:  
2023-03-21 15:44:10.149568: Epoch 759 
2023-03-21 15:44:10.149710: Current learning rate: 0.00278 
2023-03-21 15:44:23.664174: train_loss -0.9298 
2023-03-21 15:44:23.664431: val_loss -0.8374 
2023-03-21 15:44:23.664537: Pseudo dice [0.8941, 0.8801] 
2023-03-21 15:44:23.664658: Epoch time: 13.52 s 
2023-03-21 15:44:24.912831:  
2023-03-21 15:44:24.912952: Epoch 760 
2023-03-21 15:44:24.913099: Current learning rate: 0.00277 
2023-03-21 15:44:38.754322: train_loss -0.9282 
2023-03-21 15:44:38.754517: val_loss -0.8441 
2023-03-21 15:44:38.754639: Pseudo dice [0.8985, 0.882] 
2023-03-21 15:44:38.754794: Epoch time: 13.84 s 
2023-03-21 15:44:40.142902:  
2023-03-21 15:44:40.143064: Epoch 761 
2023-03-21 15:44:40.143236: Current learning rate: 0.00276 
2023-03-21 15:44:53.913932: train_loss -0.9285 
2023-03-21 15:44:53.914190: val_loss -0.8408 
2023-03-21 15:44:53.914292: Pseudo dice [0.8964, 0.8791] 
2023-03-21 15:44:53.914391: Epoch time: 13.77 s 
2023-03-21 15:44:55.134242:  
2023-03-21 15:44:55.134367: Epoch 762 
2023-03-21 15:44:55.134508: Current learning rate: 0.00275 
2023-03-21 15:45:08.621012: train_loss -0.9307 
2023-03-21 15:45:08.621262: val_loss -0.832 
2023-03-21 15:45:08.621375: Pseudo dice [0.8909, 0.8738] 
2023-03-21 15:45:08.621473: Epoch time: 13.49 s 
2023-03-21 15:45:09.985578:  
2023-03-21 15:45:09.985756: Epoch 763 
2023-03-21 15:45:09.985908: Current learning rate: 0.00274 
2023-03-21 15:45:23.613855: train_loss -0.9305 
2023-03-21 15:45:23.614110: val_loss -0.8297 
2023-03-21 15:45:23.614213: Pseudo dice [0.8894, 0.8729] 
2023-03-21 15:45:23.614333: Epoch time: 13.63 s 
2023-03-21 15:45:24.890559:  
2023-03-21 15:45:24.890683: Epoch 764 
2023-03-21 15:45:24.890901: Current learning rate: 0.00273 
2023-03-21 15:45:38.539204: train_loss -0.93 
2023-03-21 15:45:38.539464: val_loss -0.8351 
2023-03-21 15:45:38.539591: Pseudo dice [0.8938, 0.8774] 
2023-03-21 15:45:38.539702: Epoch time: 13.65 s 
2023-03-21 15:45:39.831833:  
2023-03-21 15:45:39.831955: Epoch 765 
2023-03-21 15:45:39.832098: Current learning rate: 0.00272 
2023-03-21 15:45:53.440521: train_loss -0.931 
2023-03-21 15:45:53.440896: val_loss -0.8349 
2023-03-21 15:45:53.441208: Pseudo dice [0.8939, 0.8774] 
2023-03-21 15:45:53.441373: Epoch time: 13.61 s 
2023-03-21 15:45:54.867939:  
2023-03-21 15:45:54.868070: Epoch 766 
2023-03-21 15:45:54.868216: Current learning rate: 0.00271 
2023-03-21 15:46:08.505654: train_loss -0.9301 
2023-03-21 15:46:08.506057: val_loss -0.8367 
2023-03-21 15:46:08.506157: Pseudo dice [0.8959, 0.8787] 
2023-03-21 15:46:08.506256: Epoch time: 13.64 s 
2023-03-21 15:46:09.745080:  
2023-03-21 15:46:09.745217: Epoch 767 
2023-03-21 15:46:09.745359: Current learning rate: 0.0027 
2023-03-21 15:46:23.571805: train_loss -0.9301 
2023-03-21 15:46:23.572054: val_loss -0.8324 
2023-03-21 15:46:23.572150: Pseudo dice [0.8924, 0.8749] 
2023-03-21 15:46:23.572260: Epoch time: 13.83 s 
2023-03-21 15:46:24.826596:  
2023-03-21 15:46:24.826766: Epoch 768 
2023-03-21 15:46:24.826923: Current learning rate: 0.00268 
2023-03-21 15:46:38.723470: train_loss -0.9283 
2023-03-21 15:46:38.723713: val_loss -0.8381 
2023-03-21 15:46:38.723832: Pseudo dice [0.8943, 0.879] 
2023-03-21 15:46:38.723938: Epoch time: 13.9 s 
2023-03-21 15:46:40.026384:  
2023-03-21 15:46:40.026515: Epoch 769 
2023-03-21 15:46:40.026668: Current learning rate: 0.00267 
2023-03-21 15:46:53.873349: train_loss -0.9293 
2023-03-21 15:46:53.873724: val_loss -0.8325 
2023-03-21 15:46:53.873851: Pseudo dice [0.8921, 0.8757] 
2023-03-21 15:46:53.873960: Epoch time: 13.85 s 
2023-03-21 15:46:55.246131:  
2023-03-21 15:46:55.246279: Epoch 770 
2023-03-21 15:46:55.246499: Current learning rate: 0.00266 
2023-03-21 15:47:09.002678: train_loss -0.93 
2023-03-21 15:47:09.003012: val_loss -0.8351 
2023-03-21 15:47:09.003136: Pseudo dice [0.8945, 0.8769] 
2023-03-21 15:47:09.003247: Epoch time: 13.76 s 
2023-03-21 15:47:10.313851:  
2023-03-21 15:47:10.313986: Epoch 771 
2023-03-21 15:47:10.314138: Current learning rate: 0.00265 
2023-03-21 15:47:24.340275: train_loss -0.9295 
2023-03-21 15:47:24.340523: val_loss -0.8335 
2023-03-21 15:47:24.340642: Pseudo dice [0.893, 0.877] 
2023-03-21 15:47:24.340755: Epoch time: 14.03 s 
2023-03-21 15:47:25.614462:  
2023-03-21 15:47:25.614593: Epoch 772 
2023-03-21 15:47:25.614751: Current learning rate: 0.00264 
2023-03-21 15:47:39.342356: train_loss -0.9298 
2023-03-21 15:47:39.342651: val_loss -0.8365 
2023-03-21 15:47:39.342788: Pseudo dice [0.8941, 0.8769] 
2023-03-21 15:47:39.342918: Epoch time: 13.73 s 
2023-03-21 15:47:40.648408:  
2023-03-21 15:47:40.648529: Epoch 773 
2023-03-21 15:47:40.648672: Current learning rate: 0.00263 
2023-03-21 15:47:54.184275: train_loss -0.9291 
2023-03-21 15:47:54.184610: val_loss -0.8368 
2023-03-21 15:47:54.184869: Pseudo dice [0.8942, 0.8798] 
2023-03-21 15:47:54.185094: Epoch time: 13.54 s 
2023-03-21 15:47:55.447294:  
2023-03-21 15:47:55.447417: Epoch 774 
2023-03-21 15:47:55.447578: Current learning rate: 0.00262 
2023-03-21 15:48:09.002582: train_loss -0.9304 
2023-03-21 15:48:09.002949: val_loss -0.8371 
2023-03-21 15:48:09.003128: Pseudo dice [0.8949, 0.88] 
2023-03-21 15:48:09.003365: Epoch time: 13.56 s 
2023-03-21 15:48:10.377293:  
2023-03-21 15:48:10.377453: Epoch 775 
2023-03-21 15:48:10.377586: Current learning rate: 0.00261 
2023-03-21 15:48:24.901337: train_loss -0.9303 
2023-03-21 15:48:24.901610: val_loss -0.835 
2023-03-21 15:48:24.901709: Pseudo dice [0.8947, 0.8764] 
2023-03-21 15:48:24.901808: Epoch time: 14.52 s 
2023-03-21 15:48:26.158314:  
2023-03-21 15:48:26.158445: Epoch 776 
2023-03-21 15:48:26.158588: Current learning rate: 0.0026 
2023-03-21 15:48:40.468006: train_loss -0.9309 
2023-03-21 15:48:40.468277: val_loss -0.8304 
2023-03-21 15:48:40.468387: Pseudo dice [0.8916, 0.875] 
2023-03-21 15:48:40.468494: Epoch time: 14.31 s 
2023-03-21 15:48:41.770907:  
2023-03-21 15:48:41.771068: Epoch 777 
2023-03-21 15:48:41.771204: Current learning rate: 0.00259 
2023-03-21 15:48:55.770611: train_loss -0.9298 
2023-03-21 15:48:55.770892: val_loss -0.8367 
2023-03-21 15:48:55.771000: Pseudo dice [0.8928, 0.878] 
2023-03-21 15:48:55.771116: Epoch time: 14.0 s 
2023-03-21 15:48:57.036253:  
2023-03-21 15:48:57.036379: Epoch 778 
2023-03-21 15:48:57.036507: Current learning rate: 0.00258 
2023-03-21 15:49:10.580574: train_loss -0.9312 
2023-03-21 15:49:10.580824: val_loss -0.8378 
2023-03-21 15:49:10.580939: Pseudo dice [0.8938, 0.8794] 
2023-03-21 15:49:10.581048: Epoch time: 13.54 s 
2023-03-21 15:49:11.832399:  
2023-03-21 15:49:11.832664: Epoch 779 
2023-03-21 15:49:11.833039: Current learning rate: 0.00257 
2023-03-21 15:49:25.527909: train_loss -0.9297 
2023-03-21 15:49:25.528202: val_loss -0.8351 
2023-03-21 15:49:25.528309: Pseudo dice [0.8937, 0.8764] 
2023-03-21 15:49:25.528432: Epoch time: 13.7 s 
2023-03-21 15:49:26.932827:  
2023-03-21 15:49:26.932980: Epoch 780 
2023-03-21 15:49:26.933128: Current learning rate: 0.00256 
2023-03-21 15:49:40.710088: train_loss -0.9297 
2023-03-21 15:49:40.710392: val_loss -0.8365 
2023-03-21 15:49:40.710520: Pseudo dice [0.8931, 0.8778] 
2023-03-21 15:49:40.710681: Epoch time: 13.78 s 
2023-03-21 15:49:42.095311:  
2023-03-21 15:49:42.095448: Epoch 781 
2023-03-21 15:49:42.095626: Current learning rate: 0.00255 
2023-03-21 15:49:55.766763: train_loss -0.9293 
2023-03-21 15:49:55.767038: val_loss -0.8366 
2023-03-21 15:49:55.767144: Pseudo dice [0.8937, 0.8767] 
2023-03-21 15:49:55.767251: Epoch time: 13.67 s 
2023-03-21 15:49:57.051768:  
2023-03-21 15:49:57.051897: Epoch 782 
2023-03-21 15:49:57.052045: Current learning rate: 0.00254 
2023-03-21 15:50:10.895785: train_loss -0.9299 
2023-03-21 15:50:10.897354: val_loss -0.8381 
2023-03-21 15:50:10.897489: Pseudo dice [0.8957, 0.8767] 
2023-03-21 15:50:10.897616: Epoch time: 13.84 s 
2023-03-21 15:50:12.172194:  
2023-03-21 15:50:12.172333: Epoch 783 
2023-03-21 15:50:12.172492: Current learning rate: 0.00253 
2023-03-21 15:50:25.697544: train_loss -0.9314 
2023-03-21 15:50:25.697747: val_loss -0.8289 
2023-03-21 15:50:25.697869: Pseudo dice [0.8905, 0.8722] 
2023-03-21 15:50:25.697969: Epoch time: 13.53 s 
2023-03-21 15:50:26.948922:  
2023-03-21 15:50:26.949048: Epoch 784 
2023-03-21 15:50:26.949198: Current learning rate: 0.00252 
2023-03-21 15:50:40.606810: train_loss -0.9309 
2023-03-21 15:50:40.607121: val_loss -0.8399 
2023-03-21 15:50:40.607226: Pseudo dice [0.8966, 0.8819] 
2023-03-21 15:50:40.607329: Epoch time: 13.66 s 
2023-03-21 15:50:42.063953:  
2023-03-21 15:50:42.064092: Epoch 785 
2023-03-21 15:50:42.064243: Current learning rate: 0.00251 
2023-03-21 15:50:56.220656: train_loss -0.9291 
2023-03-21 15:50:56.221083: val_loss -0.8373 
2023-03-21 15:50:56.221452: Pseudo dice [0.8939, 0.8766] 
2023-03-21 15:50:56.221610: Epoch time: 14.16 s 
2023-03-21 15:50:57.505985:  
2023-03-21 15:50:57.506144: Epoch 786 
2023-03-21 15:50:57.506295: Current learning rate: 0.0025 
2023-03-21 15:51:11.166337: train_loss -0.9296 
2023-03-21 15:51:11.166871: val_loss -0.8421 
2023-03-21 15:51:11.167103: Pseudo dice [0.8971, 0.8809] 
2023-03-21 15:51:11.167341: Epoch time: 13.66 s 
2023-03-21 15:51:12.433829:  
2023-03-21 15:51:12.433969: Epoch 787 
2023-03-21 15:51:12.434148: Current learning rate: 0.00249 
2023-03-21 15:51:26.184397: train_loss -0.9301 
2023-03-21 15:51:26.184592: val_loss -0.8363 
2023-03-21 15:51:26.184692: Pseudo dice [0.8938, 0.8763] 
2023-03-21 15:51:26.184786: Epoch time: 13.75 s 
2023-03-21 15:51:27.403014:  
2023-03-21 15:51:27.403180: Epoch 788 
2023-03-21 15:51:27.403342: Current learning rate: 0.00248 
2023-03-21 15:51:41.294666: train_loss -0.9289 
2023-03-21 15:51:41.294967: val_loss -0.8405 
2023-03-21 15:51:41.295099: Pseudo dice [0.8959, 0.881] 
2023-03-21 15:51:41.295196: Epoch time: 13.89 s 
2023-03-21 15:51:42.559619:  
2023-03-21 15:51:42.559746: Epoch 789 
2023-03-21 15:51:42.559878: Current learning rate: 0.00247 
2023-03-21 15:51:56.244650: train_loss -0.9311 
2023-03-21 15:51:56.244919: val_loss -0.839 
2023-03-21 15:51:56.245039: Pseudo dice [0.8942, 0.8782] 
2023-03-21 15:51:56.245143: Epoch time: 13.69 s 
2023-03-21 15:51:57.713348:  
2023-03-21 15:51:57.713484: Epoch 790 
2023-03-21 15:51:57.713617: Current learning rate: 0.00245 
2023-03-21 15:52:11.540054: train_loss -0.9297 
2023-03-21 15:52:11.540388: val_loss -0.8339 
2023-03-21 15:52:11.540528: Pseudo dice [0.8931, 0.877] 
2023-03-21 15:52:11.540663: Epoch time: 13.83 s 
2023-03-21 15:52:12.846402:  
2023-03-21 15:52:12.846550: Epoch 791 
2023-03-21 15:52:12.846694: Current learning rate: 0.00244 
2023-03-21 15:52:26.731974: train_loss -0.9308 
2023-03-21 15:52:26.732529: val_loss -0.8407 
2023-03-21 15:52:26.732697: Pseudo dice [0.8968, 0.8805] 
2023-03-21 15:52:26.732803: Epoch time: 13.89 s 
2023-03-21 15:52:27.983355:  
2023-03-21 15:52:27.983478: Epoch 792 
2023-03-21 15:52:27.983619: Current learning rate: 0.00243 
2023-03-21 15:52:41.773203: train_loss -0.9291 
2023-03-21 15:52:41.773488: val_loss -0.8349 
2023-03-21 15:52:41.773598: Pseudo dice [0.8939, 0.8773] 
2023-03-21 15:52:41.773708: Epoch time: 13.79 s 
2023-03-21 15:52:43.063136:  
2023-03-21 15:52:43.063268: Epoch 793 
2023-03-21 15:52:43.063420: Current learning rate: 0.00242 
2023-03-21 15:52:57.203148: train_loss -0.93 
2023-03-21 15:52:57.203409: val_loss -0.8407 
2023-03-21 15:52:57.203526: Pseudo dice [0.8973, 0.8807] 
2023-03-21 15:52:57.203630: Epoch time: 14.14 s 
2023-03-21 15:52:58.665779:  
2023-03-21 15:52:58.666029: Epoch 794 
2023-03-21 15:52:58.666311: Current learning rate: 0.00241 
2023-03-21 15:53:12.321815: train_loss -0.931 
2023-03-21 15:53:12.322071: val_loss -0.8391 
2023-03-21 15:53:12.322172: Pseudo dice [0.8941, 0.8789] 
2023-03-21 15:53:12.322288: Epoch time: 13.66 s 
2023-03-21 15:53:13.577591:  
2023-03-21 15:53:13.577721: Epoch 795 
2023-03-21 15:53:13.577863: Current learning rate: 0.0024 
2023-03-21 15:53:27.286295: train_loss -0.9319 
2023-03-21 15:53:27.286517: val_loss -0.8368 
2023-03-21 15:53:27.286626: Pseudo dice [0.8945, 0.8789] 
2023-03-21 15:53:27.286726: Epoch time: 13.71 s 
2023-03-21 15:53:28.571866:  
2023-03-21 15:53:28.572004: Epoch 796 
2023-03-21 15:53:28.572132: Current learning rate: 0.00239 
2023-03-21 15:53:42.282824: train_loss -0.9302 
2023-03-21 15:53:42.283776: val_loss -0.8344 
2023-03-21 15:53:42.283877: Pseudo dice [0.8935, 0.8765] 
2023-03-21 15:53:42.283979: Epoch time: 13.71 s 
2023-03-21 15:53:43.521621:  
2023-03-21 15:53:43.521746: Epoch 797 
2023-03-21 15:53:43.521889: Current learning rate: 0.00238 
2023-03-21 15:53:57.128681: train_loss -0.93 
2023-03-21 15:53:57.128880: val_loss -0.8378 
2023-03-21 15:53:57.128991: Pseudo dice [0.8955, 0.8768] 
2023-03-21 15:53:57.129083: Epoch time: 13.61 s 
2023-03-21 15:53:58.414110:  
2023-03-21 15:53:58.414262: Epoch 798 
2023-03-21 15:53:58.414548: Current learning rate: 0.00237 
2023-03-21 15:54:12.032709: train_loss -0.9321 
2023-03-21 15:54:12.033139: val_loss -0.8339 
2023-03-21 15:54:12.035827: Pseudo dice [0.8924, 0.8774] 
2023-03-21 15:54:12.035935: Epoch time: 13.62 s 
2023-03-21 15:54:13.440078:  
2023-03-21 15:54:13.440227: Epoch 799 
2023-03-21 15:54:13.440377: Current learning rate: 0.00236 
2023-03-21 15:54:27.502574: train_loss -0.9321 
2023-03-21 15:54:27.502856: val_loss -0.8382 
2023-03-21 15:54:27.502960: Pseudo dice [0.8945, 0.878] 
2023-03-21 15:54:27.503087: Epoch time: 14.06 s 
2023-03-21 15:54:29.029525:  
2023-03-21 15:54:29.029659: Epoch 800 
2023-03-21 15:54:29.029803: Current learning rate: 0.00235 
2023-03-21 15:54:42.577186: train_loss -0.9291 
2023-03-21 15:54:42.577438: val_loss -0.8346 
2023-03-21 15:54:42.577558: Pseudo dice [0.8931, 0.8766] 
2023-03-21 15:54:42.577662: Epoch time: 13.55 s 
2023-03-21 15:54:43.828008:  
2023-03-21 15:54:43.828132: Epoch 801 
2023-03-21 15:54:43.828274: Current learning rate: 0.00234 
2023-03-21 15:54:57.620684: train_loss -0.9329 
2023-03-21 15:54:57.620920: val_loss -0.8353 
2023-03-21 15:54:57.621020: Pseudo dice [0.8934, 0.8756] 
2023-03-21 15:54:57.621122: Epoch time: 13.79 s 
2023-03-21 15:54:58.882793:  
2023-03-21 15:54:58.882917: Epoch 802 
2023-03-21 15:54:58.883078: Current learning rate: 0.00233 
2023-03-21 15:55:12.498383: train_loss -0.9323 
2023-03-21 15:55:12.498585: val_loss -0.8354 
2023-03-21 15:55:12.498698: Pseudo dice [0.8939, 0.8792] 
2023-03-21 15:55:12.498829: Epoch time: 13.62 s 
2023-03-21 15:55:13.857056:  
2023-03-21 15:55:13.857183: Epoch 803 
2023-03-21 15:55:13.857340: Current learning rate: 0.00232 
2023-03-21 15:55:27.619605: train_loss -0.9321 
2023-03-21 15:55:27.619912: val_loss -0.8341 
2023-03-21 15:55:27.620046: Pseudo dice [0.8937, 0.8775] 
2023-03-21 15:55:27.620172: Epoch time: 13.76 s 
2023-03-21 15:55:28.930863:  
2023-03-21 15:55:28.930999: Epoch 804 
2023-03-21 15:55:28.931151: Current learning rate: 0.00231 
2023-03-21 15:55:42.761235: train_loss -0.9302 
2023-03-21 15:55:42.761427: val_loss -0.8352 
2023-03-21 15:55:42.761522: Pseudo dice [0.8925, 0.8777] 
2023-03-21 15:55:42.761620: Epoch time: 13.83 s 
2023-03-21 15:55:43.984531:  
2023-03-21 15:55:43.984782: Epoch 805 
2023-03-21 15:55:43.984941: Current learning rate: 0.0023 
2023-03-21 15:55:57.711962: train_loss -0.9316 
2023-03-21 15:55:57.712269: val_loss -0.8293 
2023-03-21 15:55:57.712410: Pseudo dice [0.8911, 0.8727] 
2023-03-21 15:55:57.712533: Epoch time: 13.73 s 
2023-03-21 15:55:59.091734:  
2023-03-21 15:55:59.091862: Epoch 806 
2023-03-21 15:55:59.092010: Current learning rate: 0.00229 
2023-03-21 15:56:12.775098: train_loss -0.9322 
2023-03-21 15:56:12.775480: val_loss -0.8428 
2023-03-21 15:56:12.775673: Pseudo dice [0.899, 0.8808] 
2023-03-21 15:56:12.775801: Epoch time: 13.68 s 
2023-03-21 15:56:14.029396:  
2023-03-21 15:56:14.029518: Epoch 807 
2023-03-21 15:56:14.029679: Current learning rate: 0.00228 
2023-03-21 15:56:27.740795: train_loss -0.9325 
2023-03-21 15:56:27.741026: val_loss -0.8368 
2023-03-21 15:56:27.741156: Pseudo dice [0.8941, 0.8765] 
2023-03-21 15:56:27.741288: Epoch time: 13.71 s 
2023-03-21 15:56:29.182598:  
2023-03-21 15:56:29.182838: Epoch 808 
2023-03-21 15:56:29.182973: Current learning rate: 0.00226 
2023-03-21 15:56:42.952487: train_loss -0.93 
2023-03-21 15:56:42.952735: val_loss -0.835 
2023-03-21 15:56:42.952835: Pseudo dice [0.8954, 0.8767] 
2023-03-21 15:56:42.952931: Epoch time: 13.77 s 
2023-03-21 15:56:44.218479:  
2023-03-21 15:56:44.218743: Epoch 809 
2023-03-21 15:56:44.218983: Current learning rate: 0.00225 
2023-03-21 15:56:57.983971: train_loss -0.9318 
2023-03-21 15:56:57.984453: val_loss -0.832 
2023-03-21 15:56:57.984672: Pseudo dice [0.893, 0.8749] 
2023-03-21 15:56:57.984881: Epoch time: 13.77 s 
2023-03-21 15:56:59.242905:  
2023-03-21 15:56:59.243049: Epoch 810 
2023-03-21 15:56:59.243209: Current learning rate: 0.00224 
2023-03-21 15:57:12.758619: train_loss -0.9325 
2023-03-21 15:57:12.758913: val_loss -0.8328 
2023-03-21 15:57:12.759018: Pseudo dice [0.893, 0.8753] 
2023-03-21 15:57:12.759119: Epoch time: 13.52 s 
2023-03-21 15:57:14.024781:  
2023-03-21 15:57:14.024909: Epoch 811 
2023-03-21 15:57:14.025044: Current learning rate: 0.00223 
2023-03-21 15:57:27.361564: train_loss -0.9321 
2023-03-21 15:57:27.361835: val_loss -0.8333 
2023-03-21 15:57:27.361954: Pseudo dice [0.8923, 0.8772] 
2023-03-21 15:57:27.362057: Epoch time: 13.34 s 
2023-03-21 15:57:28.647438:  
2023-03-21 15:57:28.647564: Epoch 812 
2023-03-21 15:57:28.647697: Current learning rate: 0.00222 
2023-03-21 15:57:42.556103: train_loss -0.9312 
2023-03-21 15:57:42.556344: val_loss -0.8368 
2023-03-21 15:57:42.556444: Pseudo dice [0.8954, 0.8785] 
2023-03-21 15:57:42.556556: Epoch time: 13.91 s 
2023-03-21 15:57:44.075235:  
2023-03-21 15:57:44.075394: Epoch 813 
2023-03-21 15:57:44.075552: Current learning rate: 0.00221 
2023-03-21 15:57:57.894421: train_loss -0.9309 
2023-03-21 15:57:57.894796: val_loss -0.8376 
2023-03-21 15:57:57.894946: Pseudo dice [0.896, 0.8792] 
2023-03-21 15:57:57.895157: Epoch time: 13.82 s 
2023-03-21 15:57:59.158928:  
2023-03-21 15:57:59.159069: Epoch 814 
2023-03-21 15:57:59.159214: Current learning rate: 0.0022 
2023-03-21 15:58:12.813788: train_loss -0.9302 
2023-03-21 15:58:12.814199: val_loss -0.8329 
2023-03-21 15:58:12.814397: Pseudo dice [0.8939, 0.8744] 
2023-03-21 15:58:12.814756: Epoch time: 13.66 s 
2023-03-21 15:58:14.091177:  
2023-03-21 15:58:14.091320: Epoch 815 
2023-03-21 15:58:14.091483: Current learning rate: 0.00219 
2023-03-21 15:58:28.442127: train_loss -0.9314 
2023-03-21 15:58:28.442604: val_loss -0.8376 
2023-03-21 15:58:28.442915: Pseudo dice [0.8953, 0.8801] 
2023-03-21 15:58:28.443133: Epoch time: 14.35 s 
2023-03-21 15:58:29.753786:  
2023-03-21 15:58:29.753914: Epoch 816 
2023-03-21 15:58:29.754062: Current learning rate: 0.00218 
2023-03-21 15:58:43.971147: train_loss -0.9325 
2023-03-21 15:58:43.971415: val_loss -0.8344 
2023-03-21 15:58:43.971529: Pseudo dice [0.8923, 0.877] 
2023-03-21 15:58:43.971634: Epoch time: 14.22 s 
2023-03-21 15:58:45.357246:  
2023-03-21 15:58:45.357396: Epoch 817 
2023-03-21 15:58:45.357544: Current learning rate: 0.00217 
2023-03-21 15:58:58.994022: train_loss -0.932 
2023-03-21 15:58:58.994303: val_loss -0.8317 
2023-03-21 15:58:58.994431: Pseudo dice [0.8919, 0.876] 
2023-03-21 15:58:58.994551: Epoch time: 13.64 s 
2023-03-21 15:59:00.266352:  
2023-03-21 15:59:00.266495: Epoch 818 
2023-03-21 15:59:00.266656: Current learning rate: 0.00216 
2023-03-21 15:59:13.653962: train_loss -0.9333 
2023-03-21 15:59:13.654233: val_loss -0.8378 
2023-03-21 15:59:13.654357: Pseudo dice [0.8956, 0.8812] 
2023-03-21 15:59:13.654469: Epoch time: 13.39 s 
2023-03-21 15:59:14.977939:  
2023-03-21 15:59:14.978063: Epoch 819 
2023-03-21 15:59:14.978208: Current learning rate: 0.00215 
2023-03-21 15:59:28.247017: train_loss -0.9321 
2023-03-21 15:59:28.247208: val_loss -0.8396 
2023-03-21 15:59:28.247325: Pseudo dice [0.8952, 0.8791] 
2023-03-21 15:59:28.247422: Epoch time: 13.27 s 
2023-03-21 15:59:29.411836:  
2023-03-21 15:59:29.412007: Epoch 820 
2023-03-21 15:59:29.412142: Current learning rate: 0.00214 
2023-03-21 15:59:43.163907: train_loss -0.9326 
2023-03-21 15:59:43.164220: val_loss -0.8396 
2023-03-21 15:59:43.164348: Pseudo dice [0.8956, 0.8813] 
2023-03-21 15:59:43.164470: Epoch time: 13.75 s 
2023-03-21 15:59:44.391181:  
2023-03-21 15:59:44.391303: Epoch 821 
2023-03-21 15:59:44.391448: Current learning rate: 0.00213 
2023-03-21 15:59:58.397334: train_loss -0.9316 
2023-03-21 15:59:58.397632: val_loss -0.8354 
2023-03-21 15:59:58.397811: Pseudo dice [0.8933, 0.8783] 
2023-03-21 15:59:58.397999: Epoch time: 14.01 s 
2023-03-21 15:59:59.815061:  
2023-03-21 15:59:59.815197: Epoch 822 
2023-03-21 15:59:59.815369: Current learning rate: 0.00212 
2023-03-21 16:00:13.532532: train_loss -0.9319 
2023-03-21 16:00:13.532795: val_loss -0.8338 
2023-03-21 16:00:13.532907: Pseudo dice [0.8937, 0.8763] 
2023-03-21 16:00:13.533013: Epoch time: 13.72 s 
2023-03-21 16:00:14.717689:  
2023-03-21 16:00:14.717818: Epoch 823 
2023-03-21 16:00:14.717966: Current learning rate: 0.0021 
2023-03-21 16:00:28.290435: train_loss -0.9315 
2023-03-21 16:00:28.290900: val_loss -0.8386 
2023-03-21 16:00:28.291010: Pseudo dice [0.8957, 0.8786] 
2023-03-21 16:00:28.291143: Epoch time: 13.57 s 
2023-03-21 16:00:29.436273:  
2023-03-21 16:00:29.436400: Epoch 824 
2023-03-21 16:00:29.436545: Current learning rate: 0.00209 
2023-03-21 16:00:42.879769: train_loss -0.9319 
2023-03-21 16:00:42.879958: val_loss -0.8358 
2023-03-21 16:00:42.880059: Pseudo dice [0.8955, 0.8777] 
2023-03-21 16:00:42.880156: Epoch time: 13.44 s 
2023-03-21 16:00:44.051098:  
2023-03-21 16:00:44.051314: Epoch 825 
2023-03-21 16:00:44.051587: Current learning rate: 0.00208 
2023-03-21 16:00:57.523444: train_loss -0.933 
2023-03-21 16:00:57.524555: val_loss -0.8374 
2023-03-21 16:00:57.524668: Pseudo dice [0.8939, 0.8781] 
2023-03-21 16:00:57.524764: Epoch time: 13.47 s 
2023-03-21 16:00:58.725738:  
2023-03-21 16:00:58.725885: Epoch 826 
2023-03-21 16:00:58.726039: Current learning rate: 0.00207 
2023-03-21 16:01:12.567068: train_loss -0.9323 
2023-03-21 16:01:12.567502: val_loss -0.8339 
2023-03-21 16:01:12.567714: Pseudo dice [0.8922, 0.8767] 
2023-03-21 16:01:12.568141: Epoch time: 13.84 s 
2023-03-21 16:01:13.952709:  
2023-03-21 16:01:13.952969: Epoch 827 
2023-03-21 16:01:13.953145: Current learning rate: 0.00206 
2023-03-21 16:01:28.058485: train_loss -0.9327 
2023-03-21 16:01:28.059060: val_loss -0.8364 
2023-03-21 16:01:28.059212: Pseudo dice [0.8953, 0.8796] 
2023-03-21 16:01:28.059332: Epoch time: 14.11 s 
2023-03-21 16:01:29.273129:  
2023-03-21 16:01:29.273265: Epoch 828 
2023-03-21 16:01:29.273417: Current learning rate: 0.00205 
2023-03-21 16:01:43.245405: train_loss -0.9333 
2023-03-21 16:01:43.245780: val_loss -0.8359 
2023-03-21 16:01:43.246170: Pseudo dice [0.8947, 0.8787] 
2023-03-21 16:01:43.246376: Epoch time: 13.97 s 
2023-03-21 16:01:44.462015:  
2023-03-21 16:01:44.462152: Epoch 829 
2023-03-21 16:01:44.462303: Current learning rate: 0.00204 
2023-03-21 16:01:58.219890: train_loss -0.9325 
2023-03-21 16:01:58.220222: val_loss -0.8361 
2023-03-21 16:01:58.220453: Pseudo dice [0.8951, 0.8785] 
2023-03-21 16:01:58.220772: Epoch time: 13.76 s 
2023-03-21 16:01:59.395946:  
2023-03-21 16:01:59.396091: Epoch 830 
2023-03-21 16:01:59.396239: Current learning rate: 0.00203 
2023-03-21 16:02:12.668469: train_loss -0.9314 
2023-03-21 16:02:12.668713: val_loss -0.8331 
2023-03-21 16:02:12.669039: Pseudo dice [0.8919, 0.8765] 
2023-03-21 16:02:12.669142: Epoch time: 13.27 s 
2023-03-21 16:02:13.850819:  
2023-03-21 16:02:13.850957: Epoch 831 
2023-03-21 16:02:13.851103: Current learning rate: 0.00202 
2023-03-21 16:02:27.692232: train_loss -0.9317 
2023-03-21 16:02:27.692734: val_loss -0.8359 
2023-03-21 16:02:27.692874: Pseudo dice [0.895, 0.8764] 
2023-03-21 16:02:27.693152: Epoch time: 13.84 s 
2023-03-21 16:02:29.084189:  
2023-03-21 16:02:29.084501: Epoch 832 
2023-03-21 16:02:29.084677: Current learning rate: 0.00201 
2023-03-21 16:02:42.872586: train_loss -0.9338 
2023-03-21 16:02:42.872899: val_loss -0.8345 
2023-03-21 16:02:42.873113: Pseudo dice [0.8931, 0.8769] 
2023-03-21 16:02:42.873347: Epoch time: 13.79 s 
2023-03-21 16:02:44.095665:  
2023-03-21 16:02:44.095821: Epoch 833 
2023-03-21 16:02:44.095963: Current learning rate: 0.002 
2023-03-21 16:02:57.829461: train_loss -0.9315 
2023-03-21 16:02:57.829710: val_loss -0.8309 
2023-03-21 16:02:57.829825: Pseudo dice [0.8903, 0.8749] 
2023-03-21 16:02:57.829939: Epoch time: 13.73 s 
2023-03-21 16:02:59.026648:  
2023-03-21 16:02:59.026814: Epoch 834 
2023-03-21 16:02:59.026963: Current learning rate: 0.00199 
2023-03-21 16:03:12.530642: train_loss -0.9321 
2023-03-21 16:03:12.531104: val_loss -0.8323 
2023-03-21 16:03:12.531463: Pseudo dice [0.892, 0.8757] 
2023-03-21 16:03:12.531645: Epoch time: 13.5 s 
2023-03-21 16:03:13.714978:  
2023-03-21 16:03:13.715104: Epoch 835 
2023-03-21 16:03:13.715248: Current learning rate: 0.00198 
2023-03-21 16:03:27.582282: train_loss -0.9332 
2023-03-21 16:03:27.582521: val_loss -0.8337 
2023-03-21 16:03:27.582620: Pseudo dice [0.8923, 0.8767] 
2023-03-21 16:03:27.582784: Epoch time: 13.87 s 
2023-03-21 16:03:28.768562:  
2023-03-21 16:03:28.768687: Epoch 836 
2023-03-21 16:03:28.768825: Current learning rate: 0.00196 
2023-03-21 16:03:42.525460: train_loss -0.9328 
2023-03-21 16:03:42.525757: val_loss -0.834 
2023-03-21 16:03:42.525908: Pseudo dice [0.8937, 0.876] 
2023-03-21 16:03:42.526043: Epoch time: 13.76 s 
2023-03-21 16:03:43.894243:  
2023-03-21 16:03:43.894411: Epoch 837 
2023-03-21 16:03:43.894561: Current learning rate: 0.00195 
2023-03-21 16:03:57.711473: train_loss -0.9314 
2023-03-21 16:03:57.711776: val_loss -0.8399 
2023-03-21 16:03:57.711910: Pseudo dice [0.8967, 0.88] 
2023-03-21 16:03:57.712039: Epoch time: 13.82 s 
2023-03-21 16:03:58.932276:  
2023-03-21 16:03:58.932412: Epoch 838 
2023-03-21 16:03:58.932550: Current learning rate: 0.00194 
2023-03-21 16:04:12.924129: train_loss -0.9335 
2023-03-21 16:04:12.924383: val_loss -0.8366 
2023-03-21 16:04:12.924501: Pseudo dice [0.8932, 0.8791] 
2023-03-21 16:04:12.924602: Epoch time: 13.99 s 
2023-03-21 16:04:14.156940:  
2023-03-21 16:04:14.157070: Epoch 839 
2023-03-21 16:04:14.157218: Current learning rate: 0.00193 
2023-03-21 16:04:27.589667: train_loss -0.9344 
2023-03-21 16:04:27.589909: val_loss -0.8393 
2023-03-21 16:04:27.590005: Pseudo dice [0.8973, 0.8798] 
2023-03-21 16:04:27.590101: Epoch time: 13.43 s 
2023-03-21 16:04:28.880036:  
2023-03-21 16:04:28.880162: Epoch 840 
2023-03-21 16:04:28.880302: Current learning rate: 0.00192 
2023-03-21 16:04:42.857462: train_loss -0.9335 
2023-03-21 16:04:42.857708: val_loss -0.8365 
2023-03-21 16:04:42.857823: Pseudo dice [0.8949, 0.8783] 
2023-03-21 16:04:42.857922: Epoch time: 13.98 s 
2023-03-21 16:04:44.132522:  
2023-03-21 16:04:44.132647: Epoch 841 
2023-03-21 16:04:44.132790: Current learning rate: 0.00191 
2023-03-21 16:04:58.121166: train_loss -0.9334 
2023-03-21 16:04:58.121611: val_loss -0.8377 
2023-03-21 16:04:58.121812: Pseudo dice [0.8959, 0.8804] 
2023-03-21 16:04:58.122016: Epoch time: 13.99 s 
2023-03-21 16:04:59.490345:  
2023-03-21 16:04:59.490483: Epoch 842 
2023-03-21 16:04:59.490634: Current learning rate: 0.0019 
2023-03-21 16:05:13.112788: train_loss -0.9324 
2023-03-21 16:05:13.113297: val_loss -0.8345 
2023-03-21 16:05:13.113601: Pseudo dice [0.8936, 0.8761] 
2023-03-21 16:05:13.113815: Epoch time: 13.62 s 
2023-03-21 16:05:14.350393:  
2023-03-21 16:05:14.350524: Epoch 843 
2023-03-21 16:05:14.350666: Current learning rate: 0.00189 
2023-03-21 16:05:28.269915: train_loss -0.9308 
2023-03-21 16:05:28.270357: val_loss -0.8438 
2023-03-21 16:05:28.270652: Pseudo dice [0.8987, 0.8814] 
2023-03-21 16:05:28.271005: Epoch time: 13.92 s 
2023-03-21 16:05:29.474334:  
2023-03-21 16:05:29.474464: Epoch 844 
2023-03-21 16:05:29.474657: Current learning rate: 0.00188 
2023-03-21 16:05:43.543628: train_loss -0.9332 
2023-03-21 16:05:43.543887: val_loss -0.8391 
2023-03-21 16:05:43.544029: Pseudo dice [0.895, 0.8799] 
2023-03-21 16:05:43.544152: Epoch time: 14.07 s 
2023-03-21 16:05:44.746960:  
2023-03-21 16:05:44.747095: Epoch 845 
2023-03-21 16:05:44.747244: Current learning rate: 0.00187 
2023-03-21 16:05:58.355541: train_loss -0.9333 
2023-03-21 16:05:58.355928: val_loss -0.8374 
2023-03-21 16:05:58.356293: Pseudo dice [0.8948, 0.8769] 
2023-03-21 16:05:58.356513: Epoch time: 13.61 s 
2023-03-21 16:05:59.564280:  
2023-03-21 16:05:59.564398: Epoch 846 
2023-03-21 16:05:59.564541: Current learning rate: 0.00186 
2023-03-21 16:06:13.115708: train_loss -0.933 
2023-03-21 16:06:13.115952: val_loss -0.8315 
2023-03-21 16:06:13.116064: Pseudo dice [0.8915, 0.8733] 
2023-03-21 16:06:13.116161: Epoch time: 13.55 s 
2023-03-21 16:06:14.435210:  
2023-03-21 16:06:14.435347: Epoch 847 
2023-03-21 16:06:14.435491: Current learning rate: 0.00185 
2023-03-21 16:06:27.795217: train_loss -0.9326 
2023-03-21 16:06:27.795470: val_loss -0.8366 
2023-03-21 16:06:27.795583: Pseudo dice [0.8948, 0.8787] 
2023-03-21 16:06:27.795680: Epoch time: 13.36 s 
2023-03-21 16:06:28.960795:  
2023-03-21 16:06:28.960929: Epoch 848 
2023-03-21 16:06:28.961088: Current learning rate: 0.00184 
2023-03-21 16:06:42.755599: train_loss -0.9338 
2023-03-21 16:06:42.755944: val_loss -0.8326 
2023-03-21 16:06:42.756097: Pseudo dice [0.8925, 0.8755] 
2023-03-21 16:06:42.756369: Epoch time: 13.8 s 
2023-03-21 16:06:44.032949:  
2023-03-21 16:06:44.033090: Epoch 849 
2023-03-21 16:06:44.033235: Current learning rate: 0.00182 
2023-03-21 16:06:57.485735: train_loss -0.9336 
2023-03-21 16:06:57.486076: val_loss -0.8368 
2023-03-21 16:06:57.486206: Pseudo dice [0.8948, 0.88] 
2023-03-21 16:06:57.486332: Epoch time: 13.45 s 
2023-03-21 16:06:58.959351:  
2023-03-21 16:06:58.959504: Epoch 850 
2023-03-21 16:06:58.959650: Current learning rate: 0.00181 
2023-03-21 16:07:12.776819: train_loss -0.9342 
2023-03-21 16:07:12.777036: val_loss -0.8367 
2023-03-21 16:07:12.777169: Pseudo dice [0.8956, 0.8776] 
2023-03-21 16:07:12.777293: Epoch time: 13.82 s 
2023-03-21 16:07:13.947928:  
2023-03-21 16:07:13.948067: Epoch 851 
2023-03-21 16:07:13.948231: Current learning rate: 0.0018 
2023-03-21 16:07:27.711976: train_loss -0.9335 
2023-03-21 16:07:27.712217: val_loss -0.8371 
2023-03-21 16:07:27.712347: Pseudo dice [0.8954, 0.8786] 
2023-03-21 16:07:27.712448: Epoch time: 13.76 s 
2023-03-21 16:07:29.079009:  
2023-03-21 16:07:29.079180: Epoch 852 
2023-03-21 16:07:29.079349: Current learning rate: 0.00179 
2023-03-21 16:07:42.450463: train_loss -0.9328 
2023-03-21 16:07:42.450668: val_loss -0.8335 
2023-03-21 16:07:42.450818: Pseudo dice [0.8936, 0.8772] 
2023-03-21 16:07:42.450949: Epoch time: 13.37 s 
2023-03-21 16:07:43.619498:  
2023-03-21 16:07:43.619624: Epoch 853 
2023-03-21 16:07:43.619765: Current learning rate: 0.00178 
2023-03-21 16:07:56.812686: train_loss -0.933 
2023-03-21 16:07:56.812933: val_loss -0.8394 
2023-03-21 16:07:56.813069: Pseudo dice [0.897, 0.8799] 
2023-03-21 16:07:56.813189: Epoch time: 13.19 s 
2023-03-21 16:07:58.005622:  
2023-03-21 16:07:58.005766: Epoch 854 
2023-03-21 16:07:58.005922: Current learning rate: 0.00177 
2023-03-21 16:08:12.244944: train_loss -0.9324 
2023-03-21 16:08:12.245132: val_loss -0.8418 
2023-03-21 16:08:12.245243: Pseudo dice [0.8985, 0.8818] 
2023-03-21 16:08:12.245336: Epoch time: 14.24 s 
2023-03-21 16:08:13.425753:  
2023-03-21 16:08:13.425883: Epoch 855 
2023-03-21 16:08:13.426034: Current learning rate: 0.00176 
2023-03-21 16:08:27.292205: train_loss -0.9331 
2023-03-21 16:08:27.292475: val_loss -0.8381 
2023-03-21 16:08:27.292587: Pseudo dice [0.896, 0.8785] 
2023-03-21 16:08:27.292692: Epoch time: 13.87 s 
2023-03-21 16:08:28.517812:  
2023-03-21 16:08:28.517936: Epoch 856 
2023-03-21 16:08:28.518093: Current learning rate: 0.00175 
2023-03-21 16:08:42.223793: train_loss -0.9342 
2023-03-21 16:08:42.224048: val_loss -0.836 
2023-03-21 16:08:42.224191: Pseudo dice [0.8945, 0.879] 
2023-03-21 16:08:42.224361: Epoch time: 13.71 s 
2023-03-21 16:08:43.795697:  
2023-03-21 16:08:43.795948: Epoch 857 
2023-03-21 16:08:43.796178: Current learning rate: 0.00174 
2023-03-21 16:08:57.433904: train_loss -0.9334 
2023-03-21 16:08:57.434161: val_loss -0.8314 
2023-03-21 16:08:57.434264: Pseudo dice [0.891, 0.8739] 
2023-03-21 16:08:57.434381: Epoch time: 13.64 s 
2023-03-21 16:08:58.606760:  
2023-03-21 16:08:58.606908: Epoch 858 
2023-03-21 16:08:58.607071: Current learning rate: 0.00173 
2023-03-21 16:09:12.260144: train_loss -0.9324 
2023-03-21 16:09:12.260347: val_loss -0.8395 
2023-03-21 16:09:12.260461: Pseudo dice [0.8962, 0.8801] 
2023-03-21 16:09:12.260556: Epoch time: 13.65 s 
2023-03-21 16:09:13.448057:  
2023-03-21 16:09:13.448207: Epoch 859 
2023-03-21 16:09:13.448382: Current learning rate: 0.00172 
2023-03-21 16:09:27.190453: train_loss -0.9337 
2023-03-21 16:09:27.190849: val_loss -0.8363 
2023-03-21 16:09:27.191088: Pseudo dice [0.8954, 0.8771] 
2023-03-21 16:09:27.191238: Epoch time: 13.74 s 
2023-03-21 16:09:28.410330:  
2023-03-21 16:09:28.410471: Epoch 860 
2023-03-21 16:09:28.410701: Current learning rate: 0.0017 
2023-03-21 16:09:42.125696: train_loss -0.934 
2023-03-21 16:09:42.125976: val_loss -0.8373 
2023-03-21 16:09:42.126081: Pseudo dice [0.8954, 0.8785] 
2023-03-21 16:09:42.126183: Epoch time: 13.72 s 
2023-03-21 16:09:43.304416:  
2023-03-21 16:09:43.304553: Epoch 861 
2023-03-21 16:09:43.304716: Current learning rate: 0.00169 
2023-03-21 16:09:56.951225: train_loss -0.9338 
2023-03-21 16:09:56.951530: val_loss -0.835 
2023-03-21 16:09:56.951656: Pseudo dice [0.8944, 0.8762] 
2023-03-21 16:09:56.951781: Epoch time: 13.65 s 
2023-03-21 16:09:58.301658:  
2023-03-21 16:09:58.301790: Epoch 862 
2023-03-21 16:09:58.301950: Current learning rate: 0.00168 
2023-03-21 16:10:11.663346: train_loss -0.935 
2023-03-21 16:10:11.663625: val_loss -0.8391 
2023-03-21 16:10:11.663746: Pseudo dice [0.8967, 0.8797] 
2023-03-21 16:10:11.663850: Epoch time: 13.36 s 
2023-03-21 16:10:12.966590:  
2023-03-21 16:10:12.966758: Epoch 863 
2023-03-21 16:10:12.966906: Current learning rate: 0.00167 
2023-03-21 16:10:26.424699: train_loss -0.9336 
2023-03-21 16:10:26.425115: val_loss -0.8333 
2023-03-21 16:10:26.425480: Pseudo dice [0.8933, 0.8757] 
2023-03-21 16:10:26.425720: Epoch time: 13.46 s 
2023-03-21 16:10:27.595274:  
2023-03-21 16:10:27.595407: Epoch 864 
2023-03-21 16:10:27.595563: Current learning rate: 0.00166 
2023-03-21 16:10:41.128952: train_loss -0.9342 
2023-03-21 16:10:41.129923: val_loss -0.831 
2023-03-21 16:10:41.130053: Pseudo dice [0.8928, 0.875] 
2023-03-21 16:10:41.130163: Epoch time: 13.53 s 
2023-03-21 16:10:42.386450:  
2023-03-21 16:10:42.386595: Epoch 865 
2023-03-21 16:10:42.386738: Current learning rate: 0.00165 
2023-03-21 16:10:56.412902: train_loss -0.9346 
2023-03-21 16:10:56.413094: val_loss -0.8367 
2023-03-21 16:10:56.413191: Pseudo dice [0.8951, 0.8781] 
2023-03-21 16:10:56.413285: Epoch time: 14.03 s 
2023-03-21 16:10:57.573598:  
2023-03-21 16:10:57.573722: Epoch 866 
2023-03-21 16:10:57.573866: Current learning rate: 0.00164 
2023-03-21 16:11:11.751794: train_loss -0.9349 
2023-03-21 16:11:11.752050: val_loss -0.8364 
2023-03-21 16:11:11.752150: Pseudo dice [0.8942, 0.8768] 
2023-03-21 16:11:11.752244: Epoch time: 14.18 s 
2023-03-21 16:11:13.114474:  
2023-03-21 16:11:13.114620: Epoch 867 
2023-03-21 16:11:13.114782: Current learning rate: 0.00163 
2023-03-21 16:11:26.669927: train_loss -0.9351 
2023-03-21 16:11:26.670243: val_loss -0.831 
2023-03-21 16:11:26.670373: Pseudo dice [0.892, 0.8751] 
2023-03-21 16:11:26.670495: Epoch time: 13.56 s 
2023-03-21 16:11:27.870200:  
2023-03-21 16:11:27.870340: Epoch 868 
2023-03-21 16:11:27.870509: Current learning rate: 0.00162 
2023-03-21 16:11:41.655591: train_loss -0.935 
2023-03-21 16:11:41.655832: val_loss -0.835 
2023-03-21 16:11:41.655931: Pseudo dice [0.8931, 0.877] 
2023-03-21 16:11:41.656029: Epoch time: 13.79 s 
2023-03-21 16:11:42.940439:  
2023-03-21 16:11:42.940570: Epoch 869 
2023-03-21 16:11:42.940713: Current learning rate: 0.00161 
2023-03-21 16:11:56.585236: train_loss -0.9327 
2023-03-21 16:11:56.585472: val_loss -0.8351 
2023-03-21 16:11:56.585585: Pseudo dice [0.8944, 0.8776] 
2023-03-21 16:11:56.585681: Epoch time: 13.65 s 
2023-03-21 16:11:57.880722:  
2023-03-21 16:11:57.880850: Epoch 870 
2023-03-21 16:11:57.880996: Current learning rate: 0.00159 
2023-03-21 16:12:11.537655: train_loss -0.9332 
2023-03-21 16:12:11.538057: val_loss -0.8325 
2023-03-21 16:12:11.538269: Pseudo dice [0.8942, 0.8765] 
2023-03-21 16:12:11.538461: Epoch time: 13.66 s 
2023-03-21 16:12:12.703675:  
2023-03-21 16:12:12.703797: Epoch 871 
2023-03-21 16:12:12.703924: Current learning rate: 0.00158 
2023-03-21 16:12:26.364088: train_loss -0.9339 
2023-03-21 16:12:26.364332: val_loss -0.8351 
2023-03-21 16:12:26.364431: Pseudo dice [0.8949, 0.8773] 
2023-03-21 16:12:26.364883: Epoch time: 13.66 s 
2023-03-21 16:12:27.589091:  
2023-03-21 16:12:27.589393: Epoch 872 
2023-03-21 16:12:27.589767: Current learning rate: 0.00157 
2023-03-21 16:12:41.196389: train_loss -0.934 
2023-03-21 16:12:41.196625: val_loss -0.8325 
2023-03-21 16:12:41.196737: Pseudo dice [0.8926, 0.8753] 
2023-03-21 16:12:41.196833: Epoch time: 13.61 s 
2023-03-21 16:12:42.551143:  
2023-03-21 16:12:42.551283: Epoch 873 
2023-03-21 16:12:42.551425: Current learning rate: 0.00156 
2023-03-21 16:12:56.178082: train_loss -0.9355 
2023-03-21 16:12:56.178367: val_loss -0.8315 
2023-03-21 16:12:56.178473: Pseudo dice [0.893, 0.8768] 
2023-03-21 16:12:56.178578: Epoch time: 13.63 s 
2023-03-21 16:12:57.373719:  
2023-03-21 16:12:57.373988: Epoch 874 
2023-03-21 16:12:57.374259: Current learning rate: 0.00155 
2023-03-21 16:13:10.822247: train_loss -0.9339 
2023-03-21 16:13:10.822512: val_loss -0.8389 
2023-03-21 16:13:10.822635: Pseudo dice [0.8958, 0.8792] 
2023-03-21 16:13:10.822780: Epoch time: 13.45 s 
2023-03-21 16:13:12.109124:  
2023-03-21 16:13:12.109262: Epoch 875 
2023-03-21 16:13:12.109414: Current learning rate: 0.00154 
2023-03-21 16:13:26.074045: train_loss -0.9346 
2023-03-21 16:13:26.074453: val_loss -0.8299 
2023-03-21 16:13:26.074677: Pseudo dice [0.8918, 0.8735] 
2023-03-21 16:13:26.074940: Epoch time: 13.97 s 
2023-03-21 16:13:27.267717:  
2023-03-21 16:13:27.267921: Epoch 876 
2023-03-21 16:13:27.268109: Current learning rate: 0.00153 
2023-03-21 16:13:41.264420: train_loss -0.9346 
2023-03-21 16:13:41.264725: val_loss -0.8386 
2023-03-21 16:13:41.264869: Pseudo dice [0.8957, 0.8794] 
2023-03-21 16:13:41.264993: Epoch time: 14.0 s 
2023-03-21 16:13:42.466864:  
2023-03-21 16:13:42.466987: Epoch 877 
2023-03-21 16:13:42.467130: Current learning rate: 0.00152 
2023-03-21 16:13:56.151703: train_loss -0.9354 
2023-03-21 16:13:56.152132: val_loss -0.8356 
2023-03-21 16:13:56.152427: Pseudo dice [0.8945, 0.8777] 
2023-03-21 16:13:56.152819: Epoch time: 13.69 s 
2023-03-21 16:13:57.474042:  
2023-03-21 16:13:57.474179: Epoch 878 
2023-03-21 16:13:57.474324: Current learning rate: 0.00151 
2023-03-21 16:14:11.490814: train_loss -0.9359 
2023-03-21 16:14:11.491111: val_loss -0.8373 
2023-03-21 16:14:11.491212: Pseudo dice [0.8941, 0.8787] 
2023-03-21 16:14:11.491309: Epoch time: 14.02 s 
2023-03-21 16:14:12.676837:  
2023-03-21 16:14:12.676976: Epoch 879 
2023-03-21 16:14:12.677138: Current learning rate: 0.00149 
2023-03-21 16:14:26.426833: train_loss -0.9346 
2023-03-21 16:14:26.427076: val_loss -0.8378 
2023-03-21 16:14:26.427173: Pseudo dice [0.8948, 0.879] 
2023-03-21 16:14:26.427268: Epoch time: 13.75 s 
2023-03-21 16:14:27.571298:  
2023-03-21 16:14:27.571441: Epoch 880 
2023-03-21 16:14:27.571586: Current learning rate: 0.00148 
2023-03-21 16:14:41.023021: train_loss -0.9354 
2023-03-21 16:14:41.023309: val_loss -0.8378 
2023-03-21 16:14:41.023413: Pseudo dice [0.896, 0.8777] 
2023-03-21 16:14:41.023517: Epoch time: 13.45 s 
2023-03-21 16:14:42.219703:  
2023-03-21 16:14:42.219828: Epoch 881 
2023-03-21 16:14:42.219976: Current learning rate: 0.00147 
2023-03-21 16:14:56.217070: train_loss -0.9352 
2023-03-21 16:14:56.217396: val_loss -0.8367 
2023-03-21 16:14:56.217509: Pseudo dice [0.8956, 0.8778] 
2023-03-21 16:14:56.217640: Epoch time: 14.0 s 
2023-03-21 16:14:57.391073:  
2023-03-21 16:14:57.391198: Epoch 882 
2023-03-21 16:14:57.391357: Current learning rate: 0.00146 
2023-03-21 16:15:10.969347: train_loss -0.9353 
2023-03-21 16:15:10.969590: val_loss -0.8332 
2023-03-21 16:15:10.969704: Pseudo dice [0.8942, 0.8765] 
2023-03-21 16:15:10.969802: Epoch time: 13.58 s 
2023-03-21 16:15:12.288513:  
2023-03-21 16:15:12.288651: Epoch 883 
2023-03-21 16:15:12.288797: Current learning rate: 0.00145 
2023-03-21 16:15:26.092057: train_loss -0.9354 
2023-03-21 16:15:26.092340: val_loss -0.8345 
2023-03-21 16:15:26.092470: Pseudo dice [0.8932, 0.8771] 
2023-03-21 16:15:26.092584: Epoch time: 13.8 s 
2023-03-21 16:15:27.319117:  
2023-03-21 16:15:27.319261: Epoch 884 
2023-03-21 16:15:27.319414: Current learning rate: 0.00144 
2023-03-21 16:15:40.928116: train_loss -0.9358 
2023-03-21 16:15:40.928708: val_loss -0.8335 
2023-03-21 16:15:40.929026: Pseudo dice [0.8939, 0.8764] 
2023-03-21 16:15:40.929136: Epoch time: 13.61 s 
2023-03-21 16:15:42.172440:  
2023-03-21 16:15:42.172571: Epoch 885 
2023-03-21 16:15:42.172718: Current learning rate: 0.00143 
2023-03-21 16:15:56.095385: train_loss -0.9365 
2023-03-21 16:15:56.095742: val_loss -0.8371 
2023-03-21 16:15:56.095976: Pseudo dice [0.8956, 0.8792] 
2023-03-21 16:15:56.096113: Epoch time: 13.92 s 
2023-03-21 16:15:57.269457:  
2023-03-21 16:15:57.269581: Epoch 886 
2023-03-21 16:15:57.269735: Current learning rate: 0.00142 
2023-03-21 16:16:10.672993: train_loss -0.9362 
2023-03-21 16:16:10.673250: val_loss -0.8342 
2023-03-21 16:16:10.673364: Pseudo dice [0.8937, 0.8768] 
2023-03-21 16:16:10.673460: Epoch time: 13.4 s 
2023-03-21 16:16:11.826216:  
2023-03-21 16:16:11.826340: Epoch 887 
2023-03-21 16:16:11.826495: Current learning rate: 0.00141 
2023-03-21 16:16:25.355436: train_loss -0.9348 
2023-03-21 16:16:25.355881: val_loss -0.8354 
2023-03-21 16:16:25.356175: Pseudo dice [0.8952, 0.8781] 
2023-03-21 16:16:25.356351: Epoch time: 13.53 s 
2023-03-21 16:16:26.723823:  
2023-03-21 16:16:26.723986: Epoch 888 
2023-03-21 16:16:26.724155: Current learning rate: 0.00139 
2023-03-21 16:16:40.155556: train_loss -0.9351 
2023-03-21 16:16:40.155808: val_loss -0.8341 
2023-03-21 16:16:40.155913: Pseudo dice [0.8946, 0.8763] 
2023-03-21 16:16:40.156017: Epoch time: 13.43 s 
2023-03-21 16:16:41.354349:  
2023-03-21 16:16:41.354502: Epoch 889 
2023-03-21 16:16:41.354669: Current learning rate: 0.00138 
2023-03-21 16:16:54.884799: train_loss -0.9356 
2023-03-21 16:16:54.885041: val_loss -0.8355 
2023-03-21 16:16:54.885146: Pseudo dice [0.8936, 0.8765] 
2023-03-21 16:16:54.885247: Epoch time: 13.53 s 
2023-03-21 16:16:56.079001:  
2023-03-21 16:16:56.079133: Epoch 890 
2023-03-21 16:16:56.079283: Current learning rate: 0.00137 
2023-03-21 16:17:09.703564: train_loss -0.9353 
2023-03-21 16:17:09.703774: val_loss -0.8349 
2023-03-21 16:17:09.703888: Pseudo dice [0.8942, 0.8778] 
2023-03-21 16:17:09.703981: Epoch time: 13.63 s 
2023-03-21 16:17:10.852514:  
2023-03-21 16:17:10.852638: Epoch 891 
2023-03-21 16:17:10.852782: Current learning rate: 0.00136 
2023-03-21 16:17:24.374269: train_loss -0.9356 
2023-03-21 16:17:24.374712: val_loss -0.8357 
2023-03-21 16:17:24.375161: Pseudo dice [0.8954, 0.8786] 
2023-03-21 16:17:24.375384: Epoch time: 13.52 s 
2023-03-21 16:17:25.571227:  
2023-03-21 16:17:25.571359: Epoch 892 
2023-03-21 16:17:25.571512: Current learning rate: 0.00135 
2023-03-21 16:17:39.135416: train_loss -0.9361 
2023-03-21 16:17:39.135679: val_loss -0.8393 
2023-03-21 16:17:39.135802: Pseudo dice [0.8962, 0.8798] 
2023-03-21 16:17:39.135908: Epoch time: 13.56 s 
2023-03-21 16:17:40.506539:  
2023-03-21 16:17:40.506680: Epoch 893 
2023-03-21 16:17:40.506862: Current learning rate: 0.00134 
2023-03-21 16:17:54.235490: train_loss -0.9364 
2023-03-21 16:17:54.235727: val_loss -0.8327 
2023-03-21 16:17:54.235841: Pseudo dice [0.8952, 0.8735] 
2023-03-21 16:17:54.235935: Epoch time: 13.73 s 
2023-03-21 16:17:55.406580:  
2023-03-21 16:17:55.406706: Epoch 894 
2023-03-21 16:17:55.406906: Current learning rate: 0.00133 
2023-03-21 16:18:09.206128: train_loss -0.935 
2023-03-21 16:18:09.206333: val_loss -0.8287 
2023-03-21 16:18:09.206450: Pseudo dice [0.8909, 0.8733] 
2023-03-21 16:18:09.206544: Epoch time: 13.8 s 
2023-03-21 16:18:10.354530:  
2023-03-21 16:18:10.354673: Epoch 895 
2023-03-21 16:18:10.354923: Current learning rate: 0.00132 
2023-03-21 16:18:24.155945: train_loss -0.9352 
2023-03-21 16:18:24.156193: val_loss -0.8363 
2023-03-21 16:18:24.156332: Pseudo dice [0.8953, 0.8785] 
2023-03-21 16:18:24.156460: Epoch time: 13.8 s 
2023-03-21 16:18:25.323375:  
2023-03-21 16:18:25.323507: Epoch 896 
2023-03-21 16:18:25.323652: Current learning rate: 0.0013 
2023-03-21 16:18:38.837925: train_loss -0.9367 
2023-03-21 16:18:38.838251: val_loss -0.8274 
2023-03-21 16:18:38.838575: Pseudo dice [0.8903, 0.8724] 
2023-03-21 16:18:38.838801: Epoch time: 13.52 s 
2023-03-21 16:18:39.991930:  
2023-03-21 16:18:39.992053: Epoch 897 
2023-03-21 16:18:39.992199: Current learning rate: 0.00129 
2023-03-21 16:18:53.532025: train_loss -0.937 
2023-03-21 16:18:53.532346: val_loss -0.8313 
2023-03-21 16:18:53.532563: Pseudo dice [0.8917, 0.8748] 
2023-03-21 16:18:53.532779: Epoch time: 13.54 s 
2023-03-21 16:18:54.889451:  
2023-03-21 16:18:54.889586: Epoch 898 
2023-03-21 16:18:54.889730: Current learning rate: 0.00128 
2023-03-21 16:19:08.649020: train_loss -0.9362 
2023-03-21 16:19:08.649281: val_loss -0.8347 
2023-03-21 16:19:08.649382: Pseudo dice [0.8938, 0.8774] 
2023-03-21 16:19:08.649481: Epoch time: 13.76 s 
2023-03-21 16:19:09.875768:  
2023-03-21 16:19:09.875898: Epoch 899 
2023-03-21 16:19:09.876039: Current learning rate: 0.00127 
2023-03-21 16:19:23.463598: train_loss -0.9348 
2023-03-21 16:19:23.463901: val_loss -0.8302 
2023-03-21 16:19:23.464035: Pseudo dice [0.8921, 0.8742] 
2023-03-21 16:19:23.464172: Epoch time: 13.59 s 
2023-03-21 16:19:24.977240:  
2023-03-21 16:19:24.977381: Epoch 900 
2023-03-21 16:19:24.977564: Current learning rate: 0.00126 
2023-03-21 16:19:38.511858: train_loss -0.9363 
2023-03-21 16:19:38.512119: val_loss -0.8338 
2023-03-21 16:19:38.512234: Pseudo dice [0.8943, 0.8773] 
2023-03-21 16:19:38.512331: Epoch time: 13.54 s 
2023-03-21 16:19:39.700516:  
2023-03-21 16:19:39.700639: Epoch 901 
2023-03-21 16:19:39.700799: Current learning rate: 0.00125 
2023-03-21 16:19:53.164579: train_loss -0.9365 
2023-03-21 16:19:53.164852: val_loss -0.8298 
2023-03-21 16:19:53.164970: Pseudo dice [0.8931, 0.8737] 
2023-03-21 16:19:53.165068: Epoch time: 13.46 s 
2023-03-21 16:19:54.558275:  
2023-03-21 16:19:54.558398: Epoch 902 
2023-03-21 16:19:54.558538: Current learning rate: 0.00124 
2023-03-21 16:20:08.180733: train_loss -0.9357 
2023-03-21 16:20:08.180997: val_loss -0.834 
2023-03-21 16:20:08.181101: Pseudo dice [0.8934, 0.8763] 
2023-03-21 16:20:08.181221: Epoch time: 13.62 s 
2023-03-21 16:20:09.525356:  
2023-03-21 16:20:09.525497: Epoch 903 
2023-03-21 16:20:09.525660: Current learning rate: 0.00122 
2023-03-21 16:20:23.401744: train_loss -0.9355 
2023-03-21 16:20:23.402251: val_loss -0.8284 
2023-03-21 16:20:23.402473: Pseudo dice [0.8915, 0.8746] 
2023-03-21 16:20:23.402703: Epoch time: 13.88 s 
2023-03-21 16:20:24.591404:  
2023-03-21 16:20:24.591534: Epoch 904 
2023-03-21 16:20:24.591785: Current learning rate: 0.00121 
2023-03-21 16:20:38.176329: train_loss -0.9361 
2023-03-21 16:20:38.176699: val_loss -0.8361 
2023-03-21 16:20:38.176933: Pseudo dice [0.8955, 0.8789] 
2023-03-21 16:20:38.177183: Epoch time: 13.59 s 
2023-03-21 16:20:39.367975:  
2023-03-21 16:20:39.368107: Epoch 905 
2023-03-21 16:20:39.368256: Current learning rate: 0.0012 
2023-03-21 16:20:53.114587: train_loss -0.937 
2023-03-21 16:20:53.114889: val_loss -0.8317 
2023-03-21 16:20:53.114992: Pseudo dice [0.8924, 0.874] 
2023-03-21 16:20:53.115132: Epoch time: 13.75 s 
2023-03-21 16:20:54.268480:  
2023-03-21 16:20:54.268607: Epoch 906 
2023-03-21 16:20:54.268751: Current learning rate: 0.00119 
2023-03-21 16:21:07.883403: train_loss -0.9366 
2023-03-21 16:21:07.883648: val_loss -0.8371 
2023-03-21 16:21:07.883748: Pseudo dice [0.8961, 0.8787] 
2023-03-21 16:21:07.883844: Epoch time: 13.62 s 
2023-03-21 16:21:09.062997:  
2023-03-21 16:21:09.063126: Epoch 907 
2023-03-21 16:21:09.063274: Current learning rate: 0.00118 
2023-03-21 16:21:22.525053: train_loss -0.9363 
2023-03-21 16:21:22.525520: val_loss -0.8387 
2023-03-21 16:21:22.525797: Pseudo dice [0.8962, 0.8798] 
2023-03-21 16:21:22.526227: Epoch time: 13.46 s 
2023-03-21 16:21:23.695816:  
2023-03-21 16:21:23.695937: Epoch 908 
2023-03-21 16:21:23.696080: Current learning rate: 0.00117 
2023-03-21 16:21:37.498040: train_loss -0.9361 
2023-03-21 16:21:37.498366: val_loss -0.8317 
2023-03-21 16:21:37.498582: Pseudo dice [0.8929, 0.8744] 
2023-03-21 16:21:37.498691: Epoch time: 13.8 s 
2023-03-21 16:21:38.716764:  
2023-03-21 16:21:38.716900: Epoch 909 
2023-03-21 16:21:38.717040: Current learning rate: 0.00116 
2023-03-21 16:21:52.340538: train_loss -0.9367 
2023-03-21 16:21:52.340801: val_loss -0.8368 
2023-03-21 16:21:52.340922: Pseudo dice [0.8953, 0.8777] 
2023-03-21 16:21:52.341027: Epoch time: 13.62 s 
2023-03-21 16:21:53.499476:  
2023-03-21 16:21:53.499603: Epoch 910 
2023-03-21 16:21:53.499749: Current learning rate: 0.00115 
2023-03-21 16:22:07.313671: train_loss -0.9373 
2023-03-21 16:22:07.313902: val_loss -0.8334 
2023-03-21 16:22:07.314008: Pseudo dice [0.894, 0.8757] 
2023-03-21 16:22:07.314110: Epoch time: 13.81 s 
2023-03-21 16:22:08.510506:  
2023-03-21 16:22:08.510656: Epoch 911 
2023-03-21 16:22:08.510855: Current learning rate: 0.00113 
2023-03-21 16:22:22.590548: train_loss -0.9356 
2023-03-21 16:22:22.590821: val_loss -0.8369 
2023-03-21 16:22:22.590932: Pseudo dice [0.8956, 0.8776] 
2023-03-21 16:22:22.591057: Epoch time: 14.08 s 
2023-03-21 16:22:23.751459:  
2023-03-21 16:22:23.751582: Epoch 912 
2023-03-21 16:22:23.751724: Current learning rate: 0.00112 
2023-03-21 16:22:37.119089: train_loss -0.9361 
2023-03-21 16:22:37.119341: val_loss -0.8344 
2023-03-21 16:22:37.119454: Pseudo dice [0.8944, 0.8763] 
2023-03-21 16:22:37.119565: Epoch time: 13.37 s 
2023-03-21 16:22:38.300112:  
2023-03-21 16:22:38.300239: Epoch 913 
2023-03-21 16:22:38.300383: Current learning rate: 0.00111 
2023-03-21 16:22:52.023584: train_loss -0.9366 
2023-03-21 16:22:52.023944: val_loss -0.8367 
2023-03-21 16:22:52.024231: Pseudo dice [0.8948, 0.8776] 
2023-03-21 16:22:52.024499: Epoch time: 13.72 s 
2023-03-21 16:22:53.384826:  
2023-03-21 16:22:53.384969: Epoch 914 
2023-03-21 16:22:53.385113: Current learning rate: 0.0011 
2023-03-21 16:23:07.043234: train_loss -0.9372 
2023-03-21 16:23:07.043526: val_loss -0.8394 
2023-03-21 16:23:07.043859: Pseudo dice [0.8963, 0.8797] 
2023-03-21 16:23:07.044250: Epoch time: 13.66 s 
2023-03-21 16:23:08.331316:  
2023-03-21 16:23:08.331444: Epoch 915 
2023-03-21 16:23:08.331623: Current learning rate: 0.00109 
2023-03-21 16:23:21.691116: train_loss -0.9369 
2023-03-21 16:23:21.691468: val_loss -0.8374 
2023-03-21 16:23:21.691704: Pseudo dice [0.896, 0.8793] 
2023-03-21 16:23:21.691939: Epoch time: 13.36 s 
2023-03-21 16:23:22.873232:  
2023-03-21 16:23:22.873397: Epoch 916 
2023-03-21 16:23:22.873547: Current learning rate: 0.00108 
2023-03-21 16:23:36.717090: train_loss -0.9359 
2023-03-21 16:23:36.718588: val_loss -0.8403 
2023-03-21 16:23:36.718708: Pseudo dice [0.8974, 0.8807] 
2023-03-21 16:23:36.718860: Epoch time: 13.84 s 
2023-03-21 16:23:37.886210:  
2023-03-21 16:23:37.886335: Epoch 917 
2023-03-21 16:23:37.886477: Current learning rate: 0.00106 
2023-03-21 16:23:51.648296: train_loss -0.9362 
2023-03-21 16:23:51.648620: val_loss -0.8375 
2023-03-21 16:23:51.648754: Pseudo dice [0.8955, 0.8781] 
2023-03-21 16:23:51.648875: Epoch time: 13.76 s 
2023-03-21 16:23:52.841986:  
2023-03-21 16:23:52.842110: Epoch 918 
2023-03-21 16:23:52.842253: Current learning rate: 0.00105 
2023-03-21 16:24:06.232434: train_loss -0.9389 
2023-03-21 16:24:06.232845: val_loss -0.8327 
2023-03-21 16:24:06.233039: Pseudo dice [0.8923, 0.8771] 
2023-03-21 16:24:06.233319: Epoch time: 13.39 s 
2023-03-21 16:24:07.595307:  
2023-03-21 16:24:07.595451: Epoch 919 
2023-03-21 16:24:07.595599: Current learning rate: 0.00104 
2023-03-21 16:24:21.382794: train_loss -0.9366 
2023-03-21 16:24:21.383047: val_loss -0.837 
2023-03-21 16:24:21.383150: Pseudo dice [0.8965, 0.8792] 
2023-03-21 16:24:21.383270: Epoch time: 13.79 s 
2023-03-21 16:24:22.560822:  
2023-03-21 16:24:22.560953: Epoch 920 
2023-03-21 16:24:22.561095: Current learning rate: 0.00103 
2023-03-21 16:24:36.284426: train_loss -0.938 
2023-03-21 16:24:36.286023: val_loss -0.8366 
2023-03-21 16:24:36.286125: Pseudo dice [0.8948, 0.8797] 
2023-03-21 16:24:36.286232: Epoch time: 13.72 s 
2023-03-21 16:24:37.471779:  
2023-03-21 16:24:37.471915: Epoch 921 
2023-03-21 16:24:37.472066: Current learning rate: 0.00102 
2023-03-21 16:24:51.354602: train_loss -0.9389 
2023-03-21 16:24:51.355081: val_loss -0.8282 
2023-03-21 16:24:51.355342: Pseudo dice [0.8913, 0.8734] 
2023-03-21 16:24:51.355626: Epoch time: 13.88 s 
2023-03-21 16:24:52.613290:  
2023-03-21 16:24:52.613419: Epoch 922 
2023-03-21 16:24:52.613574: Current learning rate: 0.00101 
2023-03-21 16:25:06.499698: train_loss -0.9357 
2023-03-21 16:25:06.499970: val_loss -0.8374 
2023-03-21 16:25:06.500087: Pseudo dice [0.8955, 0.8775] 
2023-03-21 16:25:06.500189: Epoch time: 13.89 s 
2023-03-21 16:25:07.761391:  
2023-03-21 16:25:07.761527: Epoch 923 
2023-03-21 16:25:07.761662: Current learning rate: 0.001 
2023-03-21 16:25:21.488963: train_loss -0.9364 
2023-03-21 16:25:21.489202: val_loss -0.8387 
2023-03-21 16:25:21.489336: Pseudo dice [0.898, 0.8801] 
2023-03-21 16:25:21.489467: Epoch time: 13.73 s 
2023-03-21 16:25:22.844847:  
2023-03-21 16:25:22.844986: Epoch 924 
2023-03-21 16:25:22.845138: Current learning rate: 0.00098 
2023-03-21 16:25:36.430995: train_loss -0.9372 
2023-03-21 16:25:36.431374: val_loss -0.834 
2023-03-21 16:25:36.431540: Pseudo dice [0.8939, 0.8751] 
2023-03-21 16:25:36.431723: Epoch time: 13.59 s 
2023-03-21 16:25:37.621766:  
2023-03-21 16:25:37.621903: Epoch 925 
2023-03-21 16:25:37.622053: Current learning rate: 0.00097 
2023-03-21 16:25:51.272301: train_loss -0.9387 
2023-03-21 16:25:51.272575: val_loss -0.8302 
2023-03-21 16:25:51.272693: Pseudo dice [0.891, 0.8753] 
2023-03-21 16:25:51.272799: Epoch time: 13.65 s 
2023-03-21 16:25:52.448313:  
2023-03-21 16:25:52.448622: Epoch 926 
2023-03-21 16:25:52.448867: Current learning rate: 0.00096 
2023-03-21 16:26:06.553809: train_loss -0.9373 
2023-03-21 16:26:06.554091: val_loss -0.8349 
2023-03-21 16:26:06.554201: Pseudo dice [0.894, 0.876] 
2023-03-21 16:26:06.554310: Epoch time: 14.11 s 
2023-03-21 16:26:07.734254:  
2023-03-21 16:26:07.734383: Epoch 927 
2023-03-21 16:26:07.734526: Current learning rate: 0.00095 
2023-03-21 16:26:21.470566: train_loss -0.937 
2023-03-21 16:26:21.470896: val_loss -0.8332 
2023-03-21 16:26:21.471025: Pseudo dice [0.8929, 0.8761] 
2023-03-21 16:26:21.471150: Epoch time: 13.74 s 
2023-03-21 16:26:22.701801:  
2023-03-21 16:26:22.701922: Epoch 928 
2023-03-21 16:26:22.702081: Current learning rate: 0.00094 
2023-03-21 16:26:36.365778: train_loss -0.9378 
2023-03-21 16:26:36.366021: val_loss -0.8315 
2023-03-21 16:26:36.366116: Pseudo dice [0.8924, 0.8743] 
2023-03-21 16:26:36.366212: Epoch time: 13.66 s 
2023-03-21 16:26:37.703046:  
2023-03-21 16:26:37.703180: Epoch 929 
2023-03-21 16:26:37.703322: Current learning rate: 0.00092 
2023-03-21 16:26:51.458650: train_loss -0.9381 
2023-03-21 16:26:51.459045: val_loss -0.8347 
2023-03-21 16:26:51.459279: Pseudo dice [0.8941, 0.8785] 
2023-03-21 16:26:51.459526: Epoch time: 13.76 s 
2023-03-21 16:26:52.632738:  
2023-03-21 16:26:52.632888: Epoch 930 
2023-03-21 16:26:52.633047: Current learning rate: 0.00091 
2023-03-21 16:27:06.311942: train_loss -0.9384 
2023-03-21 16:27:06.313532: val_loss -0.8336 
2023-03-21 16:27:06.313672: Pseudo dice [0.8938, 0.8762] 
2023-03-21 16:27:06.313803: Epoch time: 13.68 s 
2023-03-21 16:27:07.471625:  
2023-03-21 16:27:07.471819: Epoch 931 
2023-03-21 16:27:07.471978: Current learning rate: 0.0009 
2023-03-21 16:27:21.097700: train_loss -0.9375 
2023-03-21 16:27:21.097977: val_loss -0.8268 
2023-03-21 16:27:21.098082: Pseudo dice [0.8893, 0.8708] 
2023-03-21 16:27:21.098192: Epoch time: 13.63 s 
2023-03-21 16:27:22.278944:  
2023-03-21 16:27:22.279094: Epoch 932 
2023-03-21 16:27:22.279256: Current learning rate: 0.00089 
2023-03-21 16:27:35.719003: train_loss -0.9374 
2023-03-21 16:27:35.719200: val_loss -0.832 
2023-03-21 16:27:35.719300: Pseudo dice [0.8939, 0.8774] 
2023-03-21 16:27:35.719395: Epoch time: 13.44 s 
2023-03-21 16:27:36.865260:  
2023-03-21 16:27:36.865386: Epoch 933 
2023-03-21 16:27:36.865528: Current learning rate: 0.00088 
2023-03-21 16:27:50.501460: train_loss -0.9371 
2023-03-21 16:27:50.501714: val_loss -0.8335 
2023-03-21 16:27:50.501826: Pseudo dice [0.894, 0.8779] 
2023-03-21 16:27:50.501923: Epoch time: 13.64 s 
2023-03-21 16:27:51.833650:  
2023-03-21 16:27:51.833920: Epoch 934 
2023-03-21 16:27:51.834203: Current learning rate: 0.00087 
2023-03-21 16:28:05.516412: train_loss -0.9368 
2023-03-21 16:28:05.516699: val_loss -0.8302 
2023-03-21 16:28:05.516836: Pseudo dice [0.8912, 0.8745] 
2023-03-21 16:28:05.516958: Epoch time: 13.68 s 
2023-03-21 16:28:06.715019:  
2023-03-21 16:28:06.715198: Epoch 935 
2023-03-21 16:28:06.715361: Current learning rate: 0.00085 
2023-03-21 16:28:20.676985: train_loss -0.9373 
2023-03-21 16:28:20.677699: val_loss -0.8334 
2023-03-21 16:28:20.677815: Pseudo dice [0.8929, 0.877] 
2023-03-21 16:28:20.677910: Epoch time: 13.96 s 
2023-03-21 16:28:21.848135:  
2023-03-21 16:28:21.848279: Epoch 936 
2023-03-21 16:28:21.848439: Current learning rate: 0.00084 
2023-03-21 16:28:35.247548: train_loss -0.9373 
2023-03-21 16:28:35.247794: val_loss -0.8282 
2023-03-21 16:28:35.247907: Pseudo dice [0.8901, 0.8753] 
2023-03-21 16:28:35.247999: Epoch time: 13.4 s 
2023-03-21 16:28:36.398262:  
2023-03-21 16:28:36.398403: Epoch 937 
2023-03-21 16:28:36.398565: Current learning rate: 0.00083 
2023-03-21 16:28:50.131871: train_loss -0.9374 
2023-03-21 16:28:50.132124: val_loss -0.8302 
2023-03-21 16:28:50.132223: Pseudo dice [0.8918, 0.875] 
2023-03-21 16:28:50.132336: Epoch time: 13.73 s 
2023-03-21 16:28:51.291526:  
2023-03-21 16:28:51.291731: Epoch 938 
2023-03-21 16:28:51.291940: Current learning rate: 0.00082 
2023-03-21 16:29:05.110808: train_loss -0.9374 
2023-03-21 16:29:05.111096: val_loss -0.8347 
2023-03-21 16:29:05.111199: Pseudo dice [0.8944, 0.8775] 
2023-03-21 16:29:05.111311: Epoch time: 13.82 s 
2023-03-21 16:29:06.291046:  
2023-03-21 16:29:06.291172: Epoch 939 
2023-03-21 16:29:06.291320: Current learning rate: 0.00081 
2023-03-21 16:29:20.116745: train_loss -0.9397 
2023-03-21 16:29:20.117059: val_loss -0.8364 
2023-03-21 16:29:20.117188: Pseudo dice [0.8956, 0.8778] 
2023-03-21 16:29:20.117314: Epoch time: 13.83 s 
2023-03-21 16:29:21.475714:  
2023-03-21 16:29:21.475855: Epoch 940 
2023-03-21 16:29:21.475998: Current learning rate: 0.00079 
2023-03-21 16:29:35.498112: train_loss -0.9387 
2023-03-21 16:29:35.499150: val_loss -0.8298 
2023-03-21 16:29:35.499268: Pseudo dice [0.892, 0.875] 
2023-03-21 16:29:35.499365: Epoch time: 14.02 s 
2023-03-21 16:29:36.653454:  
2023-03-21 16:29:36.653585: Epoch 941 
2023-03-21 16:29:36.653746: Current learning rate: 0.00078 
2023-03-21 16:29:50.335511: train_loss -0.9382 
2023-03-21 16:29:50.335772: val_loss -0.8301 
2023-03-21 16:29:50.335874: Pseudo dice [0.8919, 0.8733] 
2023-03-21 16:29:50.335974: Epoch time: 13.68 s 
2023-03-21 16:29:51.512599:  
2023-03-21 16:29:51.512761: Epoch 942 
2023-03-21 16:29:51.512939: Current learning rate: 0.00077 
2023-03-21 16:30:05.159968: train_loss -0.9391 
2023-03-21 16:30:05.160168: val_loss -0.8334 
2023-03-21 16:30:05.160286: Pseudo dice [0.8945, 0.8761] 
2023-03-21 16:30:05.160382: Epoch time: 13.65 s 
2023-03-21 16:30:06.301797:  
2023-03-21 16:30:06.301921: Epoch 943 
2023-03-21 16:30:06.302086: Current learning rate: 0.00076 
2023-03-21 16:30:19.460357: train_loss -0.9374 
2023-03-21 16:30:19.460605: val_loss -0.8336 
2023-03-21 16:30:19.460718: Pseudo dice [0.8925, 0.8771] 
2023-03-21 16:30:19.460829: Epoch time: 13.16 s 
2023-03-21 16:30:20.684702:  
2023-03-21 16:30:20.684829: Epoch 944 
2023-03-21 16:30:20.684980: Current learning rate: 0.00075 
2023-03-21 16:30:34.214824: train_loss -0.9392 
2023-03-21 16:30:34.215269: val_loss -0.8343 
2023-03-21 16:30:34.215703: Pseudo dice [0.894, 0.8785] 
2023-03-21 16:30:34.215869: Epoch time: 13.53 s 
2023-03-21 16:30:35.583746:  
2023-03-21 16:30:35.583887: Epoch 945 
2023-03-21 16:30:35.584037: Current learning rate: 0.00074 
2023-03-21 16:30:49.115538: train_loss -0.9389 
2023-03-21 16:30:49.115721: val_loss -0.8298 
2023-03-21 16:30:49.115831: Pseudo dice [0.8909, 0.8758] 
2023-03-21 16:30:49.115922: Epoch time: 13.53 s 
2023-03-21 16:30:50.258828:  
2023-03-21 16:30:50.258965: Epoch 946 
2023-03-21 16:30:50.259156: Current learning rate: 0.00072 
2023-03-21 16:31:04.005715: train_loss -0.9372 
2023-03-21 16:31:04.005967: val_loss -0.834 
2023-03-21 16:31:04.006088: Pseudo dice [0.8933, 0.8765] 
2023-03-21 16:31:04.006227: Epoch time: 13.75 s 
2023-03-21 16:31:05.197457:  
2023-03-21 16:31:05.197593: Epoch 947 
2023-03-21 16:31:05.197746: Current learning rate: 0.00071 
2023-03-21 16:31:19.717409: train_loss -0.9379 
2023-03-21 16:31:19.717599: val_loss -0.8338 
2023-03-21 16:31:19.717695: Pseudo dice [0.8941, 0.8778] 
2023-03-21 16:31:19.717801: Epoch time: 14.52 s 
2023-03-21 16:31:20.870466:  
2023-03-21 16:31:20.870589: Epoch 948 
2023-03-21 16:31:20.870750: Current learning rate: 0.0007 
2023-03-21 16:31:34.719744: train_loss -0.9393 
2023-03-21 16:31:34.720001: val_loss -0.8321 
2023-03-21 16:31:34.720103: Pseudo dice [0.8921, 0.877] 
2023-03-21 16:31:34.720202: Epoch time: 13.85 s 
2023-03-21 16:31:35.917753:  
2023-03-21 16:31:35.917898: Epoch 949 
2023-03-21 16:31:35.918063: Current learning rate: 0.00069 
2023-03-21 16:31:49.356823: train_loss -0.9396 
2023-03-21 16:31:49.357078: val_loss -0.8336 
2023-03-21 16:31:49.357194: Pseudo dice [0.8941, 0.8767] 
2023-03-21 16:31:49.357291: Epoch time: 13.44 s 
2023-03-21 16:31:50.967366:  
2023-03-21 16:31:50.967511: Epoch 950 
2023-03-21 16:31:50.967659: Current learning rate: 0.00067 
2023-03-21 16:32:04.712473: train_loss -0.9382 
2023-03-21 16:32:04.712723: val_loss -0.8343 
2023-03-21 16:32:04.712821: Pseudo dice [0.8938, 0.8778] 
2023-03-21 16:32:04.712919: Epoch time: 13.75 s 
2023-03-21 16:32:05.887559:  
2023-03-21 16:32:05.887688: Epoch 951 
2023-03-21 16:32:05.887841: Current learning rate: 0.00066 
2023-03-21 16:32:19.424354: train_loss -0.9384 
2023-03-21 16:32:19.424763: val_loss -0.832 
2023-03-21 16:32:19.424941: Pseudo dice [0.8925, 0.8753] 
2023-03-21 16:32:19.425056: Epoch time: 13.54 s 
2023-03-21 16:32:20.738572:  
2023-03-21 16:32:20.738703: Epoch 952 
2023-03-21 16:32:20.738865: Current learning rate: 0.00065 
2023-03-21 16:32:34.453136: train_loss -0.9382 
2023-03-21 16:32:34.453577: val_loss -0.8343 
2023-03-21 16:32:34.453979: Pseudo dice [0.8933, 0.8771] 
2023-03-21 16:32:34.454183: Epoch time: 13.72 s 
2023-03-21 16:32:35.630387:  
2023-03-21 16:32:35.630513: Epoch 953 
2023-03-21 16:32:35.630670: Current learning rate: 0.00064 
2023-03-21 16:32:49.250570: train_loss -0.9379 
2023-03-21 16:32:49.250848: val_loss -0.8376 
2023-03-21 16:32:49.250957: Pseudo dice [0.8959, 0.8799] 
2023-03-21 16:32:49.251072: Epoch time: 13.62 s 
2023-03-21 16:32:50.442659:  
2023-03-21 16:32:50.442796: Epoch 954 
2023-03-21 16:32:50.442944: Current learning rate: 0.00063 
2023-03-21 16:33:03.899784: train_loss -0.9405 
2023-03-21 16:33:03.900010: val_loss -0.8345 
2023-03-21 16:33:03.900127: Pseudo dice [0.8939, 0.8782] 
2023-03-21 16:33:03.900228: Epoch time: 13.46 s 
2023-03-21 16:33:05.263575:  
2023-03-21 16:33:05.263718: Epoch 955 
2023-03-21 16:33:05.263870: Current learning rate: 0.00061 
2023-03-21 16:33:19.057920: train_loss -0.9376 
2023-03-21 16:33:19.058221: val_loss -0.8362 
2023-03-21 16:33:19.058533: Pseudo dice [0.8949, 0.8781] 
2023-03-21 16:33:19.058644: Epoch time: 13.79 s 
2023-03-21 16:33:20.323813:  
2023-03-21 16:33:20.323942: Epoch 956 
2023-03-21 16:33:20.324122: Current learning rate: 0.0006 
2023-03-21 16:33:34.321506: train_loss -0.9396 
2023-03-21 16:33:34.321765: val_loss -0.833 
2023-03-21 16:33:34.321896: Pseudo dice [0.8925, 0.8783] 
2023-03-21 16:33:34.321997: Epoch time: 14.0 s 
2023-03-21 16:33:35.542686:  
2023-03-21 16:33:35.542908: Epoch 957 
2023-03-21 16:33:35.543061: Current learning rate: 0.00059 
2023-03-21 16:33:49.287144: train_loss -0.9393 
2023-03-21 16:33:49.287449: val_loss -0.8342 
2023-03-21 16:33:49.287668: Pseudo dice [0.8934, 0.8777] 
2023-03-21 16:33:49.287945: Epoch time: 13.75 s 
2023-03-21 16:33:50.463944:  
2023-03-21 16:33:50.464067: Epoch 958 
2023-03-21 16:33:50.464209: Current learning rate: 0.00058 
2023-03-21 16:34:04.164014: train_loss -0.9392 
2023-03-21 16:34:04.164366: val_loss -0.8389 
2023-03-21 16:34:04.164566: Pseudo dice [0.8969, 0.8798] 
2023-03-21 16:34:04.164680: Epoch time: 13.7 s 
2023-03-21 16:34:05.375562:  
2023-03-21 16:34:05.375689: Epoch 959 
2023-03-21 16:34:05.375838: Current learning rate: 0.00056 
2023-03-21 16:34:19.197354: train_loss -0.939 
2023-03-21 16:34:19.197937: val_loss -0.8383 
2023-03-21 16:34:19.198137: Pseudo dice [0.8962, 0.8796] 
2023-03-21 16:34:19.198402: Epoch time: 13.82 s 
2023-03-21 16:34:20.603886:  
2023-03-21 16:34:20.604071: Epoch 960 
2023-03-21 16:34:20.604275: Current learning rate: 0.00055 
2023-03-21 16:34:34.707669: train_loss -0.9384 
2023-03-21 16:34:34.707929: val_loss -0.8373 
2023-03-21 16:34:34.708051: Pseudo dice [0.8959, 0.8798] 
2023-03-21 16:34:34.708155: Epoch time: 14.1 s 
2023-03-21 16:34:35.927629:  
2023-03-21 16:34:35.927827: Epoch 961 
2023-03-21 16:34:35.927999: Current learning rate: 0.00054 
2023-03-21 16:34:49.272605: train_loss -0.9394 
2023-03-21 16:34:49.272868: val_loss -0.8365 
2023-03-21 16:34:49.272987: Pseudo dice [0.8946, 0.8781] 
2023-03-21 16:34:49.273090: Epoch time: 13.35 s 
2023-03-21 16:34:50.510521:  
2023-03-21 16:34:50.510773: Epoch 962 
2023-03-21 16:34:50.511082: Current learning rate: 0.00053 
2023-03-21 16:35:04.300552: train_loss -0.9401 
2023-03-21 16:35:04.300760: val_loss -0.8375 
2023-03-21 16:35:04.300911: Pseudo dice [0.896, 0.8791] 
2023-03-21 16:35:04.301537: Epoch time: 13.79 s 
2023-03-21 16:35:05.480822:  
2023-03-21 16:35:05.480944: Epoch 963 
2023-03-21 16:35:05.481086: Current learning rate: 0.00051 
2023-03-21 16:35:19.192370: train_loss -0.9401 
2023-03-21 16:35:19.192730: val_loss -0.8352 
2023-03-21 16:35:19.192971: Pseudo dice [0.8945, 0.8781] 
2023-03-21 16:35:19.193094: Epoch time: 13.71 s 
2023-03-21 16:35:20.399401:  
2023-03-21 16:35:20.399731: Epoch 964 
2023-03-21 16:35:20.399920: Current learning rate: 0.0005 
2023-03-21 16:35:34.166528: train_loss -0.9396 
2023-03-21 16:35:34.166836: val_loss -0.8347 
2023-03-21 16:35:34.166953: Pseudo dice [0.8932, 0.8778] 
2023-03-21 16:35:34.167066: Epoch time: 13.77 s 
2023-03-21 16:35:35.504361:  
2023-03-21 16:35:35.504507: Epoch 965 
2023-03-21 16:35:35.504658: Current learning rate: 0.00049 
2023-03-21 16:35:48.933991: train_loss -0.9385 
2023-03-21 16:35:48.934278: val_loss -0.8333 
2023-03-21 16:35:48.934385: Pseudo dice [0.8942, 0.876] 
2023-03-21 16:35:48.934494: Epoch time: 13.43 s 
2023-03-21 16:35:50.147254:  
2023-03-21 16:35:50.147390: Epoch 966 
2023-03-21 16:35:50.147544: Current learning rate: 0.00048 
2023-03-21 16:36:03.928504: train_loss -0.9393 
2023-03-21 16:36:03.928758: val_loss -0.8351 
2023-03-21 16:36:03.928876: Pseudo dice [0.8966, 0.8785] 
2023-03-21 16:36:03.928979: Epoch time: 13.78 s 
2023-03-21 16:36:05.148767:  
2023-03-21 16:36:05.148905: Epoch 967 
2023-03-21 16:36:05.149053: Current learning rate: 0.00046 
2023-03-21 16:36:18.922532: train_loss -0.9391 
2023-03-21 16:36:18.922812: val_loss -0.8313 
2023-03-21 16:36:18.922933: Pseudo dice [0.892, 0.8759] 
2023-03-21 16:36:18.923065: Epoch time: 13.77 s 
2023-03-21 16:36:20.139709:  
2023-03-21 16:36:20.139838: Epoch 968 
2023-03-21 16:36:20.139989: Current learning rate: 0.00045 
2023-03-21 16:36:34.032339: train_loss -0.9397 
2023-03-21 16:36:34.032580: val_loss -0.8327 
2023-03-21 16:36:34.032695: Pseudo dice [0.8932, 0.8755] 
2023-03-21 16:36:34.032794: Epoch time: 13.89 s 
2023-03-21 16:36:35.218300:  
2023-03-21 16:36:35.218488: Epoch 969 
2023-03-21 16:36:35.218635: Current learning rate: 0.00044 
2023-03-21 16:36:48.803967: train_loss -0.94 
2023-03-21 16:36:48.804223: val_loss -0.8348 
2023-03-21 16:36:48.804320: Pseudo dice [0.8951, 0.8764] 
2023-03-21 16:36:48.804432: Epoch time: 13.59 s 
2023-03-21 16:36:50.130084:  
2023-03-21 16:36:50.130223: Epoch 970 
2023-03-21 16:36:50.130366: Current learning rate: 0.00043 
2023-03-21 16:37:03.636824: train_loss -0.9398 
2023-03-21 16:37:03.637089: val_loss -0.8345 
2023-03-21 16:37:03.637208: Pseudo dice [0.894, 0.8781] 
2023-03-21 16:37:03.637322: Epoch time: 13.51 s 
2023-03-21 16:37:04.819756:  
2023-03-21 16:37:04.819890: Epoch 971 
2023-03-21 16:37:04.820037: Current learning rate: 0.00041 
2023-03-21 16:37:19.212428: train_loss -0.9392 
2023-03-21 16:37:19.212713: val_loss -0.8329 
2023-03-21 16:37:19.212819: Pseudo dice [0.8938, 0.8764] 
2023-03-21 16:37:19.212934: Epoch time: 14.39 s 
2023-03-21 16:37:20.405612:  
2023-03-21 16:37:20.405747: Epoch 972 
2023-03-21 16:37:20.405897: Current learning rate: 0.0004 
2023-03-21 16:37:34.062238: train_loss -0.9401 
2023-03-21 16:37:34.062687: val_loss -0.8353 
2023-03-21 16:37:34.063110: Pseudo dice [0.8955, 0.8776] 
2023-03-21 16:37:34.063312: Epoch time: 13.66 s 
2023-03-21 16:37:35.309099:  
2023-03-21 16:37:35.309284: Epoch 973 
2023-03-21 16:37:35.309528: Current learning rate: 0.00039 
2023-03-21 16:37:48.935670: train_loss -0.9393 
2023-03-21 16:37:48.936027: val_loss -0.8343 
2023-03-21 16:37:48.936193: Pseudo dice [0.8944, 0.8777] 
2023-03-21 16:37:48.936436: Epoch time: 13.63 s 
2023-03-21 16:37:50.177758:  
2023-03-21 16:37:50.177879: Epoch 974 
2023-03-21 16:37:50.178020: Current learning rate: 0.00037 
2023-03-21 16:38:04.170218: train_loss -0.9394 
2023-03-21 16:38:04.170577: val_loss -0.8296 
2023-03-21 16:38:04.170710: Pseudo dice [0.8914, 0.8743] 
2023-03-21 16:38:04.170875: Epoch time: 13.99 s 
2023-03-21 16:38:05.513279:  
2023-03-21 16:38:05.513410: Epoch 975 
2023-03-21 16:38:05.513556: Current learning rate: 0.00036 
2023-03-21 16:38:19.128994: train_loss -0.9409 
2023-03-21 16:38:19.129274: val_loss -0.8328 
2023-03-21 16:38:19.129448: Pseudo dice [0.8942, 0.8756] 
2023-03-21 16:38:19.129725: Epoch time: 13.62 s 
2023-03-21 16:38:20.348977:  
2023-03-21 16:38:20.349112: Epoch 976 
2023-03-21 16:38:20.349258: Current learning rate: 0.00035 
2023-03-21 16:38:33.849757: train_loss -0.9401 
2023-03-21 16:38:33.849976: val_loss -0.8373 
2023-03-21 16:38:33.850120: Pseudo dice [0.8959, 0.8781] 
2023-03-21 16:38:33.850256: Epoch time: 13.5 s 
2023-03-21 16:38:35.070548:  
2023-03-21 16:38:35.070958: Epoch 977 
2023-03-21 16:38:35.071187: Current learning rate: 0.00034 
2023-03-21 16:38:48.605022: train_loss -0.9415 
2023-03-21 16:38:48.605282: val_loss -0.8292 
2023-03-21 16:38:48.605396: Pseudo dice [0.8922, 0.8754] 
2023-03-21 16:38:48.605490: Epoch time: 13.54 s 
2023-03-21 16:38:49.820961:  
2023-03-21 16:38:49.821091: Epoch 978 
2023-03-21 16:38:49.821242: Current learning rate: 0.00032 
2023-03-21 16:39:03.431481: train_loss -0.9412 
2023-03-21 16:39:03.431878: val_loss -0.8336 
2023-03-21 16:39:03.432035: Pseudo dice [0.893, 0.8776] 
2023-03-21 16:39:03.432190: Epoch time: 13.61 s 
2023-03-21 16:39:04.653832:  
2023-03-21 16:39:04.653976: Epoch 979 
2023-03-21 16:39:04.654130: Current learning rate: 0.00031 
2023-03-21 16:39:18.424254: train_loss -0.9402 
2023-03-21 16:39:18.424513: val_loss -0.8318 
2023-03-21 16:39:18.424637: Pseudo dice [0.893, 0.8754] 
2023-03-21 16:39:18.424738: Epoch time: 13.77 s 
2023-03-21 16:39:19.910474:  
2023-03-21 16:39:19.910614: Epoch 980 
2023-03-21 16:39:19.910788: Current learning rate: 0.0003 
2023-03-21 16:39:33.593961: train_loss -0.9399 
2023-03-21 16:39:33.594408: val_loss -0.8343 
2023-03-21 16:39:33.594608: Pseudo dice [0.8936, 0.8771] 
2023-03-21 16:39:33.595051: Epoch time: 13.68 s 
2023-03-21 16:39:34.818695:  
2023-03-21 16:39:34.818980: Epoch 981 
2023-03-21 16:39:34.819217: Current learning rate: 0.00028 
2023-03-21 16:39:48.168988: train_loss -0.9403 
2023-03-21 16:39:48.169254: val_loss -0.8298 
2023-03-21 16:39:48.169362: Pseudo dice [0.8918, 0.8749] 
2023-03-21 16:39:48.169471: Epoch time: 13.35 s 
2023-03-21 16:39:49.397139:  
2023-03-21 16:39:49.397277: Epoch 982 
2023-03-21 16:39:49.397430: Current learning rate: 0.00027 
2023-03-21 16:40:03.236766: train_loss -0.9403 
2023-03-21 16:40:03.236979: val_loss -0.8363 
2023-03-21 16:40:03.237099: Pseudo dice [0.8961, 0.8788] 
2023-03-21 16:40:03.237194: Epoch time: 13.84 s 
2023-03-21 16:40:04.437953:  
2023-03-21 16:40:04.438099: Epoch 983 
2023-03-21 16:40:04.438238: Current learning rate: 0.00026 
2023-03-21 16:40:18.071352: train_loss -0.9401 
2023-03-21 16:40:18.072937: val_loss -0.8295 
2023-03-21 16:40:18.073071: Pseudo dice [0.89, 0.875] 
2023-03-21 16:40:18.073198: Epoch time: 13.63 s 
2023-03-21 16:40:19.307248:  
2023-03-21 16:40:19.307380: Epoch 984 
2023-03-21 16:40:19.307508: Current learning rate: 0.00024 
2023-03-21 16:40:32.775737: train_loss -0.9392 
2023-03-21 16:40:32.776157: val_loss -0.8301 
2023-03-21 16:40:32.776299: Pseudo dice [0.8931, 0.8742] 
2023-03-21 16:40:32.776416: Epoch time: 13.47 s 
2023-03-21 16:40:34.191188:  
2023-03-21 16:40:34.191336: Epoch 985 
2023-03-21 16:40:34.191485: Current learning rate: 0.00023 
2023-03-21 16:40:48.116438: train_loss -0.9402 
2023-03-21 16:40:48.116711: val_loss -0.8285 
2023-03-21 16:40:48.116818: Pseudo dice [0.891, 0.8741] 
2023-03-21 16:40:48.116925: Epoch time: 13.93 s 
2023-03-21 16:40:49.342028:  
2023-03-21 16:40:49.342186: Epoch 986 
2023-03-21 16:40:49.342323: Current learning rate: 0.00021 
2023-03-21 16:41:03.325118: train_loss -0.9421 
2023-03-21 16:41:03.325513: val_loss -0.8341 
2023-03-21 16:41:03.325641: Pseudo dice [0.895, 0.8774] 
2023-03-21 16:41:03.325750: Epoch time: 13.98 s 
2023-03-21 16:41:04.534042:  
2023-03-21 16:41:04.534255: Epoch 987 
2023-03-21 16:41:04.534461: Current learning rate: 0.0002 
2023-03-21 16:41:18.242664: train_loss -0.94 
2023-03-21 16:41:18.242888: val_loss -0.8308 
2023-03-21 16:41:18.243006: Pseudo dice [0.8923, 0.8755] 
2023-03-21 16:41:18.243114: Epoch time: 13.71 s 
2023-03-21 16:41:19.409412:  
2023-03-21 16:41:19.409545: Epoch 988 
2023-03-21 16:41:19.409673: Current learning rate: 0.00019 
2023-03-21 16:41:32.744195: train_loss -0.9414 
2023-03-21 16:41:32.744467: val_loss -0.8293 
2023-03-21 16:41:32.744568: Pseudo dice [0.8912, 0.8749] 
2023-03-21 16:41:32.744676: Epoch time: 13.34 s 
2023-03-21 16:41:33.929886:  
2023-03-21 16:41:33.930014: Epoch 989 
2023-03-21 16:41:33.930161: Current learning rate: 0.00017 
2023-03-21 16:41:47.301934: train_loss -0.9413 
2023-03-21 16:41:47.302250: val_loss -0.834 
2023-03-21 16:41:47.302492: Pseudo dice [0.8939, 0.8777] 
2023-03-21 16:41:47.302865: Epoch time: 13.37 s 
2023-03-21 16:41:48.622011:  
2023-03-21 16:41:48.622203: Epoch 990 
2023-03-21 16:41:48.622401: Current learning rate: 0.00016 
2023-03-21 16:42:02.238227: train_loss -0.9402 
2023-03-21 16:42:02.238566: val_loss -0.8282 
2023-03-21 16:42:02.238918: Pseudo dice [0.8907, 0.8745] 
2023-03-21 16:42:02.239084: Epoch time: 13.62 s 
2023-03-21 16:42:03.418787:  
2023-03-21 16:42:03.418932: Epoch 991 
2023-03-21 16:42:03.419064: Current learning rate: 0.00014 
2023-03-21 16:42:16.909716: train_loss -0.9415 
2023-03-21 16:42:16.909966: val_loss -0.8309 
2023-03-21 16:42:16.910063: Pseudo dice [0.893, 0.8737] 
2023-03-21 16:42:16.910160: Epoch time: 13.49 s 
2023-03-21 16:42:18.096707:  
2023-03-21 16:42:18.096837: Epoch 992 
2023-03-21 16:42:18.096964: Current learning rate: 0.00013 
2023-03-21 16:42:31.885616: train_loss -0.94 
2023-03-21 16:42:31.885954: val_loss -0.8294 
2023-03-21 16:42:31.886193: Pseudo dice [0.8922, 0.8736] 
2023-03-21 16:42:31.886390: Epoch time: 13.79 s 
2023-03-21 16:42:33.076177:  
2023-03-21 16:42:33.076309: Epoch 993 
2023-03-21 16:42:33.076437: Current learning rate: 0.00011 
2023-03-21 16:42:46.602199: train_loss -0.9406 
2023-03-21 16:42:46.602500: val_loss -0.8299 
2023-03-21 16:42:46.602627: Pseudo dice [0.892, 0.875] 
2023-03-21 16:42:46.602837: Epoch time: 13.53 s 
2023-03-21 16:42:47.797164:  
2023-03-21 16:42:47.797290: Epoch 994 
2023-03-21 16:42:47.797417: Current learning rate: 0.0001 
2023-03-21 16:43:01.597560: train_loss -0.9417 
2023-03-21 16:43:01.597794: val_loss -0.8337 
2023-03-21 16:43:01.597895: Pseudo dice [0.8959, 0.8777] 
2023-03-21 16:43:01.598014: Epoch time: 13.8 s 
2023-03-21 16:43:02.955810:  
2023-03-21 16:43:02.955971: Epoch 995 
2023-03-21 16:43:02.956113: Current learning rate: 8e-05 
2023-03-21 16:43:16.593934: train_loss -0.9406 
2023-03-21 16:43:16.594236: val_loss -0.8387 
2023-03-21 16:43:16.594558: Pseudo dice [0.8975, 0.8812] 
2023-03-21 16:43:16.594718: Epoch time: 13.64 s 
2023-03-21 16:43:17.760787:  
2023-03-21 16:43:17.760920: Epoch 996 
2023-03-21 16:43:17.761051: Current learning rate: 7e-05 
2023-03-21 16:43:32.088369: train_loss -0.9411 
2023-03-21 16:43:32.088610: val_loss -0.8356 
2023-03-21 16:43:32.088712: Pseudo dice [0.895, 0.8786] 
2023-03-21 16:43:32.088814: Epoch time: 14.33 s 
2023-03-21 16:43:33.312783:  
2023-03-21 16:43:33.312912: Epoch 997 
2023-03-21 16:43:33.313064: Current learning rate: 5e-05 
2023-03-21 16:43:47.403750: train_loss -0.942 
2023-03-21 16:43:47.404164: val_loss -0.8338 
2023-03-21 16:43:47.404377: Pseudo dice [0.894, 0.8768] 
2023-03-21 16:43:47.404542: Epoch time: 14.09 s 
2023-03-21 16:43:48.632885:  
2023-03-21 16:43:48.633008: Epoch 998 
2023-03-21 16:43:48.633197: Current learning rate: 4e-05 
2023-03-21 16:44:02.239489: train_loss -0.9422 
2023-03-21 16:44:02.239972: val_loss -0.83 
2023-03-21 16:44:02.240161: Pseudo dice [0.8916, 0.8735] 
2023-03-21 16:44:02.240350: Epoch time: 13.61 s 
2023-03-21 16:44:03.451412:  
2023-03-21 16:44:03.451551: Epoch 999 
2023-03-21 16:44:03.451699: Current learning rate: 2e-05 
2023-03-21 16:44:17.363339: train_loss -0.939 
2023-03-21 16:44:17.363709: val_loss -0.8322 
2023-03-21 16:44:17.363943: Pseudo dice [0.8925, 0.8767] 
2023-03-21 16:44:17.364215: Epoch time: 13.91 s 
2023-03-21 16:44:19.277013: Using splits from existing split file: /data_hdd/users/zengzhilin/nnUNet_preprocessed/Dataset004_Hippocampus/splits_final.json 
2023-03-21 16:44:19.277930: The split file contains 5 splits. 
2023-03-21 16:44:19.278007: Desired fold for training: 0 
2023-03-21 16:44:19.278079: This split has 208 training and 52 validation cases. 
2023-03-21 16:44:19.278587: predicting hippocampus_017 
2023-03-21 16:44:19.449385: predicting hippocampus_019 
2023-03-21 16:44:19.646918: predicting hippocampus_033 
2023-03-21 16:44:19.817752: predicting hippocampus_035 
2023-03-21 16:44:19.930561: predicting hippocampus_037 
2023-03-21 16:44:20.054111: predicting hippocampus_049 
2023-03-21 16:44:20.158364: predicting hippocampus_052 
2023-03-21 16:44:20.351891: predicting hippocampus_065 
2023-03-21 16:44:20.398913: predicting hippocampus_083 
2023-03-21 16:44:20.448685: predicting hippocampus_088 
2023-03-21 16:44:20.554028: predicting hippocampus_090 
2023-03-21 16:44:20.659955: predicting hippocampus_092 
2023-03-21 16:44:20.751946: predicting hippocampus_095 
2023-03-21 16:44:20.839568: predicting hippocampus_107 
2023-03-21 16:44:20.886773: predicting hippocampus_108 
2023-03-21 16:44:20.937182: predicting hippocampus_123 
2023-03-21 16:44:20.986371: predicting hippocampus_125 
2023-03-21 16:44:23.078831: predicting hippocampus_157 
2023-03-21 16:44:23.152977: predicting hippocampus_164 
2023-03-21 16:44:23.327148: predicting hippocampus_169 
2023-03-21 16:44:23.370139: predicting hippocampus_175 
2023-03-21 16:44:23.415127: predicting hippocampus_185 
2023-03-21 16:44:23.459769: predicting hippocampus_190 
2023-03-21 16:44:23.504598: predicting hippocampus_194 
2023-03-21 16:44:23.549900: predicting hippocampus_204 
2023-03-21 16:44:23.595687: predicting hippocampus_205 
2023-03-21 16:44:23.641976: predicting hippocampus_210 
2023-03-21 16:44:23.686888: predicting hippocampus_217 
2023-03-21 16:44:23.731759: predicting hippocampus_219 
2023-03-21 16:44:23.774528: predicting hippocampus_229 
2023-03-21 16:44:23.818380: predicting hippocampus_244 
2023-03-21 16:44:23.863105: predicting hippocampus_261 
2023-03-21 16:44:23.942430: predicting hippocampus_264 
2023-03-21 16:44:23.986024: predicting hippocampus_277 
2023-03-21 16:44:24.064706: predicting hippocampus_280 
2023-03-21 16:44:24.109076: predicting hippocampus_286 
2023-03-21 16:44:24.188300: predicting hippocampus_288 
2023-03-21 16:44:24.307227: predicting hippocampus_289 
2023-03-21 16:44:24.375870: predicting hippocampus_296 
2023-03-21 16:44:24.438879: predicting hippocampus_305 
2023-03-21 16:44:24.489391: predicting hippocampus_308 
2023-03-21 16:44:24.538204: predicting hippocampus_317 
2023-03-21 16:44:24.588071: predicting hippocampus_327 
2023-03-21 16:44:24.636538: predicting hippocampus_330 
2023-03-21 16:44:24.684288: predicting hippocampus_332 
2023-03-21 16:44:24.731770: predicting hippocampus_338 
2023-03-21 16:44:24.821618: predicting hippocampus_349 
2023-03-21 16:44:24.870551: predicting hippocampus_350 
2023-03-21 16:44:24.918587: predicting hippocampus_356 
2023-03-21 16:44:24.967897: predicting hippocampus_358 
2023-03-21 16:44:25.017165: predicting hippocampus_374 
2023-03-21 16:44:25.068782: predicting hippocampus_394 
2023-03-21 16:44:28.659142: Validation complete 
2023-03-21 16:44:28.659279: Mean Validation Dice:  0.8911381772097078 
